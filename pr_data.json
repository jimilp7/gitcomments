[
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/45784",
    "comments": [
      "@penpornk @annarev I have submit the pluggable device implementation PR, please help to have a review. thanks very much!",
      "@kulinseth  @wchao1115  @pawelpiskorski FYI\r\n",
      "@penpornk @sanjoy I have addressed the comments, could you help to give a further review? Any comments will be appreciated. Thanks very much,.",
      "Is this targeted for 2.5 release ?",
      "@sub-mod Yes, we are trying to get this into 2.5 release.",
      "@penpornk I have addressed all the comments, please help to review. Thanks very much.",
      "@penpornk I have revert the device_spec.py and device_test.py, please have a review. Thanks.",
      "@penpornk I fixed a  sanity check failure(builderfiler format issue), sorry for bring the trouble.\r\nand I'm not sure whether  win_gen_out is relaeted with this PR, I will continue to check this. Thanks.\r\n```\r\n2021-03-10 14:35:51.678303: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at iterator_ops.cc:1191 : Internal: Could not parse: 64 as uint64\r\nERROR:tensorflow:Could not parse: 64 as uint64\r\n\t [[node save/DeserializeIterator (defined at \\\\?\\T:\\tmp\\Bazel.runfiles_hpqk5btx\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\data\\experimental\\kernel_tests\\snapshot_test.py:1201) ]]\r\n\r\nOriginal stack trace for 'save/DeserializeIterator':\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_hpqk5btx\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\data\\experimental\\kernel_tests\\snapshot_test.py\", line 1284, in <module>\r\n    test.main()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\test.py\", line 58, in main\r\n    return _googletest.main(argv)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\googletest.py\", line 66, in main\r\n    benchmark.benchmarks_main(true_main=main_wrapper, argv=argv)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\benchmark.py\", line 518, in benchmarks_main\r\n    true_main()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\googletest.py\",\r\n```",
      "@penpornk Windows Bazel GPU report visual studio error c2039 'iota' is not a member of 'std' in device_util_id.h, I added \"#include numeric\" in that headfile, I am not sure whether it can resolve, I check other files who also used this std function include this file. Thanks. ",
      "@penpornk Seems still one checks failed(MacOs CPU python3), It seems a strange error(one is direct_session, another is grappler), I am not sure whether it is related with this PR since I have no MacOS machine to reproduce, can you help to check this also? I will also check the possible issue. Thanks very much!",
      "@jzhoulon This check failed 4 hours ago too (same errors) when it passed elsewhere. So I reran it. It's still failing so it's likely related to this PR. I'll try to find how I can reproduce this from my side as well.",
      "> @jzhoulon This check failed 4 hours ago too (same errors) when it passed elsewhere. So I reran it. It's still failing so it's likely related to this PR. I'll try to find how I can reproduce this from my side as well.\r\n\r\n@jzhoulon and @penpornk , I have pulled in the latest patches and will try to repro on my side. I will update if I find something locally.\r\n",
      "> > @jzhoulon This check failed 4 hours ago too (same errors) when it passed elsewhere. So I reran it. It's still failing so it's likely related to this PR. I'll try to find how I can reproduce this from my side as well.\r\n> \r\n> @jzhoulon and @penpornk , I have pulled in the latest patches and will try to repro on my side. I will update if I find something locally.\r\n\r\n@kulinseth  Thanks for the help! I have some findings , though I still didn't figure out why only MacOS has this issue\r\n it was caused by framework:allocator be built into several modules, and make the static variable (cpu_allocator_collect_full_stats) duplicated and have inconsistent stats in several modules. I delete the dependence of framework:allocator in common_runtime:bfc_allocator and device:device_mem_allocator), the test can pass now. \r\n\r\nDetails:\r\nGPU and PluggableDevice's bfc_allocator deps on common_runtime:bfc_allocator and device:device_mem_allocator,  both of these modules(common_runtime:bfc_allocator && device::device_mem_allocator) depends on framework:allocator. In framework/allocator.cc, it has static variable(cpu_allocator_collect_full_stats), it is supposed to be global unique, however, I printed its address, it has two address(means it has duplicated variable) when the test case failure, as a result, cpu_alloc is always an old allocator(not new TrackingAllocator) in cpu_allocator_base. And I remove the dependence (framework::allocator) from common_runtime:bfc_allocator and device:device_mem_allocator, the build seems also pass(already depends on framework headers), and the test case pass.\r\n```\r\nCode:\r\n56 // If true, cpu allocator collects full stats.\r\n57 static bool cpu_allocator_collect_full_stats = false;\r\n58\r\n59 void EnableCPUAllocatorFullStats() { cpu_allocator_collect_full_stats = true; \r\n      printf(\"set cpu_allocator_collect_full_state_ptr = %p\\n\", &cpu_allocator_collect_full_stats);\r\n}\r\n\r\n68 Allocator* cpu_allocator_base() {\r\n69   static Allocator* cpu_alloc =\r\n70       AllocatorFactoryRegistry::singleton()->GetAllocator();\r\n71   // TODO(tucker): This really seems wrong.  It's only going to be effective on\r\n72   // the first call in a process (but the desired effect is associated with a\r\n73   // session), and we probably ought to be tracking the highest level Allocator,\r\n74   // not the lowest.  Revisit the advertised semantics of the triggering option.\r\n75   printf(\"get cpu_allocator_collect_full_stats_ptr = %p\\n\",  &cpu_allocator_collect_full_stats);\r\n76\r\n77   if (cpu_allocator_collect_full_stats && !cpu_alloc->TracksAllocationSizes()) {\r\n78     cpu_alloc = new TrackingAllocator(cpu_alloc, true);\r\n79   }\r\n80   return cpu_alloc;\r\n81 }\r\n```\r\ndebug log:\r\n```\r\n2021-03-12 14:09:32.971918: F tensorflow/core/grappler/clusters/single_machine_test.cc:55] Non-OK-status: cluster_->Provision() status: Invalid argument: Tracking allocation is not enabled.\r\nset cpu_allocator_collect_full_state_ptr = 0x11629b330\r\n\r\nget cpu_allocator_collect_full_stats_ptr = 0x11fc59b90\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n```\r\nfix patch:\r\n```\r\n--- a/tensorflow/core/common_runtime/BUILD\r\n+++ b/tensorflow/core/common_runtime/BUILD\r\n@@ -1697,7 +1697,7 @@ cc_library(\r\n         \"//tensorflow/core:lib\",\r\n         \"//tensorflow/core:lib_internal\",\r\n         \"//tensorflow/core:protos_all_cc\",\r\n-        \"//tensorflow/core/framework:allocator\",\r\n+        #\"//tensorflow/core/framework:allocator\",\r\n         \"//tensorflow/core/profiler/lib:traceme\",\r\n         \"@com_google_absl//absl/container:flat_hash_set\",\r\n        \"@com_google_absl//absl/strings\",\r\ndiff --git a/tensorflow/core/common_runtime/device/BUILD b/tensorflow/core/common_runtime/device/BUILD\r\nindex 3a49500053c..f0cd9030954 100644\r\n--- a/tensorflow/core/common_runtime/device/BUILD\r\n+++ b/tensorflow/core/common_runtime/device/BUILD\r\n@@ -75,7 +75,7 @@ tf_cuda_library(\r\n         \":device_id\",\r\n         \"//tensorflow/core:lib\",\r\n         \"//tensorflow/core:lib_internal\",\r\n-        \"//tensorflow/core/framework:allocator\",\r\n+        #\"//tensorflow/core/framework:allocator\",\r\n         \"//tensorflow/core/platform:stream_executor\",\r\n\r\n```\r\n\r\n(edited by penpornk@ to fix markdown formatting error which makes the patch hard to read)",
      "@jzhoulon Thank you for the quick findings! I think the duplicate symbol might be because both PluggableDevice runtime and GPU runtime are linked statically. Could you please help test linking PluggableDevice runtime dynamically? Our [tpu_runtime](https://github.com/tensorflow/tensorflow/blob/e0f5efab558c19dae5c8fc09b73da03a343a6773/tensorflow/core/tpu/BUILD#L286-L295) is also linked dynamically.\r\n\r\n@kulinseth Thank you for your help as well!\r\n\r\nI've pulled the PR in to test internally and am fixing other failures (which obscure this failure on Mac OS CI). I'll get to this once I'm done with other failures.",
      "> @jzhoulon Thank you for the quick findings! I think the duplicate symbol might be because both PluggableDevice runtime and GPU runtime are linked statically. Could you please help test linking PluggableDevice runtime dynamically? Our [tpu_runtime](https://github.com/tensorflow/tensorflow/blob/e0f5efab558c19dae5c8fc09b73da03a343a6773/tensorflow/core/tpu/BUILD#L286-L295) is also linked dynamically.\r\n> \r\n> @kulinseth Thank you for your help as well!\r\n> \r\n> I've pulled the PR in to test internally and am fixing other failures (which obscure this failure on Mac OS CI). I'll get to this once I'm done with other failures.\r\n\r\nThanks @jzhoulon for the findings. I was also curious why this was Mac only failure. I looked at the tests and they have the \"no_gpu\" tag to it (although Ubuntu CPU would have caught it too). Its also possible that the Xcode toolchain with compiler flags have different behavior with duplicate symbols. ",
      "@jzhoulon I had a question regarding the Pluggable impl. Did you test the pluggable implementation with host_memory_allocate/deallocate APIs being exercised ? I am locally not seeing them getting used, was curious if we need to set some special flag.",
      "> @jzhoulon I had a question regarding the Pluggable impl. Did you test the pluggable implementation with host_memory_allocate/deallocate APIs being exercised ? I am locally not seeing them getting used, was curious if we need to set some special flag.\r\n\r\n@kulinseth Thanks for the review. the host_memory_allocate is used in `PluggableDeviceProcessState::GetPluggableDeviceHostAllocator`, it will call DeviceHostAllocator to create host allocator, DeviceHostAllocator will call streamexecutor's HostMemAllocator.  no, you don't need to pass any flags in plugin, only to register the host_memory_allocate func ptr\r\n```\r\n    SubAllocator* sub_allocator = new DeviceHostAllocator(\r\n        se, numa_node, pluggable_device_host_alloc_visitors_[numa_node],\r\n        pluggable_device_host_free_visitors_[numa_node]);\r\n    int64 pluggable_device_host_mem_limit_in_mb = -1;\r\n```",
      "> @kulinseth Thanks for the review. the host_memory_allocate is used in `PluggableDeviceProcessState::GetPluggableDeviceHostAllocator`, it will call DeviceHostAllocator to create host allocator, DeviceHostAllocator will call streamexecutor's HostMemAllocator. no, you don't need to pass any flags in plugin, only to register the host_memory_allocate func ptr\r\n> \r\n> ```\r\n>     SubAllocator* sub_allocator = new DeviceHostAllocator(\r\n>         se, numa_node, pluggable_device_host_alloc_visitors_[numa_node],\r\n>         pluggable_device_host_free_visitors_[numa_node]);\r\n>     int64 pluggable_device_host_mem_limit_in_mb = -1;\r\n> ```\r\n\r\nThanks @jzhoulon. I do see the DeviceHostAllocator getting called (using the BFCAllocator) and registered, and we have the host_memory_allocate and deallocate functions registered but they are not getting invoked (rest of the alloc functions are working fine, like device mem allocate()/deallocate()) . Are we missing any `set_on_host()` or `force_gpu_compatible` flags to be set ? Is there any dependence on the numa configuration or memory stats.\r\n\r\nI am curious in your backend impl did you see any issues with using host_memory allocation features.",
      "> > @kulinseth Thanks for the review. the host_memory_allocate is used in `PluggableDeviceProcessState::GetPluggableDeviceHostAllocator`, it will call DeviceHostAllocator to create host allocator, DeviceHostAllocator will call streamexecutor's HostMemAllocator. no, you don't need to pass any flags in plugin, only to register the host_memory_allocate func ptr\r\n> > ```\r\n> >     SubAllocator* sub_allocator = new DeviceHostAllocator(\r\n> >         se, numa_node, pluggable_device_host_alloc_visitors_[numa_node],\r\n> >         pluggable_device_host_free_visitors_[numa_node]);\r\n> >     int64 pluggable_device_host_mem_limit_in_mb = -1;\r\n> > ```\r\n> \r\n> Thanks @jzhoulon. I do see the DeviceHostAllocator getting called (using the BFCAllocator) and registered, and we have the host_memory_allocate and deallocate functions registered but they are not getting invoked (rest of the alloc functions are working fine, like device mem allocate()/deallocate()) . Are we missing any `set_on_host()` or `force_gpu_compatible` flags to be set ? Is there any dependence on the numa configuration or memory stats.\r\n> \r\n> I am curious in your backend impl did you see any issues with using host_memory allocation features.\r\n\r\n@kulinseth   I just confirmed that host_memory_allocate can be invoked.(I add printf in hsot_memory_allocate and printed), can you try `force_gpu_compatible` to see whether it can be invoke? although I didn't find we use this option in our model, but it seems be invoked..\r\n```\r\nAllocator* PluggableDevice::GetAllocator(AllocatorAttributes attr) {\r\n  DCHECK(cpu_allocator_) << \"CPU allocator must be set\";\r\n  if (attr.on_host()) {\r\n    if (attr.gpu_compatible() || force_gpu_compatible_) {\r\n      PluggableDeviceProcessState* ps =\r\n          PluggableDeviceProcessState::singleton(device_type(), platform_name_);\r\n      return ps->GetPluggableDeviceHostAllocator(0);\r\n    } else {\r\n      return cpu_allocator_;\r\n    }\r\n  } else {\r\n    return device_allocator_;\r\n  }\r\n}\r\n\r\n```",
      "This PR was merged in https://github.com/tensorflow/tensorflow/commit/3a3878ff2dba3169c49991552bc1981f53f10099. I'm closing the PR now. Thank you very much everyone for the hard work!\r\n(We'll see in a day or two whether it will stick.)",
      "> \r\n> @kulinseth I just confirmed that host_memory_allocate can be invoked.(I add printf in hsot_memory_allocate and printed), can you try `force_gpu_compatible` to see whether it can be invoke? although I didn't find we use this option in our model, but it seems be invoked..\r\n\r\nThanks so much for checking."
    ],
    "review_comments": [
      {
        "body": "Can you change `string& platform_name` to `string* platform_name`? We generally use the latter in TF codebase. Same comment below.",
        "diff_hunk": "@@ -743,7 +743,8 @@ port::StatusOr<std::unique_ptr<StreamExecutor>> CPlatform::GetUncachedExecutor(\n   return result;\n }\n \n-port::Status InitStreamExecutorPlugin(void* dso_handle) {\n+port::Status InitStreamExecutorPlugin(void* dso_handle, string& device_type,\n+                                      string& platform_name) {"
      },
      {
        "body": "Add a comment saying that `device_type` and `device_type_alias` are output parameters.",
        "diff_hunk": "@@ -32,11 +32,14 @@ typedef void (*SEInitPluginFn)(SE_PlatformRegistrationParams* const,\n                                TF_Status* const);\n \n // Registers StreamExecutor platform."
      },
      {
        "body": "Make `Register` public instead. The only difference I see is Register returning a Status and DynamicRegister having a void return value.\r\n\r\nYou can update `Register` comment accordingly:\r\n```\r\n// Register a function for copying between two specific DeviceTypes.\r\n// Note: This should only be called via the constructor of\r\n// CopyTensor::Registration or from PluggableDevice implementation.\r\n```",
        "diff_hunk": "@@ -60,6 +60,15 @@ class CopyTensor {\n     }\n   };\n \n+  // Dynamic registered devices(PluggableDevices) use this function to\n+  // register a copy function.\n+  static void DynamicRegister(DeviceType sender_device_type,"
      },
      {
        "body": "Remove `tensorflow_models/` visibility here. It can always be added if needed in the future.",
        "diff_hunk": "@@ -0,0 +1,167 @@\n+load(\n+    \"//tensorflow:tensorflow.bzl\",\n+    \"tf_copts\",\n+)\n+load(\n+    \"//tensorflow/core/platform:rules_cc.bzl\",\n+    \"cc_library\",\n+)\n+load(\n+    \"//tensorflow/core/platform:build_config_root.bzl\",\n+    \"if_static\",\n+)\n+\n+package(\n+    default_visibility = [\n+        \"//tensorflow:internal\",\n+        \"//tensorflow_models:__subpackages__\","
      },
      {
        "body": "`if_static` appears to be unused. So, you can remove this load.",
        "diff_hunk": "@@ -0,0 +1,167 @@\n+load(\n+    \"//tensorflow:tensorflow.bzl\",\n+    \"tf_copts\",\n+)\n+load(\n+    \"//tensorflow/core/platform:rules_cc.bzl\",\n+    \"cc_library\",\n+)\n+load(\n+    \"//tensorflow/core/platform:build_config_root.bzl\",\n+    \"if_static\","
      },
      {
        "body": "Remove this export, it seems to be unused. It is also preferred to have a `cc_library` target over exporting a header.",
        "diff_hunk": "@@ -0,0 +1,167 @@\n+load(\n+    \"//tensorflow:tensorflow.bzl\",\n+    \"tf_copts\",\n+)\n+load(\n+    \"//tensorflow/core/platform:rules_cc.bzl\",\n+    \"cc_library\",\n+)\n+load(\n+    \"//tensorflow/core/platform:build_config_root.bzl\",\n+    \"if_static\",\n+)\n+\n+package(\n+    default_visibility = [\n+        \"//tensorflow:internal\",\n+        \"//tensorflow_models:__subpackages__\",\n+    ],\n+    licenses = [\"notice\"],  # Apache 2.0\n+)\n+\n+filegroup(\n+    name = \"pluggable_device_runtime_headers\",\n+    srcs = [\n+        \"pluggable_device.h\",\n+        \"pluggable_device_bfc_allocator.h\",\n+        \"pluggable_device_context.h\",\n+        \"pluggable_device_factory.h\",\n+        \"pluggable_device_init.h\",\n+        \"pluggable_device_process_state.h\",\n+        \"pluggable_device_util.h\",\n+        \"//tensorflow/core/common_runtime/device:device_runtime_headers\",\n+    ],\n+    visibility = [\"//visibility:public\"],\n+)\n+\n+cc_library(\n+    name = \"pluggable_device_runtime_impl\",\n+    srcs = [\n+        \"pluggable_device.cc\",\n+        \"pluggable_device_context.cc\",\n+        \"pluggable_device_factory.cc\",\n+        \"pluggable_device_process_state.cc\",\n+        \"pluggable_device_util.cc\",\n+    ],\n+    hdrs = [\":pluggable_device_runtime_headers\"],\n+    copts = tf_copts(),\n+    deps = [\n+        \":pluggable_device_bfc_allocator\",\n+        \":pluggable_device_init_impl\",\n+        \"//tensorflow/core:core_cpu_lib\",\n+        \"//tensorflow/core:framework\",\n+        \"//tensorflow/core:framework_internal\",\n+        \"//tensorflow/core:graph\",\n+        \"//tensorflow/core:lib\",\n+        \"//tensorflow/core:lib_internal\",\n+        \"//tensorflow/core:protos_all_cc\",\n+        \"//tensorflow/core/platform:stream_executor\",\n+        \"//tensorflow/core/common_runtime/device:device_event_mgr\",\n+        \"//tensorflow/core/platform:tensor_float_32_utils\",\n+    ],\n+    alwayslink = 1,\n+)\n+\n+cc_library(\n+    name = \"pluggable_device_plugin_init\",\n+    srcs = [\n+        \"pluggable_device_plugin_init.cc\",\n+    ],\n+    hdrs = [\n+        \"pluggable_device_plugin_init.h\",\n+        \":pluggable_device_runtime_headers\",\n+        \"//tensorflow/c:headers\",\n+        \"//tensorflow/core/common_runtime:core_cpu_base_headers\",\n+        \"//tensorflow/core/public:session_options.h\",\n+    ],\n+    copts = tf_copts(),\n+    visibility = [\"//visibility:public\"],\n+    deps = [\n+        \"//tensorflow/c/experimental/stream_executor:stream_executor\",\n+        \"//tensorflow/core:framework\",\n+        \"//tensorflow/core:framework_internal\",\n+        \"//tensorflow/core:lib\",\n+        \"//tensorflow/core:lib_internal\",\n+        \"//tensorflow/core:protos_all_cc\",\n+        \"//tensorflow/core/platform:stream_executor\",\n+        \"//tensorflow/core/common_runtime/pluggable_device:pluggable_device_runtime_impl\",\n+    ],\n+)\n+\n+exports_files("
      },
      {
        "body": "This comment was added to `GpuDevice` a long time ago. @penpornk do you know if all the options are still relevant?\r\n",
        "diff_hunk": "@@ -0,0 +1,425 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_event_mgr.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/local_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_context.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor.pb.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/variant_op_registry.h\"\n+#include \"tensorflow/core/graph/types.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/public/session_options.h\"\n+#include \"tensorflow/core/util/device_name_utils.h\"\n+#include \"tensorflow/core/util/env_var.h\"\n+#include \"tensorflow/core/util/stream_executor_util.h\"\n+\n+namespace tensorflow {\n+\n+// This factory helps to ensure that different PluggableDevice objects that\n+// refer to the same physical device and stream group id use the same stream\n+// group object (and therefore the same device streams). This is necessary since\n+// there is a single memory allocator per device (see\n+// ProcessState::GetPluggableDeviceAllocator) and allocators must not be shared\n+// across streams.\n+class PluggableDevice::StreamGroupFactory {\n+ public:\n+  // Returns the unique stream group for use with the stream defined by\n+  // {tf_device_id, stream_group_within_device}, creating it if it does not yet\n+  // exist.\n+  // This function is thread safe.\n+  PluggableDevice::StreamGroup* GetOrCreate(const std::string& device_type,\n+                                            TfDeviceId tf_device_id,\n+                                            int stream_group_within_device,\n+                                            se::StreamExecutor* executor,\n+                                            const GPUOptions& options) {\n+    mutex_lock guard(lock_);\n+    StreamGroup* group = &streams_[key_type(device_type, tf_device_id.value(),\n+                                            stream_group_within_device)];\n+    if (!group->compute) {\n+      group->compute = new se::Stream(executor);\n+      group->compute->Init();\n+      VLOG(2) << \"Created stream[\" << stream_group_within_device\n+              << \"] = \" << group->compute;\n+\n+      group->host_to_device = new se::Stream(executor);\n+      group->host_to_device->Init();\n+      VLOG(2) << \"Created host_to_device_stream[\" << stream_group_within_device\n+              << \"] = \" << group->host_to_device;\n+\n+      group->device_to_host = new se::Stream(executor);\n+      group->device_to_host->Init();\n+      VLOG(2) << \"Created device_to_host_stream[\" << stream_group_within_device\n+              << \"] = \" << group->device_to_host;\n+\n+      int num_d2d_streams =\n+          options.experimental().num_dev_to_dev_copy_streams();\n+      if (num_d2d_streams == 0) num_d2d_streams = 1;\n+      if (num_d2d_streams < 1 || num_d2d_streams > 4) {\n+        LOG(ERROR)\n+            << \"Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=\"\n+            << num_d2d_streams << \" set to 1 instead.\";\n+        num_d2d_streams = 1;\n+      }\n+      for (int i = 0; i < num_d2d_streams; ++i) {\n+        se::Stream* stream = new se::Stream(executor);\n+        stream->Init();\n+        group->device_to_device.push_back(stream);\n+        VLOG(2) << \"Created device_to_device_stream[\"\n+                << stream_group_within_device\n+                << \"] = \" << group->device_to_device.back();\n+      }\n+    }\n+    return group;\n+  }\n+\n+  // Returns a reference to the StreamGroupFactory singleton. Note that this is\n+  // never destroyed, so the objects it owns are never deleted.\n+  static StreamGroupFactory& Global() {\n+    static StreamGroupFactory* instance = new StreamGroupFactory();\n+    return *instance;\n+  }\n+\n+ private:\n+  mutex lock_;\n+  using key_type = std::tuple<std::string, int, int>;\n+  std::map<key_type, StreamGroup> streams_;\n+\n+  // StreamGroupFactory cannot be created directly; Call\n+  // StreamGroupFactory::Global to get the global instance.\n+  StreamGroupFactory() = default;\n+  TF_DISALLOW_COPY_AND_ASSIGN(StreamGroupFactory);\n+};\n+\n+PluggableDevice::PluggableDevice(\n+    const SessionOptions& options, const std::string& name,\n+    const std::string& device_type, const std::string& platform_name,\n+    Bytes memory_limit, const DeviceLocality& locality, TfDeviceId tf_device_id,\n+    const std::string& physical_device_desc, Allocator* device_allocator,\n+    Allocator* cpu_allocator, bool sync_every_op)\n+    : LocalDevice(options, Device::BuildDeviceAttributes(\n+                               name, device_type.c_str(), memory_limit,\n+                               locality, physical_device_desc)),\n+      device_allocator_(device_allocator),\n+      cpu_allocator_(cpu_allocator),\n+      tf_device_id_(tf_device_id),\n+      platform_name_(platform_name),\n+      sync_every_op_(sync_every_op) {\n+  if (options.config.has_gpu_options()) {\n+    force_gpu_compatible_ = options.config.gpu_options().force_gpu_compatible();\n+  }\n+  PluggableDeviceProcessState::singleton(device_type, platform_name)\n+      ->EnablePluggableDevice();\n+}\n+\n+PluggableDevice::~PluggableDevice() {\n+  delete pluggable_device_info_;\n+  device_context_->Unref();\n+}\n+\n+Status PluggableDevice::Init(const SessionOptions& options) {\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  auto executor_status = DeviceIdUtil::ExecutorForTfDeviceId(\n+      DeviceType(device_type()), platform, tf_device_id_);\n+  if (!executor_status.status().ok()) {\n+    return errors::Internal(\"Failed to get StreamExecutor for device\",\n+                            tf_device_id_.value());\n+  }\n+  executor_ = executor_status.ValueOrDie();\n+\n+  em_ = EventMgrFactory::Singleton()->GetEventMgr(executor_,\n+                                                  options.config.gpu_options());\n+\n+  stream_ = StreamGroupFactory::Global().GetOrCreate(\n+      device_type(), tf_device_id_, 0, executor_, options.config.gpu_options());\n+  device_context_ = new PluggableDeviceContext(\n+      0, stream_->compute, stream_->host_to_device, stream_->device_to_host,\n+      stream_->device_to_device);\n+  pluggable_device_info_ = new GpuDeviceInfo;\n+  pluggable_device_info_->stream = stream_->compute;\n+  pluggable_device_info_->default_context = device_context_;\n+  pluggable_device_info_->event_mgr = em_;\n+  PlatformDeviceId platform_device_id;\n+  TF_RETURN_IF_ERROR(DeviceIdManager::TfToPlatformDeviceId(\n+      DeviceType(device_type()), tf_device_id_, &platform_device_id));\n+  pluggable_device_info_->gpu_id = platform_device_id.value();\n+  set_tensorflow_gpu_device_info(pluggable_device_info_);\n+\n+  // Whether and how the PluggableDevice uses its own threadpool.\n+  // This option is experimental. Once we confirm the best setting, we\n+  // may change the default behavior and completely remove this flag."
      },
      {
        "body": "nit. `Statuscallback` --> `StatusCallback`",
        "diff_hunk": "@@ -0,0 +1,114 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_PLUGGABLEDEVICE_PLUGGABLEDEVICE_H_\n+#define TENSORFLOW_CORE_COMMON_RUNTIME_PLUGGABLEDEVICE_PLUGGABLEDEVICE_H_\n+\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_event_mgr.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/local_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_context.h\"\n+#include \"tensorflow/core/common_runtime/shared_counter.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/public/session_options.h\"\n+\n+namespace tensorflow {\n+\n+class PluggableDevice : public LocalDevice {\n+ public:\n+  PluggableDevice(const SessionOptions& options, const std::string& name,\n+                  const string& device_type, const string& platform_name,\n+                  Bytes memory_limit, const DeviceLocality& locality,\n+                  TfDeviceId tf_device_id,\n+                  const std::string& physical_device_desc,\n+                  Allocator* device_allocator, Allocator* cpu_allocator,\n+                  bool sync_every_op);\n+  ~PluggableDevice() override;\n+  // Initialize the device and return the status of initialization.\n+  Status Init(const SessionOptions& options);\n+\n+  void ComputeAsync(AsyncOpKernel* op_kernel, OpKernelContext* context,\n+                    AsyncOpKernel::DoneCallback done) override;\n+\n+  void Compute(OpKernel* op_kernel, OpKernelContext* context) override;\n+\n+  Status Sync() override;\n+\n+  Allocator* GetAllocator(AllocatorAttributes attr) override;\n+\n+  Status MakeTensorFromProto(const TensorProto& tensor_proto,\n+                             const AllocatorAttributes alloc_attrs,\n+                             Tensor* tensor) override;\n+\n+  void CopyTensorInSameDevice(const Tensor* input_tensor, Tensor* output_tensor,\n+                              const DeviceContext* device_context,\n+                              StatusCallback done) override;\n+\n+  // The executor that provides control for the pluggable device;\n+  se::StreamExecutor* executor() const { return executor_; }\n+\n+ private:\n+  Allocator* device_allocator_;\n+  Allocator* cpu_allocator_;\n+\n+  se::StreamExecutor* executor_ = nullptr;\n+  struct StreamGroup {\n+    se::Stream* compute = nullptr;\n+    se::Stream* host_to_device = nullptr;\n+    se::Stream* device_to_host = nullptr;\n+    gtl::InlinedVector<se::Stream*, 4> device_to_device;\n+  };\n+\n+  class StreamGroupFactory;\n+\n+  StreamGroup* stream_;\n+  PluggableDeviceContext* device_context_;\n+  GpuDeviceInfo* pluggable_device_info_ = nullptr;\n+  TfDeviceId tf_device_id_;\n+  const string platform_name_;\n+  const bool sync_every_op_ = false;\n+  EventMgr* em_ = nullptr;\n+  std::unique_ptr<thread::ThreadPool> thread_pool_;\n+  bool force_gpu_compatible_ = false;\n+  std::string ComputeOpKernelDebugString(const OpKernel& op_kernel,\n+                                         const int& stream_id);\n+\n+  // This method returns an initialization status, in addition to\n+  // calling the \"done\" Statuscallback, if there is a failure to"
      },
      {
        "body": "This function looks pretty self-contained. Can you move it to https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/common_runtime/device/device_id_utils.h and have both `GpuDevice` and this file call it?\r\n\r\nYou can update it to take `visible_device_count` as an argument, so that `PluggableDeviceMachineManager` call is not needed within the function implementation.",
        "diff_hunk": "@@ -0,0 +1,378 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Parse 'visible_device_list' into a list of platform PluggableDevice ids.\n+Status ParseVisibleDeviceList("
      },
      {
        "body": "nit. \"Pluggable device not\" --> \"Pluggable device does not\"",
        "diff_hunk": "@@ -0,0 +1,378 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Parse 'visible_device_list' into a list of platform PluggableDevice ids.\n+Status ParseVisibleDeviceList(\n+    const string& platform_name, const string& visible_device_list,\n+    std::vector<PlatformDeviceId>* visible_device_order) {\n+  visible_device_order->clear();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+\n+  // If the user wants to remap the visible to virtual Device mapping,\n+  // check for that here.\n+  if (visible_device_list.empty()) {\n+    visible_device_order->resize(platform->VisibleDeviceCount());\n+    // By default, visible to virtual mapping is unchanged.\n+    int deviceNo = 0;\n+    std::generate(visible_device_order->begin(), visible_device_order->end(),\n+                  [&deviceNo] { return deviceNo++; });\n+  } else {\n+    const std::vector<string> order_str =\n+        str_util::Split(visible_device_list, ',');\n+    for (const string& platform_device_id_str : order_str) {\n+      int32 platform_device_id;\n+      if (!strings::safe_strto32(platform_device_id_str, &platform_device_id)) {\n+        return errors::InvalidArgument(\n+            \"Could not parse entry in 'visible_device_list': '\",\n+            platform_device_id_str,\n+            \"'. visible_device_list = \", visible_device_list);\n+      }\n+      if (platform_device_id < 0 ||\n+          platform_device_id >= platform->VisibleDeviceCount()) {\n+        return errors::InvalidArgument(\n+            \"'visible_device_list' listed an invalid Device id '\",\n+            platform_device_id, \"' but visible device count is \",\n+            platform->VisibleDeviceCount());\n+      }\n+      visible_device_order->push_back(PlatformDeviceId(platform_device_id));\n+    }\n+  }\n+\n+  // Validate no repeats.\n+  std::set<PlatformDeviceId> visible_device_set(visible_device_order->begin(),\n+                                                visible_device_order->end());\n+  if (visible_device_set.size() != visible_device_order->size()) {\n+    return errors::InvalidArgument(\n+        \"visible_device_list contained a duplicate entry: \",\n+        visible_device_list);\n+  }\n+  return Status::OK();\n+}\n+\n+int64 MinSystemMemory(int64 available_memory) {\n+  // We use the following heuristic for now:\n+  //\n+  // If the available_memory is < 2GiB, we allocate 225MiB to system memory,\n+  // Otherwise, allocate max(300MiB, kMinSystemMemoryFraction *\n+  // available_memory) to system memory.\n+  //\n+  // In the future we could be more sophisticated by using a table of devices.\n+  int64 min_system_memory;\n+  constexpr float kMinSystemMemoryFraction = 0.06;\n+  if (available_memory < (1LL << 31)) {\n+    // 225MiB\n+    min_system_memory = 255 * 1024 * 1024;\n+  } else {\n+    // max(300 MiB, kMinSystemMemoryFraction * available_memory)\n+    min_system_memory = std::max(\n+        int64{314572800},\n+        static_cast<int64>(available_memory * kMinSystemMemoryFraction));\n+  }\n+#if defined(__GNUC__) && defined(__OPTIMIZE__)\n+// Do nothing\n+#elif !defined(__GNUC__) && defined(NDEBUG)\n+// Do nothing\n+#else\n+  // Double the amount of available PluggableDevice memory in non-opt builds\n+  // (debug builds in windows); because in non-opt builds more system memory is\n+  // necessary.\n+  min_system_memory *= 2;\n+#endif\n+  VLOG(5) << \"available_memory = \" << available_memory;\n+  VLOG(5) << \"min_system_memory = \" << min_system_memory;\n+}\n+\n+// Get the memory limit for the virtual device being created on PluggableDevice\n+// with 'platform_device_id', when that virtual device is the only\n+// virtual device being created on that Plugged Device.\n+Status SingleVirtualDeviceMemoryLimit(const string& platform_name,\n+                                      const GPUOptions& gpu_options,\n+                                      PlatformDeviceId platform_device_id,\n+                                      int64* memory_limit) {\n+  int64 total_memory = 0;\n+  int64 available_memory = 0;\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+  se::StreamExecutor* se =\n+      DeviceIdUtil::ExecutorForPlatformDeviceId(platform, platform_device_id)\n+          .ValueOrDie();\n+  if (!se->DeviceMemoryUsage(&available_memory, &total_memory)) {\n+    return errors::Unknown(\n+        \"Failed to query available memory for PluggableDevice \",\n+        platform_device_id.value());\n+  }\n+\n+  int64 allocated_memory = 0;\n+  const double per_process_gpu_memory_fraction =\n+      gpu_options.per_process_gpu_memory_fraction();\n+  if (per_process_gpu_memory_fraction > 1.0 ||\n+      gpu_options.experimental().use_unified_memory()) {\n+    return errors::Internal(\"not support unified memory yet.\");\n+  }\n+\n+  if (per_process_gpu_memory_fraction == 0) {\n+    allocated_memory = available_memory;\n+    const int64 min_system_memory = MinSystemMemory(available_memory);\n+    if (min_system_memory < allocated_memory) {\n+      allocated_memory -= min_system_memory;\n+    }\n+  } else {\n+    allocated_memory = total_memory * per_process_gpu_memory_fraction;\n+  }\n+  *memory_limit = allocated_memory;\n+  return Status::OK();\n+}\n+}  // namespace\n+\n+PluggableDeviceFactory::PluggableDeviceFactory(const string& device_type,\n+                                               const string& platform_name)\n+    : device_type_(device_type), platform_name_(platform_name) {}\n+\n+Status PluggableDeviceFactory::ListPhysicalDevices(\n+    std::vector<string>* devices) {\n+  TF_RETURN_IF_ERROR(ValidatePluggableDeviceMachineManager(platform_name_));\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+\n+  int device_count = platform->VisibleDeviceCount();\n+  for (int i = 0; i < device_count; ++i) {\n+    const string device_name =\n+        strings::StrCat(\"/physical_device:\", device_type_, \":\", i);\n+    devices->push_back(device_name);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status PluggableDeviceFactory::GetDeviceDetails(\n+    int device_index, std::unordered_map<string, string>* details) {\n+  TF_RETURN_IF_ERROR(ValidatePluggableDeviceMachineManager(platform_name_));\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  if (platform == nullptr) {\n+    return Status::OK();\n+  }\n+\n+  int device_count = platform->VisibleDeviceCount();\n+  if (device_index < 0 || device_index >= device_count) {\n+    return errors::Internal(\"Invalid device index: \", device_index);\n+  }\n+\n+  auto desc_status = platform->DescriptionForDevice(device_index);\n+  if (!desc_status.ok()) {\n+    return desc_status.status();\n+  }\n+\n+  auto desc = desc_status.ConsumeValueOrDie();\n+  (*details)[\"device_name\"] = desc->name();\n+  return Status::OK();\n+}\n+\n+Status PluggableDeviceFactory::CreateDevices(\n+    const SessionOptions& options, const string& name_prefix,\n+    std::vector<std::unique_ptr<Device>>* devices) {\n+  TF_RETURN_IF_ERROR(ValidatePluggableDeviceMachineManager(platform_name_));\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  if (platform == nullptr) {\n+    return Status::OK();\n+  }\n+\n+  if (platform->VisibleDeviceCount() <= 0) {\n+    return Status::OK();\n+  }\n+\n+  size_t num_devices_to_use = INT_MAX;\n+  auto iter = options.config.device_count().find(device_type_);\n+  if (iter != options.config.device_count().end()) {\n+    num_devices_to_use = iter->second;\n+  }\n+  const auto& gpu_options = options.config.gpu_options();\n+  std::vector<PlatformDeviceId> visible_device_order;\n+\n+  if (num_devices_to_use > 0) {\n+    TF_RETURN_IF_ERROR(ParseVisibleDeviceList(platform_name_,\n+                                              gpu_options.visible_device_list(),\n+                                              &visible_device_order));\n+  }\n+  if (num_devices_to_use > visible_device_order.size()) {\n+    num_devices_to_use = visible_device_order.size();\n+  }\n+\n+  const auto& virtual_devices = gpu_options.experimental().virtual_devices();\n+  if (!virtual_devices.empty())\n+    VLOG(2) << \"PluggableDevice not support virtual device setting yet\";"
      },
      {
        "body": "Do you plan to add support for unified memory in the near future?",
        "diff_hunk": "@@ -0,0 +1,378 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Parse 'visible_device_list' into a list of platform PluggableDevice ids.\n+Status ParseVisibleDeviceList(\n+    const string& platform_name, const string& visible_device_list,\n+    std::vector<PlatformDeviceId>* visible_device_order) {\n+  visible_device_order->clear();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+\n+  // If the user wants to remap the visible to virtual Device mapping,\n+  // check for that here.\n+  if (visible_device_list.empty()) {\n+    visible_device_order->resize(platform->VisibleDeviceCount());\n+    // By default, visible to virtual mapping is unchanged.\n+    int deviceNo = 0;\n+    std::generate(visible_device_order->begin(), visible_device_order->end(),\n+                  [&deviceNo] { return deviceNo++; });\n+  } else {\n+    const std::vector<string> order_str =\n+        str_util::Split(visible_device_list, ',');\n+    for (const string& platform_device_id_str : order_str) {\n+      int32 platform_device_id;\n+      if (!strings::safe_strto32(platform_device_id_str, &platform_device_id)) {\n+        return errors::InvalidArgument(\n+            \"Could not parse entry in 'visible_device_list': '\",\n+            platform_device_id_str,\n+            \"'. visible_device_list = \", visible_device_list);\n+      }\n+      if (platform_device_id < 0 ||\n+          platform_device_id >= platform->VisibleDeviceCount()) {\n+        return errors::InvalidArgument(\n+            \"'visible_device_list' listed an invalid Device id '\",\n+            platform_device_id, \"' but visible device count is \",\n+            platform->VisibleDeviceCount());\n+      }\n+      visible_device_order->push_back(PlatformDeviceId(platform_device_id));\n+    }\n+  }\n+\n+  // Validate no repeats.\n+  std::set<PlatformDeviceId> visible_device_set(visible_device_order->begin(),\n+                                                visible_device_order->end());\n+  if (visible_device_set.size() != visible_device_order->size()) {\n+    return errors::InvalidArgument(\n+        \"visible_device_list contained a duplicate entry: \",\n+        visible_device_list);\n+  }\n+  return Status::OK();\n+}\n+\n+int64 MinSystemMemory(int64 available_memory) {\n+  // We use the following heuristic for now:\n+  //\n+  // If the available_memory is < 2GiB, we allocate 225MiB to system memory,\n+  // Otherwise, allocate max(300MiB, kMinSystemMemoryFraction *\n+  // available_memory) to system memory.\n+  //\n+  // In the future we could be more sophisticated by using a table of devices.\n+  int64 min_system_memory;\n+  constexpr float kMinSystemMemoryFraction = 0.06;\n+  if (available_memory < (1LL << 31)) {\n+    // 225MiB\n+    min_system_memory = 255 * 1024 * 1024;\n+  } else {\n+    // max(300 MiB, kMinSystemMemoryFraction * available_memory)\n+    min_system_memory = std::max(\n+        int64{314572800},\n+        static_cast<int64>(available_memory * kMinSystemMemoryFraction));\n+  }\n+#if defined(__GNUC__) && defined(__OPTIMIZE__)\n+// Do nothing\n+#elif !defined(__GNUC__) && defined(NDEBUG)\n+// Do nothing\n+#else\n+  // Double the amount of available PluggableDevice memory in non-opt builds\n+  // (debug builds in windows); because in non-opt builds more system memory is\n+  // necessary.\n+  min_system_memory *= 2;\n+#endif\n+  VLOG(5) << \"available_memory = \" << available_memory;\n+  VLOG(5) << \"min_system_memory = \" << min_system_memory;\n+}\n+\n+// Get the memory limit for the virtual device being created on PluggableDevice\n+// with 'platform_device_id', when that virtual device is the only\n+// virtual device being created on that Plugged Device.\n+Status SingleVirtualDeviceMemoryLimit(const string& platform_name,\n+                                      const GPUOptions& gpu_options,\n+                                      PlatformDeviceId platform_device_id,\n+                                      int64* memory_limit) {\n+  int64 total_memory = 0;\n+  int64 available_memory = 0;\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+  se::StreamExecutor* se =\n+      DeviceIdUtil::ExecutorForPlatformDeviceId(platform, platform_device_id)\n+          .ValueOrDie();\n+  if (!se->DeviceMemoryUsage(&available_memory, &total_memory)) {\n+    return errors::Unknown(\n+        \"Failed to query available memory for PluggableDevice \",\n+        platform_device_id.value());\n+  }\n+\n+  int64 allocated_memory = 0;\n+  const double per_process_gpu_memory_fraction =\n+      gpu_options.per_process_gpu_memory_fraction();\n+  if (per_process_gpu_memory_fraction > 1.0 ||\n+      gpu_options.experimental().use_unified_memory()) {\n+    return errors::Internal(\"not support unified memory yet.\");"
      },
      {
        "body": "This depends on the supports_unified_memory flag in stream_executor platform/DeviceDescription(currently, stream executor does not has this flag while stream executor c api has this, and we need to use stream executor here). This code logic is to check whether the platform/device supports the unified memory, if support, then continue to execute, if not, then return.",
        "diff_hunk": "@@ -0,0 +1,378 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Parse 'visible_device_list' into a list of platform PluggableDevice ids.\n+Status ParseVisibleDeviceList(\n+    const string& platform_name, const string& visible_device_list,\n+    std::vector<PlatformDeviceId>* visible_device_order) {\n+  visible_device_order->clear();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+\n+  // If the user wants to remap the visible to virtual Device mapping,\n+  // check for that here.\n+  if (visible_device_list.empty()) {\n+    visible_device_order->resize(platform->VisibleDeviceCount());\n+    // By default, visible to virtual mapping is unchanged.\n+    int deviceNo = 0;\n+    std::generate(visible_device_order->begin(), visible_device_order->end(),\n+                  [&deviceNo] { return deviceNo++; });\n+  } else {\n+    const std::vector<string> order_str =\n+        str_util::Split(visible_device_list, ',');\n+    for (const string& platform_device_id_str : order_str) {\n+      int32 platform_device_id;\n+      if (!strings::safe_strto32(platform_device_id_str, &platform_device_id)) {\n+        return errors::InvalidArgument(\n+            \"Could not parse entry in 'visible_device_list': '\",\n+            platform_device_id_str,\n+            \"'. visible_device_list = \", visible_device_list);\n+      }\n+      if (platform_device_id < 0 ||\n+          platform_device_id >= platform->VisibleDeviceCount()) {\n+        return errors::InvalidArgument(\n+            \"'visible_device_list' listed an invalid Device id '\",\n+            platform_device_id, \"' but visible device count is \",\n+            platform->VisibleDeviceCount());\n+      }\n+      visible_device_order->push_back(PlatformDeviceId(platform_device_id));\n+    }\n+  }\n+\n+  // Validate no repeats.\n+  std::set<PlatformDeviceId> visible_device_set(visible_device_order->begin(),\n+                                                visible_device_order->end());\n+  if (visible_device_set.size() != visible_device_order->size()) {\n+    return errors::InvalidArgument(\n+        \"visible_device_list contained a duplicate entry: \",\n+        visible_device_list);\n+  }\n+  return Status::OK();\n+}\n+\n+int64 MinSystemMemory(int64 available_memory) {\n+  // We use the following heuristic for now:\n+  //\n+  // If the available_memory is < 2GiB, we allocate 225MiB to system memory,\n+  // Otherwise, allocate max(300MiB, kMinSystemMemoryFraction *\n+  // available_memory) to system memory.\n+  //\n+  // In the future we could be more sophisticated by using a table of devices.\n+  int64 min_system_memory;\n+  constexpr float kMinSystemMemoryFraction = 0.06;\n+  if (available_memory < (1LL << 31)) {\n+    // 225MiB\n+    min_system_memory = 255 * 1024 * 1024;\n+  } else {\n+    // max(300 MiB, kMinSystemMemoryFraction * available_memory)\n+    min_system_memory = std::max(\n+        int64{314572800},\n+        static_cast<int64>(available_memory * kMinSystemMemoryFraction));\n+  }\n+#if defined(__GNUC__) && defined(__OPTIMIZE__)\n+// Do nothing\n+#elif !defined(__GNUC__) && defined(NDEBUG)\n+// Do nothing\n+#else\n+  // Double the amount of available PluggableDevice memory in non-opt builds\n+  // (debug builds in windows); because in non-opt builds more system memory is\n+  // necessary.\n+  min_system_memory *= 2;\n+#endif\n+  VLOG(5) << \"available_memory = \" << available_memory;\n+  VLOG(5) << \"min_system_memory = \" << min_system_memory;\n+}\n+\n+// Get the memory limit for the virtual device being created on PluggableDevice\n+// with 'platform_device_id', when that virtual device is the only\n+// virtual device being created on that Plugged Device.\n+Status SingleVirtualDeviceMemoryLimit(const string& platform_name,\n+                                      const GPUOptions& gpu_options,\n+                                      PlatformDeviceId platform_device_id,\n+                                      int64* memory_limit) {\n+  int64 total_memory = 0;\n+  int64 available_memory = 0;\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+  se::StreamExecutor* se =\n+      DeviceIdUtil::ExecutorForPlatformDeviceId(platform, platform_device_id)\n+          .ValueOrDie();\n+  if (!se->DeviceMemoryUsage(&available_memory, &total_memory)) {\n+    return errors::Unknown(\n+        \"Failed to query available memory for PluggableDevice \",\n+        platform_device_id.value());\n+  }\n+\n+  int64 allocated_memory = 0;\n+  const double per_process_gpu_memory_fraction =\n+      gpu_options.per_process_gpu_memory_fraction();\n+  if (per_process_gpu_memory_fraction > 1.0 ||\n+      gpu_options.experimental().use_unified_memory()) {\n+    return errors::Internal(\"not support unified memory yet.\");"
      },
      {
        "body": "done.",
        "diff_hunk": "@@ -743,7 +743,8 @@ port::StatusOr<std::unique_ptr<StreamExecutor>> CPlatform::GetUncachedExecutor(\n   return result;\n }\n \n-port::Status InitStreamExecutorPlugin(void* dso_handle) {\n+port::Status InitStreamExecutorPlugin(void* dso_handle, string& device_type,\n+                                      string& platform_name) {"
      },
      {
        "body": "done",
        "diff_hunk": "@@ -0,0 +1,167 @@\n+load(\n+    \"//tensorflow:tensorflow.bzl\",\n+    \"tf_copts\",\n+)\n+load(\n+    \"//tensorflow/core/platform:rules_cc.bzl\",\n+    \"cc_library\",\n+)\n+load(\n+    \"//tensorflow/core/platform:build_config_root.bzl\",\n+    \"if_static\",\n+)\n+\n+package(\n+    default_visibility = [\n+        \"//tensorflow:internal\",\n+        \"//tensorflow_models:__subpackages__\",\n+    ],\n+    licenses = [\"notice\"],  # Apache 2.0\n+)\n+\n+filegroup(\n+    name = \"pluggable_device_runtime_headers\",\n+    srcs = [\n+        \"pluggable_device.h\",\n+        \"pluggable_device_bfc_allocator.h\",\n+        \"pluggable_device_context.h\",\n+        \"pluggable_device_factory.h\",\n+        \"pluggable_device_init.h\",\n+        \"pluggable_device_process_state.h\",\n+        \"pluggable_device_util.h\",\n+        \"//tensorflow/core/common_runtime/device:device_runtime_headers\",\n+    ],\n+    visibility = [\"//visibility:public\"],\n+)\n+\n+cc_library(\n+    name = \"pluggable_device_runtime_impl\",\n+    srcs = [\n+        \"pluggable_device.cc\",\n+        \"pluggable_device_context.cc\",\n+        \"pluggable_device_factory.cc\",\n+        \"pluggable_device_process_state.cc\",\n+        \"pluggable_device_util.cc\",\n+    ],\n+    hdrs = [\":pluggable_device_runtime_headers\"],\n+    copts = tf_copts(),\n+    deps = [\n+        \":pluggable_device_bfc_allocator\",\n+        \":pluggable_device_init_impl\",\n+        \"//tensorflow/core:core_cpu_lib\",\n+        \"//tensorflow/core:framework\",\n+        \"//tensorflow/core:framework_internal\",\n+        \"//tensorflow/core:graph\",\n+        \"//tensorflow/core:lib\",\n+        \"//tensorflow/core:lib_internal\",\n+        \"//tensorflow/core:protos_all_cc\",\n+        \"//tensorflow/core/platform:stream_executor\",\n+        \"//tensorflow/core/common_runtime/device:device_event_mgr\",\n+        \"//tensorflow/core/platform:tensor_float_32_utils\",\n+    ],\n+    alwayslink = 1,\n+)\n+\n+cc_library(\n+    name = \"pluggable_device_plugin_init\",\n+    srcs = [\n+        \"pluggable_device_plugin_init.cc\",\n+    ],\n+    hdrs = [\n+        \"pluggable_device_plugin_init.h\",\n+        \":pluggable_device_runtime_headers\",\n+        \"//tensorflow/c:headers\",\n+        \"//tensorflow/core/common_runtime:core_cpu_base_headers\",\n+        \"//tensorflow/core/public:session_options.h\",\n+    ],\n+    copts = tf_copts(),\n+    visibility = [\"//visibility:public\"],\n+    deps = [\n+        \"//tensorflow/c/experimental/stream_executor:stream_executor\",\n+        \"//tensorflow/core:framework\",\n+        \"//tensorflow/core:framework_internal\",\n+        \"//tensorflow/core:lib\",\n+        \"//tensorflow/core:lib_internal\",\n+        \"//tensorflow/core:protos_all_cc\",\n+        \"//tensorflow/core/platform:stream_executor\",\n+        \"//tensorflow/core/common_runtime/pluggable_device:pluggable_device_runtime_impl\",\n+    ],\n+)\n+\n+exports_files("
      },
      {
        "body": "done",
        "diff_hunk": "@@ -32,11 +32,14 @@ typedef void (*SEInitPluginFn)(SE_PlatformRegistrationParams* const,\n                                TF_Status* const);\n \n // Registers StreamExecutor platform."
      },
      {
        "body": "done",
        "diff_hunk": "@@ -60,6 +60,15 @@ class CopyTensor {\n     }\n   };\n \n+  // Dynamic registered devices(PluggableDevices) use this function to\n+  // register a copy function.\n+  static void DynamicRegister(DeviceType sender_device_type,"
      },
      {
        "body": "done",
        "diff_hunk": "@@ -0,0 +1,167 @@\n+load(\n+    \"//tensorflow:tensorflow.bzl\",\n+    \"tf_copts\",\n+)\n+load(\n+    \"//tensorflow/core/platform:rules_cc.bzl\",\n+    \"cc_library\",\n+)\n+load(\n+    \"//tensorflow/core/platform:build_config_root.bzl\",\n+    \"if_static\",\n+)\n+\n+package(\n+    default_visibility = [\n+        \"//tensorflow:internal\",\n+        \"//tensorflow_models:__subpackages__\","
      },
      {
        "body": "done",
        "diff_hunk": "@@ -0,0 +1,167 @@\n+load(\n+    \"//tensorflow:tensorflow.bzl\",\n+    \"tf_copts\",\n+)\n+load(\n+    \"//tensorflow/core/platform:rules_cc.bzl\",\n+    \"cc_library\",\n+)\n+load(\n+    \"//tensorflow/core/platform:build_config_root.bzl\",\n+    \"if_static\","
      },
      {
        "body": "Done, thanks for reminder",
        "diff_hunk": "@@ -0,0 +1,114 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_PLUGGABLEDEVICE_PLUGGABLEDEVICE_H_\n+#define TENSORFLOW_CORE_COMMON_RUNTIME_PLUGGABLEDEVICE_PLUGGABLEDEVICE_H_\n+\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_event_mgr.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/local_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_context.h\"\n+#include \"tensorflow/core/common_runtime/shared_counter.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/public/session_options.h\"\n+\n+namespace tensorflow {\n+\n+class PluggableDevice : public LocalDevice {\n+ public:\n+  PluggableDevice(const SessionOptions& options, const std::string& name,\n+                  const string& device_type, const string& platform_name,\n+                  Bytes memory_limit, const DeviceLocality& locality,\n+                  TfDeviceId tf_device_id,\n+                  const std::string& physical_device_desc,\n+                  Allocator* device_allocator, Allocator* cpu_allocator,\n+                  bool sync_every_op);\n+  ~PluggableDevice() override;\n+  // Initialize the device and return the status of initialization.\n+  Status Init(const SessionOptions& options);\n+\n+  void ComputeAsync(AsyncOpKernel* op_kernel, OpKernelContext* context,\n+                    AsyncOpKernel::DoneCallback done) override;\n+\n+  void Compute(OpKernel* op_kernel, OpKernelContext* context) override;\n+\n+  Status Sync() override;\n+\n+  Allocator* GetAllocator(AllocatorAttributes attr) override;\n+\n+  Status MakeTensorFromProto(const TensorProto& tensor_proto,\n+                             const AllocatorAttributes alloc_attrs,\n+                             Tensor* tensor) override;\n+\n+  void CopyTensorInSameDevice(const Tensor* input_tensor, Tensor* output_tensor,\n+                              const DeviceContext* device_context,\n+                              StatusCallback done) override;\n+\n+  // The executor that provides control for the pluggable device;\n+  se::StreamExecutor* executor() const { return executor_; }\n+\n+ private:\n+  Allocator* device_allocator_;\n+  Allocator* cpu_allocator_;\n+\n+  se::StreamExecutor* executor_ = nullptr;\n+  struct StreamGroup {\n+    se::Stream* compute = nullptr;\n+    se::Stream* host_to_device = nullptr;\n+    se::Stream* device_to_host = nullptr;\n+    gtl::InlinedVector<se::Stream*, 4> device_to_device;\n+  };\n+\n+  class StreamGroupFactory;\n+\n+  StreamGroup* stream_;\n+  PluggableDeviceContext* device_context_;\n+  GpuDeviceInfo* pluggable_device_info_ = nullptr;\n+  TfDeviceId tf_device_id_;\n+  const string platform_name_;\n+  const bool sync_every_op_ = false;\n+  EventMgr* em_ = nullptr;\n+  std::unique_ptr<thread::ThreadPool> thread_pool_;\n+  bool force_gpu_compatible_ = false;\n+  std::string ComputeOpKernelDebugString(const OpKernel& op_kernel,\n+                                         const int& stream_id);\n+\n+  // This method returns an initialization status, in addition to\n+  // calling the \"done\" Statuscallback, if there is a failure to"
      },
      {
        "body": "done",
        "diff_hunk": "@@ -0,0 +1,378 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Parse 'visible_device_list' into a list of platform PluggableDevice ids.\n+Status ParseVisibleDeviceList("
      },
      {
        "body": "done",
        "diff_hunk": "@@ -0,0 +1,378 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Parse 'visible_device_list' into a list of platform PluggableDevice ids.\n+Status ParseVisibleDeviceList(\n+    const string& platform_name, const string& visible_device_list,\n+    std::vector<PlatformDeviceId>* visible_device_order) {\n+  visible_device_order->clear();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+\n+  // If the user wants to remap the visible to virtual Device mapping,\n+  // check for that here.\n+  if (visible_device_list.empty()) {\n+    visible_device_order->resize(platform->VisibleDeviceCount());\n+    // By default, visible to virtual mapping is unchanged.\n+    int deviceNo = 0;\n+    std::generate(visible_device_order->begin(), visible_device_order->end(),\n+                  [&deviceNo] { return deviceNo++; });\n+  } else {\n+    const std::vector<string> order_str =\n+        str_util::Split(visible_device_list, ',');\n+    for (const string& platform_device_id_str : order_str) {\n+      int32 platform_device_id;\n+      if (!strings::safe_strto32(platform_device_id_str, &platform_device_id)) {\n+        return errors::InvalidArgument(\n+            \"Could not parse entry in 'visible_device_list': '\",\n+            platform_device_id_str,\n+            \"'. visible_device_list = \", visible_device_list);\n+      }\n+      if (platform_device_id < 0 ||\n+          platform_device_id >= platform->VisibleDeviceCount()) {\n+        return errors::InvalidArgument(\n+            \"'visible_device_list' listed an invalid Device id '\",\n+            platform_device_id, \"' but visible device count is \",\n+            platform->VisibleDeviceCount());\n+      }\n+      visible_device_order->push_back(PlatformDeviceId(platform_device_id));\n+    }\n+  }\n+\n+  // Validate no repeats.\n+  std::set<PlatformDeviceId> visible_device_set(visible_device_order->begin(),\n+                                                visible_device_order->end());\n+  if (visible_device_set.size() != visible_device_order->size()) {\n+    return errors::InvalidArgument(\n+        \"visible_device_list contained a duplicate entry: \",\n+        visible_device_list);\n+  }\n+  return Status::OK();\n+}\n+\n+int64 MinSystemMemory(int64 available_memory) {\n+  // We use the following heuristic for now:\n+  //\n+  // If the available_memory is < 2GiB, we allocate 225MiB to system memory,\n+  // Otherwise, allocate max(300MiB, kMinSystemMemoryFraction *\n+  // available_memory) to system memory.\n+  //\n+  // In the future we could be more sophisticated by using a table of devices.\n+  int64 min_system_memory;\n+  constexpr float kMinSystemMemoryFraction = 0.06;\n+  if (available_memory < (1LL << 31)) {\n+    // 225MiB\n+    min_system_memory = 255 * 1024 * 1024;\n+  } else {\n+    // max(300 MiB, kMinSystemMemoryFraction * available_memory)\n+    min_system_memory = std::max(\n+        int64{314572800},\n+        static_cast<int64>(available_memory * kMinSystemMemoryFraction));\n+  }\n+#if defined(__GNUC__) && defined(__OPTIMIZE__)\n+// Do nothing\n+#elif !defined(__GNUC__) && defined(NDEBUG)\n+// Do nothing\n+#else\n+  // Double the amount of available PluggableDevice memory in non-opt builds\n+  // (debug builds in windows); because in non-opt builds more system memory is\n+  // necessary.\n+  min_system_memory *= 2;\n+#endif\n+  VLOG(5) << \"available_memory = \" << available_memory;\n+  VLOG(5) << \"min_system_memory = \" << min_system_memory;\n+}\n+\n+// Get the memory limit for the virtual device being created on PluggableDevice\n+// with 'platform_device_id', when that virtual device is the only\n+// virtual device being created on that Plugged Device.\n+Status SingleVirtualDeviceMemoryLimit(const string& platform_name,\n+                                      const GPUOptions& gpu_options,\n+                                      PlatformDeviceId platform_device_id,\n+                                      int64* memory_limit) {\n+  int64 total_memory = 0;\n+  int64 available_memory = 0;\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name);\n+  se::StreamExecutor* se =\n+      DeviceIdUtil::ExecutorForPlatformDeviceId(platform, platform_device_id)\n+          .ValueOrDie();\n+  if (!se->DeviceMemoryUsage(&available_memory, &total_memory)) {\n+    return errors::Unknown(\n+        \"Failed to query available memory for PluggableDevice \",\n+        platform_device_id.value());\n+  }\n+\n+  int64 allocated_memory = 0;\n+  const double per_process_gpu_memory_fraction =\n+      gpu_options.per_process_gpu_memory_fraction();\n+  if (per_process_gpu_memory_fraction > 1.0 ||\n+      gpu_options.experimental().use_unified_memory()) {\n+    return errors::Internal(\"not support unified memory yet.\");\n+  }\n+\n+  if (per_process_gpu_memory_fraction == 0) {\n+    allocated_memory = available_memory;\n+    const int64 min_system_memory = MinSystemMemory(available_memory);\n+    if (min_system_memory < allocated_memory) {\n+      allocated_memory -= min_system_memory;\n+    }\n+  } else {\n+    allocated_memory = total_memory * per_process_gpu_memory_fraction;\n+  }\n+  *memory_limit = allocated_memory;\n+  return Status::OK();\n+}\n+}  // namespace\n+\n+PluggableDeviceFactory::PluggableDeviceFactory(const string& device_type,\n+                                               const string& platform_name)\n+    : device_type_(device_type), platform_name_(platform_name) {}\n+\n+Status PluggableDeviceFactory::ListPhysicalDevices(\n+    std::vector<string>* devices) {\n+  TF_RETURN_IF_ERROR(ValidatePluggableDeviceMachineManager(platform_name_));\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+\n+  int device_count = platform->VisibleDeviceCount();\n+  for (int i = 0; i < device_count; ++i) {\n+    const string device_name =\n+        strings::StrCat(\"/physical_device:\", device_type_, \":\", i);\n+    devices->push_back(device_name);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status PluggableDeviceFactory::GetDeviceDetails(\n+    int device_index, std::unordered_map<string, string>* details) {\n+  TF_RETURN_IF_ERROR(ValidatePluggableDeviceMachineManager(platform_name_));\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  if (platform == nullptr) {\n+    return Status::OK();\n+  }\n+\n+  int device_count = platform->VisibleDeviceCount();\n+  if (device_index < 0 || device_index >= device_count) {\n+    return errors::Internal(\"Invalid device index: \", device_index);\n+  }\n+\n+  auto desc_status = platform->DescriptionForDevice(device_index);\n+  if (!desc_status.ok()) {\n+    return desc_status.status();\n+  }\n+\n+  auto desc = desc_status.ConsumeValueOrDie();\n+  (*details)[\"device_name\"] = desc->name();\n+  return Status::OK();\n+}\n+\n+Status PluggableDeviceFactory::CreateDevices(\n+    const SessionOptions& options, const string& name_prefix,\n+    std::vector<std::unique_ptr<Device>>* devices) {\n+  TF_RETURN_IF_ERROR(ValidatePluggableDeviceMachineManager(platform_name_));\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  if (platform == nullptr) {\n+    return Status::OK();\n+  }\n+\n+  if (platform->VisibleDeviceCount() <= 0) {\n+    return Status::OK();\n+  }\n+\n+  size_t num_devices_to_use = INT_MAX;\n+  auto iter = options.config.device_count().find(device_type_);\n+  if (iter != options.config.device_count().end()) {\n+    num_devices_to_use = iter->second;\n+  }\n+  const auto& gpu_options = options.config.gpu_options();\n+  std::vector<PlatformDeviceId> visible_device_order;\n+\n+  if (num_devices_to_use > 0) {\n+    TF_RETURN_IF_ERROR(ParseVisibleDeviceList(platform_name_,\n+                                              gpu_options.visible_device_list(),\n+                                              &visible_device_order));\n+  }\n+  if (num_devices_to_use > visible_device_order.size()) {\n+    num_devices_to_use = visible_device_order.size();\n+  }\n+\n+  const auto& virtual_devices = gpu_options.experimental().virtual_devices();\n+  if (!virtual_devices.empty())\n+    VLOG(2) << \"PluggableDevice not support virtual device setting yet\";"
      },
      {
        "body": "Please correct me if I missed it, it seems like we are using the default BFC memory allocator for device allocations, and not going through the SE interface discussed in the Stream Executor RFC:\r\n\r\n```\r\nvoid (*create_allocator)(const SP_Platform* platform,\r\n                           SE_CreateAllocatorParams* params, TF_Status* status);\r\n  void (*destroy_allocator)(const SP_Platform* platform,\r\n                            SP_Allocator* allocator,\r\n                            SP_AllocatorFns* allocator_fns);\r\n```\r\n\r\nAlso I think custom allocator support might need to be added. I can follow up with another PR to add support for that. Please let me know if I missed it elsewhere.",
        "diff_hunk": "@@ -0,0 +1,204 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstring>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_host_allocator.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pool_allocator.h\"\n+#include \"tensorflow/core/common_runtime/shared_counter.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/log_memory.h\"\n+#include \"tensorflow/core/framework/tracking_allocator.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/util/env_var.h\"\n+\n+namespace tensorflow {\n+\n+PluggableDeviceProcessState* PluggableDeviceProcessState::singleton(\n+    const string& device_type, const string& platform_name) {\n+  static std::unordered_map<string, PluggableDeviceProcessState*>\n+      process_state_map;\n+  auto iter = process_state_map.find(platform_name);\n+  if (iter != process_state_map.end()) {\n+    return iter->second;\n+  }\n+  process_state_map[platform_name] =\n+      new PluggableDeviceProcessState(device_type, platform_name);\n+  return process_state_map[platform_name];\n+}\n+\n+PluggableDeviceProcessState::PluggableDeviceProcessState(\n+    const string& device_type, const string& platform_name)\n+    : pluggable_device_enabled_(false),\n+      device_type_(device_type),\n+      platform_name_(platform_name) {\n+  process_state_ = ProcessState::singleton();\n+}\n+\n+int PluggableDeviceProcessState::BusIdForPluggableDevice(\n+    TfDeviceId tf_device_id) {\n+  // Return the NUMA node accociated with the PluggableDevice's StreamExecutor.\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  se::StreamExecutor* se = DeviceIdUtil::ExecutorForTfDeviceId(\n+                               DeviceType(device_type_), platform, tf_device_id)\n+                               .ValueOrDie();\n+  int numa_node = se->GetDeviceDescription().numa_node();\n+  // bus_id must be non-negative. If the numa_node is unknown, use 0\n+  return numa_node >= 0 ? numa_node : 0;\n+}\n+\n+Allocator* PluggableDeviceProcessState::GetPluggableDeviceAllocator(\n+    const GPUOptions& options, TfDeviceId tf_device_id, size_t total_bytes) {\n+  CHECK(process_state_);\n+  const string& allocator_type = options.allocator_type();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  mutex_lock lock(mu_);"
      },
      {
        "body": "Nit: Please add ticks around variable names.\r\n```suggestion\r\n// Registers StreamExecutor platform. `device_type` and `platform_name` are\r\n// output parameters.\r\n```",
        "diff_hunk": "@@ -31,12 +31,16 @@ namespace stream_executor {\n typedef void (*SEInitPluginFn)(SE_PlatformRegistrationParams* const,\n                                TF_Status* const);\n \n-// Registers StreamExecutor platform.\n-port::Status InitStreamExecutorPlugin(void* dso_handle);\n+// Registers StreamExecutor platform. device_type and platform_name are output\n+// parameters."
      },
      {
        "body": "Nit:\r\n```suggestion\r\n  // Parse `visible_device_list` into a list of platform Device ids.\r\n```",
        "diff_hunk": "@@ -59,6 +59,54 @@ class DeviceIdUtil {\n         << \" id: \" << platform_device_id\n         << \", visible device count: \" << visible_device_count;\n   }\n+\n+  // Parse 'visible_device_list' into a list of platform Device ids."
      },
      {
        "body": "I think it's simpler to use `std::iota`.\r\n```suggestion\r\n      std::iota(visible_device_order->begin(), visible_device_order->end(), 0);\r\n```",
        "diff_hunk": "@@ -59,6 +59,54 @@ class DeviceIdUtil {\n         << \" id: \" << platform_device_id\n         << \", visible device count: \" << visible_device_count;\n   }\n+\n+  // Parse 'visible_device_list' into a list of platform Device ids.\n+  static Status ParseVisibleDeviceList(\n+      const string& visible_device_list, const int visible_device_count,\n+      std::vector<PlatformDeviceId>* visible_device_order) {\n+    visible_device_order->clear();\n+\n+    // If the user wants to remap the visible to virtual Device mapping,\n+    // check for that here.\n+    if (visible_device_list.empty()) {\n+      visible_device_order->resize(visible_device_count);\n+      // By default, visible to virtual mapping is unchanged.\n+      int deviceNo = 0;\n+      std::generate(visible_device_order->begin(), visible_device_order->end(),\n+                    [&deviceNo] { return deviceNo++; });"
      },
      {
        "body": "yes, you are right, custom allocator support is needed in streamexecutor c api. we are appreciate that you can contribute custom allocator feature in PluggableDevice. ",
        "diff_hunk": "@@ -0,0 +1,204 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstring>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_host_allocator.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pool_allocator.h\"\n+#include \"tensorflow/core/common_runtime/shared_counter.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/log_memory.h\"\n+#include \"tensorflow/core/framework/tracking_allocator.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/util/env_var.h\"\n+\n+namespace tensorflow {\n+\n+PluggableDeviceProcessState* PluggableDeviceProcessState::singleton(\n+    const string& device_type, const string& platform_name) {\n+  static std::unordered_map<string, PluggableDeviceProcessState*>\n+      process_state_map;\n+  auto iter = process_state_map.find(platform_name);\n+  if (iter != process_state_map.end()) {\n+    return iter->second;\n+  }\n+  process_state_map[platform_name] =\n+      new PluggableDeviceProcessState(device_type, platform_name);\n+  return process_state_map[platform_name];\n+}\n+\n+PluggableDeviceProcessState::PluggableDeviceProcessState(\n+    const string& device_type, const string& platform_name)\n+    : pluggable_device_enabled_(false),\n+      device_type_(device_type),\n+      platform_name_(platform_name) {\n+  process_state_ = ProcessState::singleton();\n+}\n+\n+int PluggableDeviceProcessState::BusIdForPluggableDevice(\n+    TfDeviceId tf_device_id) {\n+  // Return the NUMA node accociated with the PluggableDevice's StreamExecutor.\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  se::StreamExecutor* se = DeviceIdUtil::ExecutorForTfDeviceId(\n+                               DeviceType(device_type_), platform, tf_device_id)\n+                               .ValueOrDie();\n+  int numa_node = se->GetDeviceDescription().numa_node();\n+  // bus_id must be non-negative. If the numa_node is unknown, use 0\n+  return numa_node >= 0 ? numa_node : 0;\n+}\n+\n+Allocator* PluggableDeviceProcessState::GetPluggableDeviceAllocator(\n+    const GPUOptions& options, TfDeviceId tf_device_id, size_t total_bytes) {\n+  CHECK(process_state_);\n+  const string& allocator_type = options.allocator_type();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  mutex_lock lock(mu_);"
      },
      {
        "body": "I think @annarev has already started an internal PR for this, but was deciding between \r\n1) Having a separate registration for the custom allocator, e.g.,\r\n```c++\r\nvoid TF_InitAllocatorPlugin(TF_AllocatorRegistrationParams* params, TF_Status* status);\r\n```\r\n\r\n2) Adding them as part of the StreamExecutor C API surface, e.g.,\r\n```c++\r\n// For BFC.\r\nvoid (*create_allocator)(const SP_Platform* platform, const SP_Device* device,\r\n                         SE_CreateAllocatorParams* params, TF_Status* status);\r\nvoid (*destroy_allocator)(const SP_Platform* platform,\r\n                          const SP_Device* device, SP_Allocator* allocator);\r\nvoid (*create_allocator_fns)(const SP_Platform* platform,\r\n                             SP_AllocatorFns* allocator_fns,\r\n                             TF_Status* status);\r\nvoid (*destroy_allocator_fns)(const SP_Platform* platform,\r\n                              SP_AllocatorFns* allocator_fns);\r\n\r\n// For custom allocator.\r\nvoid (*create_custom_allocator)(const SP_Platform* platform,\r\n                                const SP_Device* device,\r\n                                SE_CreateCustomAllocatorParams* params,\r\n                                TF_Status* status);\r\nvoid (*destroy_custom_allocator)(const SP_Platform* platform,\r\n                                 const SP_Device* device,\r\n                                 SP_CustomAllocator* allocator);\r\nvoid (*create_custom_allocator_fns)(const SP_Platform* platform,\r\n                                    SP_CustomAllocatorFns* allocator_fns,\r\n                                    TF_Status* status);\r\nvoid (*destroy_custom_allocator_fns)(const SP_Platform* platform,\r\n                                     SP_CustomAllocatorFns* allocator_fns);\r\n```\r\n(Only the top 4 or the bottom 4 can be set.)\r\n\r\nIt has been put on hold for a while. I think @annarev plans to revisit this in the next few weeks. (Please correct me if I'm wrong.)\r\n\r\nLet us know if you have any preference/suggestion. :)",
        "diff_hunk": "@@ -0,0 +1,204 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstring>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_host_allocator.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pool_allocator.h\"\n+#include \"tensorflow/core/common_runtime/shared_counter.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/log_memory.h\"\n+#include \"tensorflow/core/framework/tracking_allocator.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/util/env_var.h\"\n+\n+namespace tensorflow {\n+\n+PluggableDeviceProcessState* PluggableDeviceProcessState::singleton(\n+    const string& device_type, const string& platform_name) {\n+  static std::unordered_map<string, PluggableDeviceProcessState*>\n+      process_state_map;\n+  auto iter = process_state_map.find(platform_name);\n+  if (iter != process_state_map.end()) {\n+    return iter->second;\n+  }\n+  process_state_map[platform_name] =\n+      new PluggableDeviceProcessState(device_type, platform_name);\n+  return process_state_map[platform_name];\n+}\n+\n+PluggableDeviceProcessState::PluggableDeviceProcessState(\n+    const string& device_type, const string& platform_name)\n+    : pluggable_device_enabled_(false),\n+      device_type_(device_type),\n+      platform_name_(platform_name) {\n+  process_state_ = ProcessState::singleton();\n+}\n+\n+int PluggableDeviceProcessState::BusIdForPluggableDevice(\n+    TfDeviceId tf_device_id) {\n+  // Return the NUMA node accociated with the PluggableDevice's StreamExecutor.\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  se::StreamExecutor* se = DeviceIdUtil::ExecutorForTfDeviceId(\n+                               DeviceType(device_type_), platform, tf_device_id)\n+                               .ValueOrDie();\n+  int numa_node = se->GetDeviceDescription().numa_node();\n+  // bus_id must be non-negative. If the numa_node is unknown, use 0\n+  return numa_node >= 0 ? numa_node : 0;\n+}\n+\n+Allocator* PluggableDeviceProcessState::GetPluggableDeviceAllocator(\n+    const GPUOptions& options, TfDeviceId tf_device_id, size_t total_bytes) {\n+  CHECK(process_state_);\n+  const string& allocator_type = options.allocator_type();\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  mutex_lock lock(mu_);"
      },
      {
        "body": "Nit: Please keep the underscore between words in the directory/file name. Same for other places.\r\n```suggestion\r\n#ifndef THIRD_PARTY_TENSORFLOW_CORE_COMMON_RUNTIME_PLUGGABLE_DEVICE_PLUGGABLE_DEVICE_H_\r\n```",
        "diff_hunk": "@@ -0,0 +1,114 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_PLUGGABLEDEVICE_PLUGGABLEDEVICE_H_"
      },
      {
        "body": "We don't use `CHECK` in new code anymore. Please use `DCHECK` instead. Same for other `CHECK*`s.\r\n```suggestion\r\n  DCHECK(cpu_allocator_) << \"bad place 1\";\r\n```",
        "diff_hunk": "@@ -0,0 +1,425 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <stdlib.h>\n+#include <string.h>\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <tuple>\n+#include <vector>\n+\n+#include \"tensorflow/core/common_runtime/device/device_event_mgr.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_manager.h\"\n+#include \"tensorflow/core/common_runtime/device/device_id_utils.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/common_runtime/local_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_context.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_init.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.h\"\n+#include \"tensorflow/core/common_runtime/pluggable_device/pluggable_device_util.h\"\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor.pb.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/variant_op_registry.h\"\n+#include \"tensorflow/core/graph/types.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/public/session_options.h\"\n+#include \"tensorflow/core/util/device_name_utils.h\"\n+#include \"tensorflow/core/util/env_var.h\"\n+#include \"tensorflow/core/util/stream_executor_util.h\"\n+\n+namespace tensorflow {\n+\n+// This factory helps to ensure that different PluggableDevice objects that\n+// refer to the same physical device and stream group id use the same stream\n+// group object (and therefore the same device streams). This is necessary since\n+// there is a single memory allocator per device (see\n+// ProcessState::GetPluggableDeviceAllocator) and allocators must not be shared\n+// across streams.\n+class PluggableDevice::StreamGroupFactory {\n+ public:\n+  // Returns the unique stream group for use with the stream defined by\n+  // {tf_device_id, stream_group_within_device}, creating it if it does not yet\n+  // exist.\n+  // This function is thread safe.\n+  PluggableDevice::StreamGroup* GetOrCreate(const std::string& device_type,\n+                                            TfDeviceId tf_device_id,\n+                                            int stream_group_within_device,\n+                                            se::StreamExecutor* executor,\n+                                            const GPUOptions& options) {\n+    mutex_lock guard(lock_);\n+    StreamGroup* group = &streams_[key_type(device_type, tf_device_id.value(),\n+                                            stream_group_within_device)];\n+    if (!group->compute) {\n+      group->compute = new se::Stream(executor);\n+      group->compute->Init();\n+      VLOG(2) << \"Created stream[\" << stream_group_within_device\n+              << \"] = \" << group->compute;\n+\n+      group->host_to_device = new se::Stream(executor);\n+      group->host_to_device->Init();\n+      VLOG(2) << \"Created host_to_device_stream[\" << stream_group_within_device\n+              << \"] = \" << group->host_to_device;\n+\n+      group->device_to_host = new se::Stream(executor);\n+      group->device_to_host->Init();\n+      VLOG(2) << \"Created device_to_host_stream[\" << stream_group_within_device\n+              << \"] = \" << group->device_to_host;\n+\n+      int num_d2d_streams =\n+          options.experimental().num_dev_to_dev_copy_streams();\n+      if (num_d2d_streams == 0) num_d2d_streams = 1;\n+      if (num_d2d_streams < 1 || num_d2d_streams > 4) {\n+        LOG(ERROR)\n+            << \"Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=\"\n+            << num_d2d_streams << \" set to 1 instead.\";\n+        num_d2d_streams = 1;\n+      }\n+      for (int i = 0; i < num_d2d_streams; ++i) {\n+        se::Stream* stream = new se::Stream(executor);\n+        stream->Init();\n+        group->device_to_device.push_back(stream);\n+        VLOG(2) << \"Created device_to_device_stream[\"\n+                << stream_group_within_device\n+                << \"] = \" << group->device_to_device.back();\n+      }\n+    }\n+    return group;\n+  }\n+\n+  // Returns a reference to the StreamGroupFactory singleton. Note that this is\n+  // never destroyed, so the objects it owns are never deleted.\n+  static StreamGroupFactory& Global() {\n+    static StreamGroupFactory* instance = new StreamGroupFactory();\n+    return *instance;\n+  }\n+\n+ private:\n+  mutex lock_;\n+  using key_type = std::tuple<std::string, int, int>;\n+  std::map<key_type, StreamGroup> streams_;\n+\n+  // StreamGroupFactory cannot be created directly; Call\n+  // StreamGroupFactory::Global to get the global instance.\n+  StreamGroupFactory() = default;\n+  TF_DISALLOW_COPY_AND_ASSIGN(StreamGroupFactory);\n+};\n+\n+PluggableDevice::PluggableDevice(\n+    const SessionOptions& options, const std::string& name,\n+    const std::string& device_type, const std::string& platform_name,\n+    Bytes memory_limit, const DeviceLocality& locality, TfDeviceId tf_device_id,\n+    const std::string& physical_device_desc, Allocator* device_allocator,\n+    Allocator* cpu_allocator, bool sync_every_op)\n+    : LocalDevice(options, Device::BuildDeviceAttributes(\n+                               name, device_type.c_str(), memory_limit,\n+                               locality, physical_device_desc)),\n+      device_allocator_(device_allocator),\n+      cpu_allocator_(cpu_allocator),\n+      tf_device_id_(tf_device_id),\n+      platform_name_(platform_name),\n+      sync_every_op_(sync_every_op) {\n+  if (options.config.has_gpu_options()) {\n+    force_gpu_compatible_ = options.config.gpu_options().force_gpu_compatible();\n+  }\n+  PluggableDeviceProcessState::singleton(device_type, platform_name)\n+      ->EnablePluggableDevice();\n+}\n+\n+PluggableDevice::~PluggableDevice() {\n+  delete pluggable_device_info_;\n+  device_context_->Unref();\n+}\n+\n+Status PluggableDevice::Init(const SessionOptions& options) {\n+  se::Platform* platform = PluggableDeviceMachineManager(platform_name_);\n+  auto executor_status = DeviceIdUtil::ExecutorForTfDeviceId(\n+      DeviceType(device_type()), platform, tf_device_id_);\n+  if (!executor_status.status().ok()) {\n+    return errors::Internal(\"Failed to get StreamExecutor for device\",\n+                            tf_device_id_.value());\n+  }\n+  executor_ = executor_status.ValueOrDie();\n+\n+  em_ = EventMgrFactory::Singleton()->GetEventMgr(executor_,\n+                                                  options.config.gpu_options());\n+\n+  stream_ = StreamGroupFactory::Global().GetOrCreate(\n+      device_type(), tf_device_id_, 0, executor_, options.config.gpu_options());\n+  device_context_ = new PluggableDeviceContext(\n+      0, stream_->compute, stream_->host_to_device, stream_->device_to_host,\n+      stream_->device_to_device);\n+  pluggable_device_info_ = new GpuDeviceInfo;\n+  pluggable_device_info_->stream = stream_->compute;\n+  pluggable_device_info_->default_context = device_context_;\n+  pluggable_device_info_->event_mgr = em_;\n+  PlatformDeviceId platform_device_id;\n+  TF_RETURN_IF_ERROR(DeviceIdManager::TfToPlatformDeviceId(\n+      DeviceType(device_type()), tf_device_id_, &platform_device_id));\n+  pluggable_device_info_->gpu_id = platform_device_id.value();\n+  set_tensorflow_gpu_device_info(pluggable_device_info_);\n+\n+  // Whether and how the PluggableDevice uses its own threadpool.\n+  // This option is experimental. Once we confirm the best setting, we\n+  // may change the default behavior and completely remove this flag.\n+  // Default values might change in future releases.\n+  // Possible values:\n+  //   * global: PluggableDevice uses threads shared with CPU in the main\n+  //       compute thread-pool. This is currently the default.\n+  //   * gpu_private: PluggableDevice uses threads dedicated to this device.\n+  //   * gpu_shared: All PluggableDevice s share a dedicated thread pool.\n+  string gpu_thread_mode;\n+  TF_RETURN_IF_ERROR(\n+      ReadStringFromEnvVar(\"TF_GPU_THREAD_MODE\", \"global\", &gpu_thread_mode));\n+  gpu_thread_mode = absl::AsciiStrToLower(gpu_thread_mode);\n+  if (gpu_thread_mode != \"global\") {\n+    int64 gpu_thread_count = -1;\n+    // Default to two threads. One for device compute and another for memory\n+    // copies.\n+    TF_RETURN_IF_ERROR(\n+        ReadInt64FromEnvVar(\"TF_GPU_THREAD_COUNT\", 2, &gpu_thread_count));\n+    if (gpu_thread_mode == \"gpu_private\") {\n+      thread_pool_.reset(new thread::ThreadPool(\n+          options.env, ThreadOptions(),\n+          strings::StrCat(\"gpu_private_\", tf_device_id_.value()),\n+          static_cast<int32>(gpu_thread_count),\n+          !options.config.experimental().disable_thread_spinning(),\n+          /*allocator=*/nullptr));\n+      set_tensorflow_device_thread_pool(thread_pool_.get());\n+    } else if (gpu_thread_mode == \"gpu_shared\") {\n+      static thread::ThreadPool* thread_pool = new thread::ThreadPool(\n+          options.env, ThreadOptions(), \"gpu_shared\",\n+          static_cast<int32>(gpu_thread_count),\n+          !options.config.experimental().disable_thread_spinning(),\n+          /*allocator=*/nullptr);\n+      set_tensorflow_device_thread_pool(thread_pool);\n+    } else {\n+      string error_message =\n+          strings::StrCat(\"Invalid gpu_thread_mode: \", gpu_thread_mode);\n+      LOG(WARNING) << error_message;\n+      return errors::InvalidArgument(error_message);\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Allocator* PluggableDevice::GetAllocator(AllocatorAttributes attr) {\n+  CHECK(cpu_allocator_) << \"bad place 1\";"
      },
      {
        "body": "Line 814 moves the variable `platform`. We should read `platform.type` and `platform.name` before then.",
        "diff_hunk": "@@ -806,16 +809,15 @@ port::Status InitStreamExecutorPlugin(SEInitPluginFn init_fn) {\n   TF_RETURN_IF_ERROR(ValidateSPTimerFns(timer_fns));\n \n   // Register new platform\n-  std::string platform_name = std::string(platform.name);\n   std::unique_ptr<stream_executor::CPlatform> cplatform(\n       new stream_executor::CPlatform(\n           std::move(platform), params.destroy_platform, std::move(platform_fns),\n           params.destroy_platform_fns, std::move(device_fns), std::move(se),\n           std::move(timer_fns)));\n   SE_CHECK_OK(stream_executor::MultiPlatformManager::RegisterPlatform(\n       std::move(cplatform)));\n-\n-  // TODO(annarev): Add pluggable device registration here.\n+  *device_type = std::string(platform.type);\n+  *platform_name = std::string(platform.name);"
      }
    ],
    "body": "Add PluggableDevice mechanism implementation, including plugin initialization, PluggableDevice create and compute, and also add a new flag(is_pluggable_device) in DeviceFactory to do some low-level device specialization. ",
    "timestamp": "2025-05-06 01:27:10"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/1771",
    "comments": [
      "Can one of the admins verify this patch?\n",
      "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n",
      "I signed it!\n",
      "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n",
      "@tmc are you ok with this PR?\n",
      "@tmc, @dave-andersen, please take a look. \n\nI think high level documentation should live in contrib/go/g3doc, we're working on glue to integrate such docs into the website.\n\nWe can look into generating markdown from godoc, maybe? I don't know the first thing about go, or godoc, so stop me if this is stupid. Our publishing to website pipeline goes via md, so that would be easiest, probably, assuming we want to have documentation for the go part on tensorflow.org.\n",
      "Hi @martinwicke , I'll prepare the docu generation using godocdown, I've never used this project before but it seems to be easy to define a template to keep a similar to the one you are using, so the generation can be automatised in an easy way :)\nI'll work on that today after work and also on the Variables generation.\n",
      "I'd like to get @tmc approval first before we go too far.\n",
      "ok, I've sent an e-mail to @tmc yesterday, perhaps he is not getting the GitHub notifications.\n",
      "Can one of the admins verify this patch?\n",
      "I added a template to generate automatically the documentation as MarkDown using godoc. The files inside contrib/go/g3doc are being generated from the comments on the source code using 'go generate'.\n\nI also added the example_test.go with some usage examples for the most important methods.\nThe index.md is the only one that is not being automatically generated since it also contains more complex examples, installation and so on, this is: [README.md](https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/contrib/go/README.md)\n",
      "@tmc can you chime in and OK this?\n",
      "Because @tmc has signed the CLA, I approve this import of his patch with modification.  Could you go ahead and sign the CLA also, Alonso?\n\nI'm fine leaving the docs in godoc format (confirmed with @martinwicke ) or as .md, since this will be in contrib/ for the time being.\n\nReviewing now.\n",
      "Hi @dave-andersen I already singed the CLA. The docs are now in MarkDown and godoc, I added a template [here](https://github.com/alonsovidales/tensorflow/tree/go_bindings_tensors/tensorflow/contrib/go/godoc_tmpl) to generate the documentation from the source code comments in MarkDown, the result is [here](https://github.com/alonsovidales/tensorflow/tree/go_bindings_tensors/tensorflow/contrib/go/g3doc). Run `go generate` creates this MarkDown.\n",
      "Cool re CLA.  @googlebot signed.  (let's see if that works to get the status updated. :)\n",
      "A quick global pass through comments to make sure they're all terminated with a period would be good in  example_test.go.  It's a bit inconsistent.\n",
      "I think that the problem with the CLA comes from the @tmc commits, I've sent time ago the 'I signed it!'. The @tmc commits have 'Error CLAs are signed, but unable to verify author consent'\n",
      "@martinwicke - can you bring your expertise to bear on this CLA problem? :)\n",
      "Thanks @dave-andersen for the code review! :)\nI've modified the code according to the comments.\n\nI'm currently working on allocating the content of the '/usr/local/tensorlow/ops.pbtxt' file inside the Go code. I think that with this modification this could be more or less ready for this first iteration. If I have time this week I'll try to add the code to save & restore Variables from disk like tf.train.Saver does.\n",
      "Want to pop in and say this looks very exciting.  Thanks for working on it!\n",
      "Sorry about going AWOL on this, I've been very busy. Really happy to see the progress here. I'm reviewing now (and fixed up my google CLA settings).\n",
      "@alonsovidales I've done an initial pass and have some style nits, looks like @dave-andersen is on the case too. This is looking great but let's be very careful with the api we expose.\n",
      "Done with comments - thanks for putting up with the flood of them combined with the fairly painful github comment mechanism. :)\n",
      "(btw, I agree with tmc about being careful with the API.  From the TensorFlow team perspective, this is in \"contrib\", so we're allowed to change it at will, but if people start using it seriously, it'll become implicitly harder to change.  Once the nits are addressed I'll take a final peek from a bigger picture API perspective and then we should be good to go.\n\nI'm more concerned _for now_ about the serving API, because I think that's the most obviously clear match for Go+TF, but your image recognition example shows a very nice reason for having some graph construction integrated with the Go code as well -- though that could also be baked into the Inception graph.)\n",
      "Wow, what a code review! :) Thanks a lot @dave-andersen , @tmc ! This is maybe one of the best code reviews I had in the last years \uff61^\u203f^\uff61\nI'm going to sleep, that in Netherlands is a bit late. I went through almost all the comments but I still have to finish:\n- Add: NewGraphFromReader\n- Use a template for the getTensorFromGraph input string in order to avoid this redundancy\n- C++ Code review\n- Apply fixes to: tensorflow/g3doc/tutorials/image_recognition/index.md\n\nAnd I want to go through all the code once again to be sure that I'm not forgetting anything, I hope I can have all ready for next week :)\n",
      "I have been working during this weekend on fixing the code based on the feedback from the code reviews.\nThis is the changelog of what has been modified:\n- Fixed typos, test messages, etc.\n- Replaced t.Error + t.FailNow by t.Fatal\n- Replaced most of the naked returns\n- Removed some duplicated code from the test suite removing redundant tests and adding some templates.\n- Modified AsBool, AsFloat32, \u2026 by Bool, Float32, \u2026\n- Replaced NewGraphFromText and LoadGraphFromFile by NewGraphFromReader\n- Replaced the DataType Dt\\* by DT*\n- Added auto-generation of the probobuf files as also the ops definition file\n- Replaced example image by:  tensorflow/./examples/label_image/data/grace_hopper.jpg and removed the Ceres image from the repo\n- TensorInt -> TensorInterface\n\nPending questions:\n- Should we use the DT\\* as prefix for DataTypes?\n- Should we remove the TensorInterface, or add interfaces for Session, Graph, etc? I think that is better to add the interfaces to make more easy the definition of Mocks for testing proposals.\n- Why the dimensions of a Tensor are represented as a matrix instead of using a vector?\n- Should we add GetValInt32, GetValFloat64, etc as helpers for the current GetVal method?\n- Sould we add/remove some methods to the API?, I think that for a first iteration is enugh, it could be nice to have other stuff implemented like the [tf.train.Saver](https://www.tensorflow.org/versions/r0.8/api_docs/python/state_ops.html#Saver) but this could be added in a future.\n\nI think that could be a good idea if we send an e-mail to some groups like golang-nuts@googlegroups.com or discuss@tensorflow.org to try to get more feedback, what do you think?\n\n@dave-andersen @vrv @tmc\n",
      "There are still a number of named returns that IMO need justification.\n",
      "@tmc I just replaced the remaining naked returns :)\nI'm only leaving this two:\n- https://github.com/tensorflow/tensorflow/pull/1771/files#diff-6ecada9bdae40f6b8d2258c71de75c99R653\n- https://github.com/tensorflow/tensorflow/pull/1771/files#diff-6ecada9bdae40f6b8d2258c71de75c99R661\n\nFor this two particular cases is obvious that the idea is to return just the error and there are multiple params to be returned.\n",
      "I'm not convinced these named result parameters are helpful:\n\n``` go\n    func NewGraphFromReader(reader io.Reader, asText bool) (gr *Graph, err error)\n    func (gr *Graph) Constant(name string, data interface{}) (op *GraphNode, err error)\n    func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error)\n```\n\nvs\n\n``` go\n    func NewGraphFromReader(reader io.Reader, asText bool) (*Graph, error)\n    func (gr *Graph) Constant(name string, data interface{}) (*GraphNode, error)\n    func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (*GraphNode, error)\n```\n",
      "Re questions about adding more to the API:  I'd strongly suggest against it.  There's a lot in here already, and I think it would be useful to get it in to contrib/ and let people play around with it to get experience with what's really wanted.  If everyone starts screaming that they need saver, it's easy to add later - but harder to remove if we add it now.\n"
    ],
    "review_comments": [
      {
        "body": "writing:  TensorFlow.  This package\n'as also to' -> and also to\n",
        "diff_hunk": "@@ -1,20 +1,16 @@\n # TensorFlow for Go\n \n This package provides a high-level Go API for TensorFlow, this package"
      },
      {
        "body": "provided API.\n\nnext Python code:\n",
        "diff_hunk": "@@ -0,0 +1,163 @@\n+# TensorFlow for Go\n+\n+This package provides a high-level Go API for TensorFlow, this package\n+provides the necessary tools to create and manipulate Tensors, Variables,\n+Constants as also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API\n+\n+In order to generate a valid Graph you can use the next Python code"
      },
      {
        "body": "'respectively and other' -> respectively, and another tensor used as the output ...\n",
        "diff_hunk": "@@ -0,0 +1,163 @@\n+# TensorFlow for Go\n+\n+This package provides a high-level Go API for TensorFlow, this package\n+provides the necessary tools to create and manipulate Tensors, Variables,\n+Constants as also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API\n+\n+In order to generate a valid Graph you can use the next Python code\n+\n+```python\n+import tensorflow as tf\n+\n+input1 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input1')\n+input2 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input2')\n+output = tf.add(input1, input2, name='output')\n+\n+with tf.Session() as sess:\n+    tf.train.write_graph(sess.graph_def, '/tmp/graph/', 'test_graph.pb', as_text=True)\n+```\n+\n+The previous code prepares two placeholders with names 'input1' and\n+'input2' respectively and other tensor used as output of the addition of the"
      },
      {
        "body": "this looks like it got weirdly line-wrapped and lost its initial comment (or maybe it's just github formatting)\n",
        "diff_hunk": "@@ -0,0 +1,163 @@\n+# TensorFlow for Go\n+\n+This package provides a high-level Go API for TensorFlow, this package\n+provides the necessary tools to create and manipulate Tensors, Variables,\n+Constants as also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API\n+\n+In order to generate a valid Graph you can use the next Python code\n+\n+```python\n+import tensorflow as tf\n+\n+input1 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input1')\n+input2 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input2')\n+output = tf.add(input1, input2, name='output')\n+\n+with tf.Session() as sess:\n+    tf.train.write_graph(sess.graph_def, '/tmp/graph/', 'test_graph.pb', as_text=True)\n+```\n+\n+The previous code prepares two placeholders with names 'input1' and\n+'input2' respectively and other tensor used as output of the addition of the\n+two placeholders. At the end, it dumps the graph as text into a text file with\n+path:\n+'/tmp/graph/test_graph.pb'.\n+\n+From a Go application, you can use the next code to execute the graph:\n+\n+```go\n+package main\n+\n+import (\n+\t\"log\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func main() {\n+\t// These are the input tensors to be used\n+\tinputSlice1 := [][][]int64{\n+\t\t{\n+\t\t\t{1, 2},\n+\t\t\t{3, 4},\n+\t\t}, {\n+\t\t\t{5, 6},\n+\t\t\t{7, 8},\n+\t\t},\n+\t}\n+\tinputSlice2 := [][][]int64{\n+\t\t{\n+\t\t\t{9, 10},\n+\t\t\t{11, 12},\n+\t\t}, {\n+\t\t\t{13, 14},\n+\t\t\t{15, 16},\n+\t\t},\n+\t}\n+\n+\t// Create the two tensors, the data type is recognized automatically as\n+\t// also the tensor shape from the input slice\n+\tt1, err := tensorflow.NewTensor(inputSlice1)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\tt2, err := tensorflow.NewTensor(inputSlice2)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\t// Load the graph from the file that we generated from Python on the\n+\tprevious step"
      },
      {
        "body": "s/As you can see from//   (-->  \"The previous code ...\")\n",
        "diff_hunk": "@@ -0,0 +1,163 @@\n+# TensorFlow for Go\n+\n+This package provides a high-level Go API for TensorFlow, this package\n+provides the necessary tools to create and manipulate Tensors, Variables,\n+Constants as also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API\n+\n+In order to generate a valid Graph you can use the next Python code\n+\n+```python\n+import tensorflow as tf\n+\n+input1 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input1')\n+input2 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input2')\n+output = tf.add(input1, input2, name='output')\n+\n+with tf.Session() as sess:\n+    tf.train.write_graph(sess.graph_def, '/tmp/graph/', 'test_graph.pb', as_text=True)\n+```\n+\n+The previous code prepares two placeholders with names 'input1' and\n+'input2' respectively and other tensor used as output of the addition of the\n+two placeholders. At the end, it dumps the graph as text into a text file with\n+path:\n+'/tmp/graph/test_graph.pb'.\n+\n+From a Go application, you can use the next code to execute the graph:\n+\n+```go\n+package main\n+\n+import (\n+\t\"log\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func main() {\n+\t// These are the input tensors to be used\n+\tinputSlice1 := [][][]int64{\n+\t\t{\n+\t\t\t{1, 2},\n+\t\t\t{3, 4},\n+\t\t}, {\n+\t\t\t{5, 6},\n+\t\t\t{7, 8},\n+\t\t},\n+\t}\n+\tinputSlice2 := [][][]int64{\n+\t\t{\n+\t\t\t{9, 10},\n+\t\t\t{11, 12},\n+\t\t}, {\n+\t\t\t{13, 14},\n+\t\t\t{15, 16},\n+\t\t},\n+\t}\n+\n+\t// Create the two tensors, the data type is recognized automatically as\n+\t// also the tensor shape from the input slice\n+\tt1, err := tensorflow.NewTensor(inputSlice1)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\tt2, err := tensorflow.NewTensor(inputSlice2)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\t// Load the graph from the file that we generated from Python on the\n+\tprevious step\n+\tgraph, err := tensorflow.LoadGraphFromTextFile(\"/tmp/graph/test_graph.pb\")\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem reading the graph from the text file, Error:\", err)\n+\t}\n+\n+\t// Create the session and extend the Graph\n+\ts, err := tensorflow.NewSession()\n+\tif err := s.ExtendGraph(graph); err != nil {\n+\t\tlog.Fatal(\"Problem extending the Graph, Error:\", err)\n+\t}\n+\n+\tinput := map[string]*tensorflow.Tensor{\n+\t\t\"input1\": t1,\n+\t\t\"input2\": t2,\n+\t}\n+\t// Execute the graph with the two input tensors, and specify the names\n+\t// of the tensors to be returned, for this case just one\n+\tout, err := s.Run(input, []string{\"output\"}, nil)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying to run the saved graph, Error:\", err)\n+\t}\n+\n+\tif len(out) != 1 {\n+\t\tlog.Fatalf(\"The expected number of outputs is 1 but: %d returned\", len(out))\n+\t}\n+\n+\toutputTensor := out[0]\n+\tfor x := 0; x < outputTensor.Dim(0); x++ {\n+\t\tfor y := 0; y < outputTensor.Dim(1); y++ {\n+\t\t\tfor z := 0; z < outputTensor.Dim(2); z++ {\n+\t\t\t\t// Using GetVal we can access to the corresponding positions of\n+\t\t\t\t// the tensor as if we had been accessing to the positions in a\n+\t\t\t\t// multidimensional array, for instance GetVal(1, 2, 3) is\n+\t\t\t\t// equivalent to array[1][2][3] on a three dimensional array\n+\t\t\t\tval, err := out[0].GetVal(x, y, z)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tlog.Fatal(\"Error trying to read the output tensor, Error:\", err)\n+\t\t\t\t}\n+\t\t\t\tif val != inputSlice1[x][y][z]+inputSlice2[x][y][z] {\n+\t\t\t\t\tlog.Printf(\n+\t\t\t\t\t\t\"The sum of the two elements: %d + %d doesn't match with the returned value: %d\",\n+\t\t\t\t\t\tinputSlice1[x][y][z], inputSlice2[x][y][z], val)\n+\t\t\t\t}\n+\n+\t\t\t\tlog.Println(\"The value value on coordinates:\", x, y, z, \"is:\", val)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+```\n+\n+As you can see from the previous code creates two Tensors to be processed by the"
      },
      {
        "body": "eliminate word 'really' - complete is sufficient. ;)\n",
        "diff_hunk": "@@ -0,0 +1,163 @@\n+# TensorFlow for Go\n+\n+This package provides a high-level Go API for TensorFlow, this package\n+provides the necessary tools to create and manipulate Tensors, Variables,\n+Constants as also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API\n+\n+In order to generate a valid Graph you can use the next Python code\n+\n+```python\n+import tensorflow as tf\n+\n+input1 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input1')\n+input2 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input2')\n+output = tf.add(input1, input2, name='output')\n+\n+with tf.Session() as sess:\n+    tf.train.write_graph(sess.graph_def, '/tmp/graph/', 'test_graph.pb', as_text=True)\n+```\n+\n+The previous code prepares two placeholders with names 'input1' and\n+'input2' respectively and other tensor used as output of the addition of the\n+two placeholders. At the end, it dumps the graph as text into a text file with\n+path:\n+'/tmp/graph/test_graph.pb'.\n+\n+From a Go application, you can use the next code to execute the graph:\n+\n+```go\n+package main\n+\n+import (\n+\t\"log\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func main() {\n+\t// These are the input tensors to be used\n+\tinputSlice1 := [][][]int64{\n+\t\t{\n+\t\t\t{1, 2},\n+\t\t\t{3, 4},\n+\t\t}, {\n+\t\t\t{5, 6},\n+\t\t\t{7, 8},\n+\t\t},\n+\t}\n+\tinputSlice2 := [][][]int64{\n+\t\t{\n+\t\t\t{9, 10},\n+\t\t\t{11, 12},\n+\t\t}, {\n+\t\t\t{13, 14},\n+\t\t\t{15, 16},\n+\t\t},\n+\t}\n+\n+\t// Create the two tensors, the data type is recognized automatically as\n+\t// also the tensor shape from the input slice\n+\tt1, err := tensorflow.NewTensor(inputSlice1)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\tt2, err := tensorflow.NewTensor(inputSlice2)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\t// Load the graph from the file that we generated from Python on the\n+\tprevious step\n+\tgraph, err := tensorflow.LoadGraphFromTextFile(\"/tmp/graph/test_graph.pb\")\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem reading the graph from the text file, Error:\", err)\n+\t}\n+\n+\t// Create the session and extend the Graph\n+\ts, err := tensorflow.NewSession()\n+\tif err := s.ExtendGraph(graph); err != nil {\n+\t\tlog.Fatal(\"Problem extending the Graph, Error:\", err)\n+\t}\n+\n+\tinput := map[string]*tensorflow.Tensor{\n+\t\t\"input1\": t1,\n+\t\t\"input2\": t2,\n+\t}\n+\t// Execute the graph with the two input tensors, and specify the names\n+\t// of the tensors to be returned, for this case just one\n+\tout, err := s.Run(input, []string{\"output\"}, nil)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying to run the saved graph, Error:\", err)\n+\t}\n+\n+\tif len(out) != 1 {\n+\t\tlog.Fatalf(\"The expected number of outputs is 1 but: %d returned\", len(out))\n+\t}\n+\n+\toutputTensor := out[0]\n+\tfor x := 0; x < outputTensor.Dim(0); x++ {\n+\t\tfor y := 0; y < outputTensor.Dim(1); y++ {\n+\t\t\tfor z := 0; z < outputTensor.Dim(2); z++ {\n+\t\t\t\t// Using GetVal we can access to the corresponding positions of\n+\t\t\t\t// the tensor as if we had been accessing to the positions in a\n+\t\t\t\t// multidimensional array, for instance GetVal(1, 2, 3) is\n+\t\t\t\t// equivalent to array[1][2][3] on a three dimensional array\n+\t\t\t\tval, err := out[0].GetVal(x, y, z)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tlog.Fatal(\"Error trying to read the output tensor, Error:\", err)\n+\t\t\t\t}\n+\t\t\t\tif val != inputSlice1[x][y][z]+inputSlice2[x][y][z] {\n+\t\t\t\t\tlog.Printf(\n+\t\t\t\t\t\t\"The sum of the two elements: %d + %d doesn't match with the returned value: %d\",\n+\t\t\t\t\t\tinputSlice1[x][y][z], inputSlice2[x][y][z], val)\n+\t\t\t\t}\n+\n+\t\t\t\tlog.Println(\"The value value on coordinates:\", x, y, z, \"is:\", val)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+```\n+\n+As you can see from the previous code creates two Tensors to be processed by the\n+previously generated Graph, after the execution returns the Tensor with the\n+result.\n+\n+### Image Recognition\n+\n+This is a really complete code example that shows how to generate Graphs on\n+Go and execute them:"
      },
      {
        "body": "s/elements/element/\n",
        "diff_hunk": "@@ -0,0 +1,184 @@\n+package tensorflow_test\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func ExampleGraph_Op() {\n+\tvar out []*tensorflow.Tensor\n+\n+\tadditions := 10\n+\tinputSlice1 := []int32{1, 2, 3, 4}\n+\tinputSlice2 := []int32{5, 6, 7, 8}\n+\n+\tgraph := tensorflow.NewGraph()\n+\tinput1, _ := graph.Variable(\"input1\", inputSlice1)\n+\tinput2, _ := graph.Constant(\"input2\", inputSlice2)\n+\n+\tadd, _ := graph.Op(\"Add\", \"add_tensors\", []*tensorflow.GraphNode{input1, input2}, \"\", map[string]interface{}{})\n+\tgraph.Op(\"Assign\", \"assign_inp1\", []*tensorflow.GraphNode{input1, add}, \"\", map[string]interface{}{})\n+\n+\ts, _ := tensorflow.NewSession()\n+\ts.ExtendAndInitializeAllVariables(graph)\n+\n+\tfor i := 0; i < additions; i++ {\n+\t\tout, _ = s.Run(nil, []string{\"input1\"}, []string{\"assign_inp1\"})\n+\t}\n+\n+\tfor i := 0; i < len(inputSlice1); i++ {\n+\t\tval, _ := out[0].GetVal(i)\n+\t\tfmt.Println(\"The result of the operation: %d + (%d*%d) is: %d\", inputSlice1[i], inputSlice2[i], additions, val)\n+\t}\n+}\n+\n+func ExampleNewTensor() {\n+\ttensorflow.NewTensor([][]int64{\n+\t\t{1, 2, 3, 4},\n+\t\t{5, 6, 7, 8},\n+\t})\n+}\n+\n+func ExampleNewGraphFromText() {\n+\tgraph, err := tensorflow.NewGraphFromText(`\n+\t\tnode {\n+\t\t\tname: \"output\"\n+\t\t\top: \"Const\"\n+\t\t\tattr {\n+\t\t\t\tkey: \"dtype\"\n+\t\t\t\tvalue {\n+\t\t\t\t\ttype: DT_FLOAT\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tattr {\n+\t\t\t\tkey: \"value\"\n+\t\t\t\tvalue {\n+\t\t\t\t\ttensor {\n+\t\t\t\t\t\tdtype: DT_FLOAT\n+\t\t\t\t\t\ttensor_shape {\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tfloat_val: 1.5 \n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tversion: 5`)\n+\n+\tfmt.Println(graph, err)\n+}\n+\n+func ExampleGraph_Constant() {\n+\tgraph := tensorflow.NewGraph()\n+\t// Adds a scalar string to the graph with named 'const1'.\n+\tgraph.Constant(\"const1\", \"this is a test...\")\n+\n+\t// Adds a bidimensional constant to the graph named 'const2'.\n+\tgraph.Constant(\"const2\", [][]int64{\n+\t\t{1, 2},\n+\t\t{3, 4},\n+\t})\n+}\n+\n+func ExampleGraph_Placeholder() {\n+\tgraph := tensorflow.NewGraph()\n+\t// Adds a placeholder named \"input1\" that must allocate a three elements"
      },
      {
        "body": "s/variable/variables/ ;\ns/on/in/\n",
        "diff_hunk": "@@ -0,0 +1,184 @@\n+package tensorflow_test\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func ExampleGraph_Op() {\n+\tvar out []*tensorflow.Tensor\n+\n+\tadditions := 10\n+\tinputSlice1 := []int32{1, 2, 3, 4}\n+\tinputSlice2 := []int32{5, 6, 7, 8}\n+\n+\tgraph := tensorflow.NewGraph()\n+\tinput1, _ := graph.Variable(\"input1\", inputSlice1)\n+\tinput2, _ := graph.Constant(\"input2\", inputSlice2)\n+\n+\tadd, _ := graph.Op(\"Add\", \"add_tensors\", []*tensorflow.GraphNode{input1, input2}, \"\", map[string]interface{}{})\n+\tgraph.Op(\"Assign\", \"assign_inp1\", []*tensorflow.GraphNode{input1, add}, \"\", map[string]interface{}{})\n+\n+\ts, _ := tensorflow.NewSession()\n+\ts.ExtendAndInitializeAllVariables(graph)\n+\n+\tfor i := 0; i < additions; i++ {\n+\t\tout, _ = s.Run(nil, []string{\"input1\"}, []string{\"assign_inp1\"})\n+\t}\n+\n+\tfor i := 0; i < len(inputSlice1); i++ {\n+\t\tval, _ := out[0].GetVal(i)\n+\t\tfmt.Println(\"The result of the operation: %d + (%d*%d) is: %d\", inputSlice1[i], inputSlice2[i], additions, val)\n+\t}\n+}\n+\n+func ExampleNewTensor() {\n+\ttensorflow.NewTensor([][]int64{\n+\t\t{1, 2, 3, 4},\n+\t\t{5, 6, 7, 8},\n+\t})\n+}\n+\n+func ExampleNewGraphFromText() {\n+\tgraph, err := tensorflow.NewGraphFromText(`\n+\t\tnode {\n+\t\t\tname: \"output\"\n+\t\t\top: \"Const\"\n+\t\t\tattr {\n+\t\t\t\tkey: \"dtype\"\n+\t\t\t\tvalue {\n+\t\t\t\t\ttype: DT_FLOAT\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tattr {\n+\t\t\t\tkey: \"value\"\n+\t\t\t\tvalue {\n+\t\t\t\t\ttensor {\n+\t\t\t\t\t\tdtype: DT_FLOAT\n+\t\t\t\t\t\ttensor_shape {\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tfloat_val: 1.5 \n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tversion: 5`)\n+\n+\tfmt.Println(graph, err)\n+}\n+\n+func ExampleGraph_Constant() {\n+\tgraph := tensorflow.NewGraph()\n+\t// Adds a scalar string to the graph with named 'const1'.\n+\tgraph.Constant(\"const1\", \"this is a test...\")\n+\n+\t// Adds a bidimensional constant to the graph named 'const2'.\n+\tgraph.Constant(\"const2\", [][]int64{\n+\t\t{1, 2},\n+\t\t{3, 4},\n+\t})\n+}\n+\n+func ExampleGraph_Placeholder() {\n+\tgraph := tensorflow.NewGraph()\n+\t// Adds a placeholder named \"input1\" that must allocate a three elements\n+\t// DtInt32 tensor.\n+\tgraph.Placeholder(\"input1\", tensorflow.DtInt32, []int64{3}, []string{})\n+}\n+\n+func ExampleGraph_Variable() {\n+\tvar out []*tensorflow.Tensor\n+\n+\tgraph := tensorflow.NewGraph()\n+\t// Create a Variable that will be used as input and also as storage of\n+\t// the result on every execution.\n+\tinput1, _ := graph.Variable(\"input1\", []int32{1, 2, 3, 4})\n+\tinput2, _ := graph.Constant(\"input2\", []int32{5, 6, 7, 8})\n+\n+\t// Add the two inputs.\n+\tadd, _ := graph.Op(\"Add\", \"add_tensors\", []*tensorflow.GraphNode{input1, input2}, \"\", map[string]interface{}{})\n+\t// Store the result on the input1 varable\n+\tgraph.Op(\"Assign\", \"assign_inp1\", []*tensorflow.GraphNode{input1, add}, \"\", map[string]interface{}{})\n+\n+\ts, _ := tensorflow.NewSession()\n+\t// Initialize all the variable in memory, on this case only the\n+\t// 'input1' variable."
      },
      {
        "body": "it inputs -> its input\n",
        "diff_hunk": "@@ -0,0 +1,184 @@\n+package tensorflow_test\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func ExampleGraph_Op() {\n+\tvar out []*tensorflow.Tensor\n+\n+\tadditions := 10\n+\tinputSlice1 := []int32{1, 2, 3, 4}\n+\tinputSlice2 := []int32{5, 6, 7, 8}\n+\n+\tgraph := tensorflow.NewGraph()\n+\tinput1, _ := graph.Variable(\"input1\", inputSlice1)\n+\tinput2, _ := graph.Constant(\"input2\", inputSlice2)\n+\n+\tadd, _ := graph.Op(\"Add\", \"add_tensors\", []*tensorflow.GraphNode{input1, input2}, \"\", map[string]interface{}{})\n+\tgraph.Op(\"Assign\", \"assign_inp1\", []*tensorflow.GraphNode{input1, add}, \"\", map[string]interface{}{})\n+\n+\ts, _ := tensorflow.NewSession()\n+\ts.ExtendAndInitializeAllVariables(graph)\n+\n+\tfor i := 0; i < additions; i++ {\n+\t\tout, _ = s.Run(nil, []string{\"input1\"}, []string{\"assign_inp1\"})\n+\t}\n+\n+\tfor i := 0; i < len(inputSlice1); i++ {\n+\t\tval, _ := out[0].GetVal(i)\n+\t\tfmt.Println(\"The result of the operation: %d + (%d*%d) is: %d\", inputSlice1[i], inputSlice2[i], additions, val)\n+\t}\n+}\n+\n+func ExampleNewTensor() {\n+\ttensorflow.NewTensor([][]int64{\n+\t\t{1, 2, 3, 4},\n+\t\t{5, 6, 7, 8},\n+\t})\n+}\n+\n+func ExampleNewGraphFromText() {\n+\tgraph, err := tensorflow.NewGraphFromText(`\n+\t\tnode {\n+\t\t\tname: \"output\"\n+\t\t\top: \"Const\"\n+\t\t\tattr {\n+\t\t\t\tkey: \"dtype\"\n+\t\t\t\tvalue {\n+\t\t\t\t\ttype: DT_FLOAT\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tattr {\n+\t\t\t\tkey: \"value\"\n+\t\t\t\tvalue {\n+\t\t\t\t\ttensor {\n+\t\t\t\t\t\tdtype: DT_FLOAT\n+\t\t\t\t\t\ttensor_shape {\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tfloat_val: 1.5 \n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tversion: 5`)\n+\n+\tfmt.Println(graph, err)\n+}\n+\n+func ExampleGraph_Constant() {\n+\tgraph := tensorflow.NewGraph()\n+\t// Adds a scalar string to the graph with named 'const1'.\n+\tgraph.Constant(\"const1\", \"this is a test...\")\n+\n+\t// Adds a bidimensional constant to the graph named 'const2'.\n+\tgraph.Constant(\"const2\", [][]int64{\n+\t\t{1, 2},\n+\t\t{3, 4},\n+\t})\n+}\n+\n+func ExampleGraph_Placeholder() {\n+\tgraph := tensorflow.NewGraph()\n+\t// Adds a placeholder named \"input1\" that must allocate a three elements\n+\t// DtInt32 tensor.\n+\tgraph.Placeholder(\"input1\", tensorflow.DtInt32, []int64{3}, []string{})\n+}\n+\n+func ExampleGraph_Variable() {\n+\tvar out []*tensorflow.Tensor\n+\n+\tgraph := tensorflow.NewGraph()\n+\t// Create a Variable that will be used as input and also as storage of\n+\t// the result on every execution.\n+\tinput1, _ := graph.Variable(\"input1\", []int32{1, 2, 3, 4})\n+\tinput2, _ := graph.Constant(\"input2\", []int32{5, 6, 7, 8})\n+\n+\t// Add the two inputs.\n+\tadd, _ := graph.Op(\"Add\", \"add_tensors\", []*tensorflow.GraphNode{input1, input2}, \"\", map[string]interface{}{})\n+\t// Store the result on the input1 varable\n+\tgraph.Op(\"Assign\", \"assign_inp1\", []*tensorflow.GraphNode{input1, add}, \"\", map[string]interface{}{})\n+\n+\ts, _ := tensorflow.NewSession()\n+\t// Initialize all the variable in memory, on this case only the\n+\t// 'input1' variable.\n+\ts.ExtendAndInitializeAllVariables(graph)\n+\n+\t// Runs ten times the 'assign_inp1\"' that will run also the 'Add'\n+\t// operation since it inputs depends on the result of the 'Add'"
      },
      {
        "body": "missiong period:  Graph.  A\n",
        "diff_hunk": "@@ -0,0 +1,607 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph a"
      },
      {
        "body": "Comment is a bit unclear about whether the Op() function performs the internal check, or whether the operation being added does.  I think you mean \"This function performs\", right?\n\n\"try to deduct\" -> \"try to deduce\" \n\"in case of\" -> \"in case\"\n",
        "diff_hunk": "@@ -0,0 +1,607 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph a\n+// node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this"
      },
      {
        "body": "I think it would really clarify this code to reverse the test here.  If the op is _not_ in the list of available ops, immediately return the ErrOperationNotFound error.  Otherwise, proceed into the function as before.  (This saves you a lot of unnecessary indentation and makes the error vs normal case a little more clear.)\n",
        "diff_hunk": "@@ -0,0 +1,607 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph a\n+// node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this\n+// operation performs some internal check of the specified and expected\n+// attributes for the operation and try to deduct the corresponding DataTypes\n+// in case of they are not specified.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\tif op, ok := gr.availableOps[strings.ToLower(opName)]; ok {"
      },
      {
        "body": "Yes, the comment is a bit confusing I've modified it by:\n\n> Op Adds a new Node to the Graph with the specified operation, this function could return an error if any of the mandatory attributes is not be present or the value is not the expected for this attribute.\n\nRemoving the reference to the auto DataType deduction that, I think, is not necessary to comment. If you don't specify the datatype the function is able to deduct it from the input or output tensors.\n",
        "diff_hunk": "@@ -0,0 +1,607 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph a\n+// node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this"
      },
      {
        "body": "s/don't/doesn't/\n",
        "diff_hunk": "@@ -0,0 +1,605 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tvar op *pb.OpDef\n+\tvar opFound bool\n+\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\tif op, opFound = gr.availableOps[strings.ToLower(opName)]; !opFound {\n+\t\terr = &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\terr = &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t\treturn\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\terr = &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and don't have a default value, return an error"
      },
      {
        "body": "I think this function would read a lot more cleanly internally if it were to use explicit return values.  For example, in the \"operation not found case\", this is both more concise and more clear:\n  if .... !opFound {\n         return gr, &ErrOperationNotFound{ op: opName }\n  }\n\nit makes it a little more clear that you're returning an error.  I think the repeated savings of this over the course of the function makes it worth the slight redundancy of having to note the 'gr' on every return.  This is a pretty big function, so it's reasonable to remind the reader what the return values are when they're 40 lines into it.  There are also some cases inside the code that already explicitly specify the return values, and the mix gets kind of confusing. \n\nIt would also be worth checking in some of these cast checks if they read better with an integrated conditional, e.g., for this code:\ndt, ok := v.(DataType)\n-              if !ok {\n-                  return nil, &ErrInvalidAttrValue{\n-                      operation:  opName,\n-                      attribName: attr.Name,\n-                  }\n-              }\n-              node.def.Attr[attr.Name] = &pb.AttrValue{\n-                  Value: &pb.AttrValue_Type{\n-                      Type: pb.DataType(dt),\n-                  },\n-              }\n\nWould this be more clear?\nif dt, ok := v.(DataType); !ok {\n   return nil, &ErrInvalidAttrValue{ ...}\n} else {\n   ...\n}\n\nI'm not 100% on this latter, so I leave it to you to see which reads better.\n",
        "diff_hunk": "@@ -0,0 +1,605 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {"
      },
      {
        "body": "I agree with you, I'm going to use explicit return values for this method. This is more a personal preference in Go, if you are changing the function definition really often, the params that you are returning, etc, assign and return makes more easy to work with the code so you don't need to change all the returned values every time the returned params changes, but since here we are returning mostly errors generated inside this function I think that is better if the error and the return goes together.\n\nI also changed the assignations and conditionals in the same line, the indentation was super ugly, I don't know what I was thinking when I did that \u00af_(\u30c4)_/\u00af\n\nPerhaps I'm going to split this in two, the logic for the operation and the logic for the attribute values casting.\n",
        "diff_hunk": "@@ -0,0 +1,605 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {"
      },
      {
        "body": "Yep, after split the Op function I removed a lot of duplicated code and now is more easy to read and understand :)\n",
        "diff_hunk": "@@ -0,0 +1,605 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {"
      },
      {
        "body": "these paths shouldn't contain references to alonsovidales\n",
        "diff_hunk": "@@ -0,0 +1,162 @@\n+# TensorFlow Go API\n+\n+TensorFlow. This package provides a high-level Go API for TensorFlow, this\n+package provides the necessary tools to create and manipulate Tensors,\n+Variables, Constants and also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API.\n+\n+In order to generate a valid Graph you can use the next Python code:\n+\n+```python\n+import tensorflow as tf\n+\n+input1 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input1')\n+input2 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input2')\n+output = tf.add(input1, input2, name='output')\n+\n+with tf.Session() as sess:\n+    tf.train.write_graph(sess.graph_def, '/tmp/graph/', 'test_graph.pb', as_text=True)\n+```\n+\n+The previous code prepares two placeholders with names 'input1' and\n+'input2' respectively, and other tensor used as output of the addition of the\n+two placeholders. At the end, it dumps the graph as text into a text file with\n+path:\n+'/tmp/graph/test_graph.pb'.\n+\n+From a Go application, you can use the next code to execute the graph:\n+\n+```go\n+package main\n+\n+import (\n+\t\"log\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func main() {\n+\t// These are the input tensors to be used\n+\tinputSlice1 := [][][]int64{\n+\t\t{\n+\t\t\t{1, 2},\n+\t\t\t{3, 4},\n+\t\t}, {\n+\t\t\t{5, 6},\n+\t\t\t{7, 8},\n+\t\t},\n+\t}\n+\tinputSlice2 := [][][]int64{\n+\t\t{\n+\t\t\t{9, 10},\n+\t\t\t{11, 12},\n+\t\t}, {\n+\t\t\t{13, 14},\n+\t\t\t{15, 16},\n+\t\t},\n+\t}\n+\n+\t// Create the two tensors, the data type is recognized automatically as\n+\t// also the tensor shape from the input slice\n+\tt1, err := tensorflow.NewTensor(inputSlice1)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\tt2, err := tensorflow.NewTensor(inputSlice2)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\t// Load the graph from the file that we generated from Python on the\n+\t// previous step\n+\tgraph, err := tensorflow.LoadGraphFromTextFile(\"/tmp/graph/test_graph.pb\")\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem reading the graph from the text file, Error:\", err)\n+\t}\n+\n+\t// Create the session and extend the Graph\n+\ts, err := tensorflow.NewSession()\n+\tif err := s.ExtendGraph(graph); err != nil {\n+\t\tlog.Fatal(\"Problem extending the Graph, Error:\", err)\n+\t}\n+\n+\tinput := map[string]*tensorflow.Tensor{\n+\t\t\"input1\": t1,\n+\t\t\"input2\": t2,\n+\t}\n+\t// Execute the graph with the two input tensors, and specify the names\n+\t// of the tensors to be returned, for this case just one\n+\tout, err := s.Run(input, []string{\"output\"}, nil)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying to run the saved graph, Error:\", err)\n+\t}\n+\n+\tif len(out) != 1 {\n+\t\tlog.Fatalf(\"The expected number of outputs is 1 but: %d returned\", len(out))\n+\t}\n+\n+\toutputTensor := out[0]\n+\tfor x := 0; x < outputTensor.Dim(0); x++ {\n+\t\tfor y := 0; y < outputTensor.Dim(1); y++ {\n+\t\t\tfor z := 0; z < outputTensor.Dim(2); z++ {\n+\t\t\t\t// Using GetVal we can access to the corresponding positions of\n+\t\t\t\t// the tensor as if we had been accessing to the positions in a\n+\t\t\t\t// multidimensional array, for instance GetVal(1, 2, 3) is\n+\t\t\t\t// equivalent to array[1][2][3] on a three dimensional array\n+\t\t\t\tval, err := out[0].GetVal(x, y, z)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tlog.Fatal(\"Error trying to read the output tensor, Error:\", err)\n+\t\t\t\t}\n+\t\t\t\tif val != inputSlice1[x][y][z]+inputSlice2[x][y][z] {\n+\t\t\t\t\tlog.Printf(\n+\t\t\t\t\t\t\"The sum of the two elements: %d + %d doesn't match with the returned value: %d\",\n+\t\t\t\t\t\tinputSlice1[x][y][z], inputSlice2[x][y][z], val)\n+\t\t\t\t}\n+\n+\t\t\t\tlog.Println(\"The value value on coordinates:\", x, y, z, \"is:\", val)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+```\n+\n+The previous code creates two Tensors to be processed by the previously\n+generated Graph, after the execution returns the Tensor with the result.\n+\n+### Image Recognition\n+\n+This is a complete code example that shows how to generate Graphs on Go and\n+execute them:\n+\n+[Image Recognition](https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/g3doc/tutorials/image_recognition/index.md#usage-with-the-go-api)"
      },
      {
        "body": "Ups, removed the references now the link paths are relative.\nI'm not sure if is correct to leave this examples out of contrib, or if it would be better to relocate them inside contrib/go.\n",
        "diff_hunk": "@@ -0,0 +1,162 @@\n+# TensorFlow Go API\n+\n+TensorFlow. This package provides a high-level Go API for TensorFlow, this\n+package provides the necessary tools to create and manipulate Tensors,\n+Variables, Constants and also to build, load and run Graphs.\n+\n+## API documentation\n+* [Session](g3doc/session.md): Encapsulates the environment in which Operation\n+  objects are executed, and Tensor objects are evaluated.\n+* [Graph](g3doc/graph.md): Contains a set of Operations, which represent units\n+  of computation; and Tensors, which represent the units of data that flow\n+  between operations.\n+* [Tensor](g3doc/tensor.md): Typed multi-dimensional array.\n+\n+## Practical Examples\n+\n+##### Python Graph generated and executed on Go\n+\n+This is just an example that shows how to interact with the provided API.\n+\n+In order to generate a valid Graph you can use the next Python code:\n+\n+```python\n+import tensorflow as tf\n+\n+input1 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input1')\n+input2 = tf.placeholder(tf.int64, shape=(2, 2, 2), name='input2')\n+output = tf.add(input1, input2, name='output')\n+\n+with tf.Session() as sess:\n+    tf.train.write_graph(sess.graph_def, '/tmp/graph/', 'test_graph.pb', as_text=True)\n+```\n+\n+The previous code prepares two placeholders with names 'input1' and\n+'input2' respectively, and other tensor used as output of the addition of the\n+two placeholders. At the end, it dumps the graph as text into a text file with\n+path:\n+'/tmp/graph/test_graph.pb'.\n+\n+From a Go application, you can use the next code to execute the graph:\n+\n+```go\n+package main\n+\n+import (\n+\t\"log\"\n+\n+\t\"github.com/tensorflow/tensorflow/tensorflow/contrib/go\"\n+)\n+\n+func main() {\n+\t// These are the input tensors to be used\n+\tinputSlice1 := [][][]int64{\n+\t\t{\n+\t\t\t{1, 2},\n+\t\t\t{3, 4},\n+\t\t}, {\n+\t\t\t{5, 6},\n+\t\t\t{7, 8},\n+\t\t},\n+\t}\n+\tinputSlice2 := [][][]int64{\n+\t\t{\n+\t\t\t{9, 10},\n+\t\t\t{11, 12},\n+\t\t}, {\n+\t\t\t{13, 14},\n+\t\t\t{15, 16},\n+\t\t},\n+\t}\n+\n+\t// Create the two tensors, the data type is recognized automatically as\n+\t// also the tensor shape from the input slice\n+\tt1, err := tensorflow.NewTensor(inputSlice1)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\tt2, err := tensorflow.NewTensor(inputSlice2)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying create a new tensor, Error:\", err)\n+\t}\n+\n+\t// Load the graph from the file that we generated from Python on the\n+\t// previous step\n+\tgraph, err := tensorflow.LoadGraphFromTextFile(\"/tmp/graph/test_graph.pb\")\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem reading the graph from the text file, Error:\", err)\n+\t}\n+\n+\t// Create the session and extend the Graph\n+\ts, err := tensorflow.NewSession()\n+\tif err := s.ExtendGraph(graph); err != nil {\n+\t\tlog.Fatal(\"Problem extending the Graph, Error:\", err)\n+\t}\n+\n+\tinput := map[string]*tensorflow.Tensor{\n+\t\t\"input1\": t1,\n+\t\t\"input2\": t2,\n+\t}\n+\t// Execute the graph with the two input tensors, and specify the names\n+\t// of the tensors to be returned, for this case just one\n+\tout, err := s.Run(input, []string{\"output\"}, nil)\n+\tif err != nil {\n+\t\tlog.Fatal(\"Problem trying to run the saved graph, Error:\", err)\n+\t}\n+\n+\tif len(out) != 1 {\n+\t\tlog.Fatalf(\"The expected number of outputs is 1 but: %d returned\", len(out))\n+\t}\n+\n+\toutputTensor := out[0]\n+\tfor x := 0; x < outputTensor.Dim(0); x++ {\n+\t\tfor y := 0; y < outputTensor.Dim(1); y++ {\n+\t\t\tfor z := 0; z < outputTensor.Dim(2); z++ {\n+\t\t\t\t// Using GetVal we can access to the corresponding positions of\n+\t\t\t\t// the tensor as if we had been accessing to the positions in a\n+\t\t\t\t// multidimensional array, for instance GetVal(1, 2, 3) is\n+\t\t\t\t// equivalent to array[1][2][3] on a three dimensional array\n+\t\t\t\tval, err := out[0].GetVal(x, y, z)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tlog.Fatal(\"Error trying to read the output tensor, Error:\", err)\n+\t\t\t\t}\n+\t\t\t\tif val != inputSlice1[x][y][z]+inputSlice2[x][y][z] {\n+\t\t\t\t\tlog.Printf(\n+\t\t\t\t\t\t\"The sum of the two elements: %d + %d doesn't match with the returned value: %d\",\n+\t\t\t\t\t\tinputSlice1[x][y][z], inputSlice2[x][y][z], val)\n+\t\t\t\t}\n+\n+\t\t\t\tlog.Println(\"The value value on coordinates:\", x, y, z, \"is:\", val)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+```\n+\n+The previous code creates two Tensors to be processed by the previously\n+generated Graph, after the execution returns the Tensor with the result.\n+\n+### Image Recognition\n+\n+This is a complete code example that shows how to generate Graphs on Go and\n+execute them:\n+\n+[Image Recognition](https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/g3doc/tutorials/image_recognition/index.md#usage-with-the-go-api)"
      },
      {
        "body": "\"creates\" doesn't need to be capitalized.\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable"
      },
      {
        "body": "Comment is hard to understand!\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it."
      },
      {
        "body": "s/Add/adds/  (capitalization and make it a verb).  Also missing period at the end of this function comment.\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to"
      },
      {
        "body": "These three lines could instead be:\n   return proto.MarshalTextString(gr.def)\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer"
      },
      {
        "body": "no need to have i separate, range returns the index:\n  for i, input := range gr.variables { ... }\n(and then you don't need i++)\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {"
      },
      {
        "body": "I'm slightly confused by the logic here of why it's only adding the name when the two are exactly equal - could you clarify with a comment?  (It's not clear to me on inspection why == instead of, e.g., >=)\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {\n+\t\tinputs[i] = \"^\" + input + \"/Assign\"\n+\t\ti++\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, &pb.NodeDef{\n+\t\tName:  \"init\",\n+\t\tOp:    \"NoOp\",\n+\t\tInput: inputs,\n+\t})\n+}\n+\n+// Placeholder Adds a placeholder to the Graph, a placeholder is an\n+// operation that must be fed with data on execution.\n+func (gr *Graph) Placeholder(name string, dataType DataType, dims []int64, dimNames []string) (op *GraphNode) {\n+\top = &GraphNode{\n+\t\toutDataTypes: map[string]DataType{\n+\t\t\tname: dataType,\n+\t\t},\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName: name,\n+\t\t\tOp:   \"Placeholder\",\n+\t\t\tAttr: make(map[string]*pb.AttrValue),\n+\t\t},\n+\t}\n+\top.def.Attr[\"dtype\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Type{\n+\t\t\tType: pb.DataType(dataType),\n+\t\t},\n+\t}\n+\n+\tshape := &pb.TensorShapeProto{\n+\t\tDim: make([]*pb.TensorShapeProto_Dim, len(dims)),\n+\t}\n+\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim,\n+\t\t}\n+\n+\t\tif len(dimNames) == len(dims) {"
      },
      {
        "body": "remove word 'expected' - the error is about the supplied input value, not an expected one.\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {\n+\t\tinputs[i] = \"^\" + input + \"/Assign\"\n+\t\ti++\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, &pb.NodeDef{\n+\t\tName:  \"init\",\n+\t\tOp:    \"NoOp\",\n+\t\tInput: inputs,\n+\t})\n+}\n+\n+// Placeholder Adds a placeholder to the Graph, a placeholder is an\n+// operation that must be fed with data on execution.\n+func (gr *Graph) Placeholder(name string, dataType DataType, dims []int64, dimNames []string) (op *GraphNode) {\n+\top = &GraphNode{\n+\t\toutDataTypes: map[string]DataType{\n+\t\t\tname: dataType,\n+\t\t},\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName: name,\n+\t\t\tOp:   \"Placeholder\",\n+\t\t\tAttr: make(map[string]*pb.AttrValue),\n+\t\t},\n+\t}\n+\top.def.Attr[\"dtype\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Type{\n+\t\t\tType: pb.DataType(dataType),\n+\t\t},\n+\t}\n+\n+\tshape := &pb.TensorShapeProto{\n+\t\tDim: make([]*pb.TensorShapeProto_Dim, len(dims)),\n+\t}\n+\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim,\n+\t\t}\n+\n+\t\tif len(dimNames) == len(dims) {\n+\t\t\tshape.Dim[i].Name = dimNames[i]\n+\t\t}\n+\t}\n+\n+\top.def.Attr[\"shape\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Shape{\n+\t\t\tShape: shape,\n+\t\t},\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, op.def)\n+\n+\treturn\n+}\n+\n+// AsStr Returns the current graph serialized so it can be exported.\n+func (gr *Graph) AsStr() []byte {\n+\tresult, _ := proto.Marshal(gr.def)\n+\n+\treturn result\n+}\n+\n+// ErrExpectedVarAsinput The specified operation is not defined.\n+type ErrExpectedVarAsinput struct {\n+\toperation string\n+\tinputPos  int\n+}\n+\n+func (e *ErrExpectedVarAsinput) Error() string {\n+\treturn fmt.Sprintf(\n+\t\t\"The expected input value at pos %d for the operation '%s' must be of type Variable\","
      },
      {
        "body": "I'd change \"the operation\" -> \"operation\" for consistency with the other error messages.\nAlso, only this and the next error have a colon after operation: '%s'.  The previous ones don't.  I'd remove it and just say 'after operation '%s'.  The use throughout the rest of your error messages uses the colon to separate the error and an explanation, and that seems more consistent.\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {\n+\t\tinputs[i] = \"^\" + input + \"/Assign\"\n+\t\ti++\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, &pb.NodeDef{\n+\t\tName:  \"init\",\n+\t\tOp:    \"NoOp\",\n+\t\tInput: inputs,\n+\t})\n+}\n+\n+// Placeholder Adds a placeholder to the Graph, a placeholder is an\n+// operation that must be fed with data on execution.\n+func (gr *Graph) Placeholder(name string, dataType DataType, dims []int64, dimNames []string) (op *GraphNode) {\n+\top = &GraphNode{\n+\t\toutDataTypes: map[string]DataType{\n+\t\t\tname: dataType,\n+\t\t},\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName: name,\n+\t\t\tOp:   \"Placeholder\",\n+\t\t\tAttr: make(map[string]*pb.AttrValue),\n+\t\t},\n+\t}\n+\top.def.Attr[\"dtype\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Type{\n+\t\t\tType: pb.DataType(dataType),\n+\t\t},\n+\t}\n+\n+\tshape := &pb.TensorShapeProto{\n+\t\tDim: make([]*pb.TensorShapeProto_Dim, len(dims)),\n+\t}\n+\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim,\n+\t\t}\n+\n+\t\tif len(dimNames) == len(dims) {\n+\t\t\tshape.Dim[i].Name = dimNames[i]\n+\t\t}\n+\t}\n+\n+\top.def.Attr[\"shape\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Shape{\n+\t\t\tShape: shape,\n+\t\t},\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, op.def)\n+\n+\treturn\n+}\n+\n+// AsStr Returns the current graph serialized so it can be exported.\n+func (gr *Graph) AsStr() []byte {\n+\tresult, _ := proto.Marshal(gr.def)\n+\n+\treturn result\n+}\n+\n+// ErrExpectedVarAsinput The specified operation is not defined.\n+type ErrExpectedVarAsinput struct {\n+\toperation string\n+\tinputPos  int\n+}\n+\n+func (e *ErrExpectedVarAsinput) Error() string {\n+\treturn fmt.Sprintf(\n+\t\t\"The expected input value at pos %d for the operation '%s' must be of type Variable\",\n+\t\te.inputPos, e.operation)\n+}\n+\n+// ErrOperationNotFound The specified operation is not defined.\n+type ErrOperationNotFound struct {\n+\top string\n+}\n+\n+func (e *ErrOperationNotFound) Error() string {\n+\treturn fmt.Sprintf(\"Operation '%s' not defined\", e.op)\n+}\n+\n+// ErrInvalidAmounthOfInputs The number of inputs doesn't corresponds with the\n+// expected for this operation.\n+type ErrInvalidAmounthOfInputs struct {\n+\toperation  string\n+\topInputs   int\n+\tspecInputs int\n+}\n+\n+func (e *ErrInvalidAmounthOfInputs) Error() string {\n+\treturn fmt.Sprintf(\"Inputs required for operation '%s': %d, but %d provided\",\n+\t\te.operation, e.opInputs, e.specInputs)\n+}\n+\n+// ErrMandatoryAttributeNotSpecified A mandatory attribute for this operation\n+// was not specified.\n+type ErrMandatoryAttributeNotSpecified struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrMandatoryAttributeNotSpecified) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' is mandatory for the operation: '%s'\","
      },
      {
        "body": "s/Returns/returns/\nmissing period at end of function comment.\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {\n+\t\tinputs[i] = \"^\" + input + \"/Assign\"\n+\t\ti++\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, &pb.NodeDef{\n+\t\tName:  \"init\",\n+\t\tOp:    \"NoOp\",\n+\t\tInput: inputs,\n+\t})\n+}\n+\n+// Placeholder Adds a placeholder to the Graph, a placeholder is an\n+// operation that must be fed with data on execution.\n+func (gr *Graph) Placeholder(name string, dataType DataType, dims []int64, dimNames []string) (op *GraphNode) {\n+\top = &GraphNode{\n+\t\toutDataTypes: map[string]DataType{\n+\t\t\tname: dataType,\n+\t\t},\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName: name,\n+\t\t\tOp:   \"Placeholder\",\n+\t\t\tAttr: make(map[string]*pb.AttrValue),\n+\t\t},\n+\t}\n+\top.def.Attr[\"dtype\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Type{\n+\t\t\tType: pb.DataType(dataType),\n+\t\t},\n+\t}\n+\n+\tshape := &pb.TensorShapeProto{\n+\t\tDim: make([]*pb.TensorShapeProto_Dim, len(dims)),\n+\t}\n+\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim,\n+\t\t}\n+\n+\t\tif len(dimNames) == len(dims) {\n+\t\t\tshape.Dim[i].Name = dimNames[i]\n+\t\t}\n+\t}\n+\n+\top.def.Attr[\"shape\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Shape{\n+\t\t\tShape: shape,\n+\t\t},\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, op.def)\n+\n+\treturn\n+}\n+\n+// AsStr Returns the current graph serialized so it can be exported.\n+func (gr *Graph) AsStr() []byte {\n+\tresult, _ := proto.Marshal(gr.def)\n+\n+\treturn result\n+}\n+\n+// ErrExpectedVarAsinput The specified operation is not defined.\n+type ErrExpectedVarAsinput struct {\n+\toperation string\n+\tinputPos  int\n+}\n+\n+func (e *ErrExpectedVarAsinput) Error() string {\n+\treturn fmt.Sprintf(\n+\t\t\"The expected input value at pos %d for the operation '%s' must be of type Variable\",\n+\t\te.inputPos, e.operation)\n+}\n+\n+// ErrOperationNotFound The specified operation is not defined.\n+type ErrOperationNotFound struct {\n+\top string\n+}\n+\n+func (e *ErrOperationNotFound) Error() string {\n+\treturn fmt.Sprintf(\"Operation '%s' not defined\", e.op)\n+}\n+\n+// ErrInvalidAmounthOfInputs The number of inputs doesn't corresponds with the\n+// expected for this operation.\n+type ErrInvalidAmounthOfInputs struct {\n+\toperation  string\n+\topInputs   int\n+\tspecInputs int\n+}\n+\n+func (e *ErrInvalidAmounthOfInputs) Error() string {\n+\treturn fmt.Sprintf(\"Inputs required for operation '%s': %d, but %d provided\",\n+\t\te.operation, e.opInputs, e.specInputs)\n+}\n+\n+// ErrMandatoryAttributeNotSpecified A mandatory attribute for this operation\n+// was not specified.\n+type ErrMandatoryAttributeNotSpecified struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrMandatoryAttributeNotSpecified) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' is mandatory for the operation: '%s'\",\n+\t\te.attribName, e.operation)\n+}\n+\n+// ErrInvalidAttrValue The data type of the value for this attribute is not valid.\n+type ErrInvalidAttrValue struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrInvalidAttrValue) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' value provided for operation: '%s' is not valid\",\n+\t\te.attribName, e.operation)\n+}\n+\n+// ErrInputOutputDataTypeMismatch The output data type doesn't match with the input one.\n+type ErrInputOutputDataTypeMismatch struct {\n+\toutDt DataType\n+\tinDt  DataType\n+}\n+\n+func (e *ErrInputOutputDataTypeMismatch) Error() string {\n+\treturn fmt.Sprintf(\"The output datatype '%s' doesn't correspond with the input data type '%s'\",\n+\t\te.outDt, e.inDt)\n+}\n+\n+// castAttrValue Returns an pb.AttrValue that contains the corresponding"
      },
      {
        "body": "s/Creates/creates/\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {\n+\t\tinputs[i] = \"^\" + input + \"/Assign\"\n+\t\ti++\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, &pb.NodeDef{\n+\t\tName:  \"init\",\n+\t\tOp:    \"NoOp\",\n+\t\tInput: inputs,\n+\t})\n+}\n+\n+// Placeholder Adds a placeholder to the Graph, a placeholder is an\n+// operation that must be fed with data on execution.\n+func (gr *Graph) Placeholder(name string, dataType DataType, dims []int64, dimNames []string) (op *GraphNode) {\n+\top = &GraphNode{\n+\t\toutDataTypes: map[string]DataType{\n+\t\t\tname: dataType,\n+\t\t},\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName: name,\n+\t\t\tOp:   \"Placeholder\",\n+\t\t\tAttr: make(map[string]*pb.AttrValue),\n+\t\t},\n+\t}\n+\top.def.Attr[\"dtype\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Type{\n+\t\t\tType: pb.DataType(dataType),\n+\t\t},\n+\t}\n+\n+\tshape := &pb.TensorShapeProto{\n+\t\tDim: make([]*pb.TensorShapeProto_Dim, len(dims)),\n+\t}\n+\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim,\n+\t\t}\n+\n+\t\tif len(dimNames) == len(dims) {\n+\t\t\tshape.Dim[i].Name = dimNames[i]\n+\t\t}\n+\t}\n+\n+\top.def.Attr[\"shape\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Shape{\n+\t\t\tShape: shape,\n+\t\t},\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, op.def)\n+\n+\treturn\n+}\n+\n+// AsStr Returns the current graph serialized so it can be exported.\n+func (gr *Graph) AsStr() []byte {\n+\tresult, _ := proto.Marshal(gr.def)\n+\n+\treturn result\n+}\n+\n+// ErrExpectedVarAsinput The specified operation is not defined.\n+type ErrExpectedVarAsinput struct {\n+\toperation string\n+\tinputPos  int\n+}\n+\n+func (e *ErrExpectedVarAsinput) Error() string {\n+\treturn fmt.Sprintf(\n+\t\t\"The expected input value at pos %d for the operation '%s' must be of type Variable\",\n+\t\te.inputPos, e.operation)\n+}\n+\n+// ErrOperationNotFound The specified operation is not defined.\n+type ErrOperationNotFound struct {\n+\top string\n+}\n+\n+func (e *ErrOperationNotFound) Error() string {\n+\treturn fmt.Sprintf(\"Operation '%s' not defined\", e.op)\n+}\n+\n+// ErrInvalidAmounthOfInputs The number of inputs doesn't corresponds with the\n+// expected for this operation.\n+type ErrInvalidAmounthOfInputs struct {\n+\toperation  string\n+\topInputs   int\n+\tspecInputs int\n+}\n+\n+func (e *ErrInvalidAmounthOfInputs) Error() string {\n+\treturn fmt.Sprintf(\"Inputs required for operation '%s': %d, but %d provided\",\n+\t\te.operation, e.opInputs, e.specInputs)\n+}\n+\n+// ErrMandatoryAttributeNotSpecified A mandatory attribute for this operation\n+// was not specified.\n+type ErrMandatoryAttributeNotSpecified struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrMandatoryAttributeNotSpecified) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' is mandatory for the operation: '%s'\",\n+\t\te.attribName, e.operation)\n+}\n+\n+// ErrInvalidAttrValue The data type of the value for this attribute is not valid.\n+type ErrInvalidAttrValue struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrInvalidAttrValue) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' value provided for operation: '%s' is not valid\",\n+\t\te.attribName, e.operation)\n+}\n+\n+// ErrInputOutputDataTypeMismatch The output data type doesn't match with the input one.\n+type ErrInputOutputDataTypeMismatch struct {\n+\toutDt DataType\n+\tinDt  DataType\n+}\n+\n+func (e *ErrInputOutputDataTypeMismatch) Error() string {\n+\treturn fmt.Sprintf(\"The output datatype '%s' doesn't correspond with the input data type '%s'\",\n+\t\te.outDt, e.inDt)\n+}\n+\n+// castAttrValue Returns an pb.AttrValue that contains the corresponding\n+// pb.AttrValue_* according to the type specified. Returns nil if the data type\n+// of the provided value can't be allocated on the AttrValue type\n+func (gr *Graph) castAttrValue(attrType string, v interface{}) (attrVal *pb.AttrValue) {\n+\tswitch attrType {\n+\tcase \"type\":\n+\t\tif dt, ok := v.(DataType); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Type{\n+\t\t\t\t\tType: pb.DataType(dt),\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"string\":\n+\t\tif st, ok := v.(string); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_S{\n+\t\t\t\t\tS: []byte(st),\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"tensor\":\n+\t\tif t, ok := v.(*Tensor); ok {\n+\t\t\ttp := &pb.TensorProto{\n+\t\t\t\tDtype:         t.Dtype,\n+\t\t\t\tTensorShape:   t.TensorShape,\n+\t\t\t\tTensorContent: t.TensorContent,\n+\t\t\t}\n+\t\t\tswitch t.DataType() {\n+\t\t\tcase DtFloat:\n+\t\t\t\ttp.FloatVal, _ = t.AsFloat32()\n+\t\t\tcase DtDouble:\n+\t\t\t\ttp.DoubleVal, _ = t.AsFloat64()\n+\t\t\tcase DtInt8, DtInt16, DtInt32, DtUint8:\n+\t\t\t\ttp.IntVal, _ = t.AsInt32()\n+\t\t\tcase DtInt64:\n+\t\t\t\ttp.Int64Val, _ = t.AsInt64()\n+\t\t\tcase DtBool:\n+\t\t\t\ttp.BoolVal, _ = t.AsBool()\n+\t\t\tcase DtString:\n+\t\t\t\ttp.StringVal, _ = t.AsStr()\n+\t\t\tdefault:\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Tensor{\n+\t\t\t\t\tTensor: tp,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"func\":\n+\t\tif f, ok := v.(*pb.NameAttrList); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Func{\n+\t\t\t\t\tFunc: f,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"int\":\n+\t\tif i, ok := v.(int64); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_I{\n+\t\t\t\t\tI: i,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"bool\":\n+\t\tif b, ok := v.(bool); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_B{\n+\t\t\t\t\tB: b,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"float\":\n+\t\tif f, ok := v.(float32); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_F{\n+\t\t\t\t\tF: f,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"shape\":\n+\t\tif s, ok := v.(*pb.TensorShapeProto); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Shape{\n+\t\t\t\t\tShape: s,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"list(type)\", \"list(int)\", \"list(shape)\", \"list(float)\":\n+\t\tif lv, ok := v.(*pb.AttrValue_ListValue); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_List{\n+\t\t\t\t\tList: lv,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// Constant Creates a tensor that is added as a constant to the Graph with the"
      },
      {
        "body": "s/Matches/matches/\ns/deducting/deducing/  (check for this globally -- to \"deduct\" is to remove from something;  to \"deduce\" is to infer or figure it out).\nPeriod in \"parameters, this\" -> parameters.  This\ns/in case of the matching/if the matching/\ns/mas have/must have/\n",
        "diff_hunk": "@@ -0,0 +1,570 @@\n+package tensorflow\n+\n+import (\n+\t\"bytes\"\n+\t\"fmt\"\n+\t\"io/ioutil\"\n+\t\"strings\"\n+\n+\t\"github.com/golang/protobuf/proto\"\n+\n+\tpb \"github.com/tensorflow/tensorflow/tensorflow/contrib/go/proto\"\n+)\n+\n+const (\n+\tcOpsProtobufDefsPath = \"/usr/local/tensorlow/ops.pbtxt\"\n+)\n+\n+// Graph Representation of the computation graph.\n+type Graph struct {\n+\tdef *pb.GraphDef\n+\n+\tavailableOps map[string]*pb.OpDef\n+\tconstants    map[string]*Tensor\n+\tvariables    map[string]*Tensor\n+}\n+\n+// GraphNode Representation of one of the nodes of the TensorFlow Graph.\n+// A node takes zero or more Tensors, performs some computation, and\n+// produces zero or more Tensors.\n+type GraphNode struct {\n+\tref          *pb.NodeDef\n+\tdef          *pb.NodeDef\n+\toutDataTypes map[string]DataType\n+}\n+\n+// NewGraph Returns an initialized instance of the Graph struct.\n+func NewGraph() *Graph {\n+\treturn &Graph{\n+\t\tdef:          new(pb.GraphDef),\n+\t\tavailableOps: make(map[string]*pb.OpDef),\n+\t\tconstants:    make(map[string]*Tensor),\n+\t\tvariables:    make(map[string]*Tensor),\n+\t}\n+}\n+\n+// NewGraphFromText Returns a new graph populated with the deserialization of\n+// the provided graph string.\n+func NewGraphFromText(graphStr string) (gr *Graph, err error) {\n+\tgr = NewGraph()\n+\terr = proto.UnmarshalText(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromFile Loads a Graph from the file on the specified path.\n+func LoadGraphFromFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tgr = NewGraph()\n+\terr = proto.Unmarshal(graphStr, gr.def)\n+\n+\treturn\n+}\n+\n+// LoadGraphFromTextFile Loads a Graph as plain text from the file on the specified\n+// path.\n+func LoadGraphFromTextFile(path string) (gr *Graph, err error) {\n+\tgraphStr, err := ioutil.ReadFile(path)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\treturn NewGraphFromText(string(graphStr))\n+}\n+\n+// Op Adds a new Node to the Graph with the specified operation, this function\n+// could return an error if any of the mandatory attributes is not be present\n+// or the value is not the expected for this attribute.\n+func (gr *Graph) Op(opName string, name string, input []*GraphNode, device string, attrs map[string]interface{}) (node *GraphNode, err error) {\n+\tif err = gr.loadAvailableOps(); err != nil {\n+\t\treturn\n+\t}\n+\n+\top, opFound := gr.availableOps[strings.ToLower(opName)]\n+\tif !opFound {\n+\t\treturn nil, &ErrOperationNotFound{\n+\t\t\top: opName,\n+\t\t}\n+\t}\n+\n+\tif len(op.InputArg) != len(input) {\n+\t\treturn nil, &ErrInvalidAmounthOfInputs{\n+\t\t\toperation:  opName,\n+\t\t\topInputs:   len(op.InputArg),\n+\t\t\tspecInputs: len(input),\n+\t\t}\n+\t}\n+\tinputs := make([]string, len(input))\n+\tfor i, inNode := range input {\n+\t\tif op.InputArg[i].IsRef {\n+\t\t\tif inNode.ref == nil {\n+\t\t\t\treturn nil, &ErrExpectedVarAsinput{\n+\t\t\t\t\toperation: opName,\n+\t\t\t\t\tinputPos:  i,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinputs[i] = inNode.ref.Name\n+\t\t} else {\n+\t\t\tinputs[i] = inNode.def.Name\n+\t\t}\n+\t}\n+\tnode = &GraphNode{\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName:   name,\n+\t\t\tOp:     opName,\n+\t\t\tInput:  inputs,\n+\t\t\tDevice: device,\n+\t\t\tAttr:   make(map[string]*pb.AttrValue),\n+\t\t},\n+\t\toutDataTypes: make(map[string]DataType),\n+\t}\n+\n+\tif attrs == nil {\n+\t\tattrs = make(map[string]interface{})\n+\t}\n+\tgr.matchTypes(input, node, attrs, op)\n+\n+\tfor _, attr := range op.Attr {\n+\t\t// Check if the attribute is specified, if it is not\n+\t\t// and doesn't have a default value, return an error since it\n+\t\t// is mandatory\n+\t\tif v, ok := attrs[attr.Name]; ok {\n+\t\t\tnode.def.Attr[attr.Name] = gr.castAttrValue(attr.Type, v)\n+\t\t\tif node.def.Attr[attr.Name] == nil {\n+\t\t\t\treturn nil, &ErrInvalidAttrValue{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif attr.DefaultValue != nil {\n+\t\t\t\tnode.def.Attr[attr.Name] = attr.DefaultValue\n+\t\t\t} else {\n+\t\t\t\treturn nil, &ErrMandatoryAttributeNotSpecified{\n+\t\t\t\t\toperation:  opName,\n+\t\t\t\t\tattribName: attr.Name,\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, node.def)\n+\n+\treturn node, nil\n+}\n+\n+// Variable Creates a variable operation and adds it to the graph. A variable\n+// is a type of tensor that holds state in the form of a tensor that persists\n+// across steps.\n+func (gr *Graph) Variable(name string, initialData interface{}) (op *GraphNode, err error) {\n+\tvar dims [][]int64\n+\n+\tts, err := NewTensor(initialData)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.variables[name] = ts\n+\n+\tshape := new(pb.TensorShapeProto)\n+\tif ts.NumDims() == 0 {\n+\t\tdims = [][]int64{{1}}\n+\t} else {\n+\t\tdims = ts.Shape()\n+\t}\n+\n+\tshape.Dim = make([]*pb.TensorShapeProto_Dim, len(dims))\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim[i],\n+\t\t}\n+\t}\n+\n+\tinitVal, err := gr.Op(\"Const\", name+\"/initial_value\", nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t\t\"shape\": shape,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable, err := gr.Op(\"Variable\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\":       ts.DataType(),\n+\t\t\"shape\":       shape,\n+\t\t\"container\":   \"\",\n+\t\t\"shared_name\": \"\",\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\tvariable.ref = variable.def\n+\n+\t_, err = gr.Op(\"Assign\", name+\"/Assign\", []*GraphNode{variable, initVal}, \"\", map[string]interface{}{\n+\t\t\"use_locking\":    true,\n+\t\t\"validate_shape\": true,\n+\t})\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\n+\top, err = gr.Op(\"Identity\", name+\"/read\", []*GraphNode{variable}, \"\", nil)\n+\n+\t// For reference this variable, use the variable as it.\n+\top.ref = variable.def\n+\n+\treturn\n+}\n+\n+// String Returns a string representation of this graph, used for debugging\n+// proposals.\n+func (gr *Graph) String() string {\n+\tvar bufStr bytes.Buffer\n+\tproto.MarshalText(&bufStr, gr.def)\n+\n+\treturn bufStr.String()\n+}\n+\n+// addInitializationGraphOp Add the initialization operation to the graph to\n+// cover all the added variables\n+func (gr *Graph) addInitializationGraphOp() {\n+\tinputs := make([]string, len(gr.variables))\n+\ti := 0\n+\tfor input := range gr.variables {\n+\t\tinputs[i] = \"^\" + input + \"/Assign\"\n+\t\ti++\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, &pb.NodeDef{\n+\t\tName:  \"init\",\n+\t\tOp:    \"NoOp\",\n+\t\tInput: inputs,\n+\t})\n+}\n+\n+// Placeholder Adds a placeholder to the Graph, a placeholder is an\n+// operation that must be fed with data on execution.\n+func (gr *Graph) Placeholder(name string, dataType DataType, dims []int64, dimNames []string) (op *GraphNode) {\n+\top = &GraphNode{\n+\t\toutDataTypes: map[string]DataType{\n+\t\t\tname: dataType,\n+\t\t},\n+\t\tdef: &pb.NodeDef{\n+\t\t\tName: name,\n+\t\t\tOp:   \"Placeholder\",\n+\t\t\tAttr: make(map[string]*pb.AttrValue),\n+\t\t},\n+\t}\n+\top.def.Attr[\"dtype\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Type{\n+\t\t\tType: pb.DataType(dataType),\n+\t\t},\n+\t}\n+\n+\tshape := &pb.TensorShapeProto{\n+\t\tDim: make([]*pb.TensorShapeProto_Dim, len(dims)),\n+\t}\n+\n+\tfor i, dim := range dims {\n+\t\tshape.Dim[i] = &pb.TensorShapeProto_Dim{\n+\t\t\tSize: dim,\n+\t\t}\n+\n+\t\tif len(dimNames) == len(dims) {\n+\t\t\tshape.Dim[i].Name = dimNames[i]\n+\t\t}\n+\t}\n+\n+\top.def.Attr[\"shape\"] = &pb.AttrValue{\n+\t\tValue: &pb.AttrValue_Shape{\n+\t\t\tShape: shape,\n+\t\t},\n+\t}\n+\n+\tgr.def.Node = append(gr.def.Node, op.def)\n+\n+\treturn\n+}\n+\n+// AsStr Returns the current graph serialized so it can be exported.\n+func (gr *Graph) AsStr() []byte {\n+\tresult, _ := proto.Marshal(gr.def)\n+\n+\treturn result\n+}\n+\n+// ErrExpectedVarAsinput The specified operation is not defined.\n+type ErrExpectedVarAsinput struct {\n+\toperation string\n+\tinputPos  int\n+}\n+\n+func (e *ErrExpectedVarAsinput) Error() string {\n+\treturn fmt.Sprintf(\n+\t\t\"The expected input value at pos %d for the operation '%s' must be of type Variable\",\n+\t\te.inputPos, e.operation)\n+}\n+\n+// ErrOperationNotFound The specified operation is not defined.\n+type ErrOperationNotFound struct {\n+\top string\n+}\n+\n+func (e *ErrOperationNotFound) Error() string {\n+\treturn fmt.Sprintf(\"Operation '%s' not defined\", e.op)\n+}\n+\n+// ErrInvalidAmounthOfInputs The number of inputs doesn't corresponds with the\n+// expected for this operation.\n+type ErrInvalidAmounthOfInputs struct {\n+\toperation  string\n+\topInputs   int\n+\tspecInputs int\n+}\n+\n+func (e *ErrInvalidAmounthOfInputs) Error() string {\n+\treturn fmt.Sprintf(\"Inputs required for operation '%s': %d, but %d provided\",\n+\t\te.operation, e.opInputs, e.specInputs)\n+}\n+\n+// ErrMandatoryAttributeNotSpecified A mandatory attribute for this operation\n+// was not specified.\n+type ErrMandatoryAttributeNotSpecified struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrMandatoryAttributeNotSpecified) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' is mandatory for the operation: '%s'\",\n+\t\te.attribName, e.operation)\n+}\n+\n+// ErrInvalidAttrValue The data type of the value for this attribute is not valid.\n+type ErrInvalidAttrValue struct {\n+\toperation  string\n+\tattribName string\n+}\n+\n+func (e *ErrInvalidAttrValue) Error() string {\n+\treturn fmt.Sprintf(\"The attribute '%s' value provided for operation: '%s' is not valid\",\n+\t\te.attribName, e.operation)\n+}\n+\n+// ErrInputOutputDataTypeMismatch The output data type doesn't match with the input one.\n+type ErrInputOutputDataTypeMismatch struct {\n+\toutDt DataType\n+\tinDt  DataType\n+}\n+\n+func (e *ErrInputOutputDataTypeMismatch) Error() string {\n+\treturn fmt.Sprintf(\"The output datatype '%s' doesn't correspond with the input data type '%s'\",\n+\t\te.outDt, e.inDt)\n+}\n+\n+// castAttrValue Returns an pb.AttrValue that contains the corresponding\n+// pb.AttrValue_* according to the type specified. Returns nil if the data type\n+// of the provided value can't be allocated on the AttrValue type\n+func (gr *Graph) castAttrValue(attrType string, v interface{}) (attrVal *pb.AttrValue) {\n+\tswitch attrType {\n+\tcase \"type\":\n+\t\tif dt, ok := v.(DataType); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Type{\n+\t\t\t\t\tType: pb.DataType(dt),\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"string\":\n+\t\tif st, ok := v.(string); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_S{\n+\t\t\t\t\tS: []byte(st),\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"tensor\":\n+\t\tif t, ok := v.(*Tensor); ok {\n+\t\t\ttp := &pb.TensorProto{\n+\t\t\t\tDtype:         t.Dtype,\n+\t\t\t\tTensorShape:   t.TensorShape,\n+\t\t\t\tTensorContent: t.TensorContent,\n+\t\t\t}\n+\t\t\tswitch t.DataType() {\n+\t\t\tcase DtFloat:\n+\t\t\t\ttp.FloatVal, _ = t.AsFloat32()\n+\t\t\tcase DtDouble:\n+\t\t\t\ttp.DoubleVal, _ = t.AsFloat64()\n+\t\t\tcase DtInt8, DtInt16, DtInt32, DtUint8:\n+\t\t\t\ttp.IntVal, _ = t.AsInt32()\n+\t\t\tcase DtInt64:\n+\t\t\t\ttp.Int64Val, _ = t.AsInt64()\n+\t\t\tcase DtBool:\n+\t\t\t\ttp.BoolVal, _ = t.AsBool()\n+\t\t\tcase DtString:\n+\t\t\t\ttp.StringVal, _ = t.AsStr()\n+\t\t\tdefault:\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Tensor{\n+\t\t\t\t\tTensor: tp,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"func\":\n+\t\tif f, ok := v.(*pb.NameAttrList); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Func{\n+\t\t\t\t\tFunc: f,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"int\":\n+\t\tif i, ok := v.(int64); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_I{\n+\t\t\t\t\tI: i,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"bool\":\n+\t\tif b, ok := v.(bool); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_B{\n+\t\t\t\t\tB: b,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"float\":\n+\t\tif f, ok := v.(float32); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_F{\n+\t\t\t\t\tF: f,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"shape\":\n+\t\tif s, ok := v.(*pb.TensorShapeProto); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_Shape{\n+\t\t\t\t\tShape: s,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\tcase \"list(type)\", \"list(int)\", \"list(shape)\", \"list(float)\":\n+\t\tif lv, ok := v.(*pb.AttrValue_ListValue); ok {\n+\t\t\treturn &pb.AttrValue{\n+\t\t\t\tValue: &pb.AttrValue_List{\n+\t\t\t\t\tList: lv,\n+\t\t\t\t},\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// Constant Creates a tensor that is added as a constant to the Graph with the\n+// specified name.\n+func (gr *Graph) Constant(name string, data interface{}) (op *GraphNode, err error) {\n+\tts, err := NewTensor(data)\n+\tif err != nil {\n+\t\treturn\n+\t}\n+\tgr.constants[name] = ts\n+\n+\treturn gr.Op(\"Const\", name, nil, \"\", map[string]interface{}{\n+\t\t\"dtype\": ts.DataType(),\n+\t\t\"value\": ts,\n+\t})\n+}\n+\n+// matchTypes Matches all the input/output parameters with their corresponding"
      }
    ],
    "body": "Hello,\nDuring the last weeks I have been working on this implementation of the Go API. **This is still a work in progress**, I'm doing this PR just to show what I'm working on and to get feedback.\n\nI forked from https://github.com/tmc/tensorflow/tree/go_bindings where the bindings was implemented by Travis Cline. This Go library allows to interact with the tensors in an easy way from Go being able to create, populate and retrieve the content from the tensors. This library also provides the API for Graph generation from Go, create sessions, run this graphs and so on. The goal is to achieve the same functionality provided by the C++ API.\n\nIn order to show how the API works, I updated the label_image example with an implementation using the [Go API](https://github.com/alonsovidales/tensorflow/tree/go_bindings_tensors/tensorflow/examples/label_image_go) and I also updated the [tutorial](https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/g3doc/tutorials/image_recognition/index.md#usage-with-the-go-api)\n\nI'm trying to keep all the code covered with tests, I still have to cover some corner cases and functionality like the data type recognition for the graph, etc.\nBy the moment the supported Tensor data types are:\n- TF_FLOAT\n- TF_DOUBLE\n- TF_INT32\n- TF_UINT8\n- TF_UINT32\n- TF_INT16\n- TF_INT8\n- TF_STRING\n- TF_INT64\n- TF_BOOL\n  I'm planing to add support for TF_QINT\\* TF_BFLOAT16 and TF_COMPLEX soon.\n\nI have some doubts:\n- Would it be better to add the API docu as md in tensorflow/g3doc/ , or just with GoDoc it would be enough? Having the docu on the MarkDown files could be better for the people without too much experience on Go, but having the docu on GoDocs is going to be updated in an easiest way and the Go developers are more used to read the documentation on GoDoc format. We can also keep it in both places, or just add a link from the MarkDown docu to GoDoc.\n- In order to get the definition for all the available operations I'm loading this info from tensorflow/core/ops/ops.pbtxt . The problem is that I'm doing this [cp of the file from go gen](https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/go/gen.go#L3). I think that this problem could be approached on three different ways:\n  - 1. Converting ops.pbtxt into a .go file: converting the content into a constant string we don't need to move this file, this option will remove this dependency, but if the file changes it will requiere to regenerate the libs that shouldn't be too much problem, we could even generate Go code for each operation so we don't need to parse the file and it would improve the performance.\n  - 2. Using something like the TF_GetOpList function used by Python in order to retrieve this info from the C++ API. This would add more dependencies from the Go to the C++ API, that is not a mayor issue but will requiere to prepare a new method or modify the original since TF_GetOpList depends on the Python libs.\n  - 3. As I'm doing right now, just copying the file. At least for a first iteration this could be a good option since it makes more easy to work with the code.\n- Regarding to the installation, I'm not sure if keep using \\\"go generate\\\" or Bazel, Go generate is more or less the standar for Go projects, but Bazel is what TensorFlow is using. I think that I'm going to change to Bazel so I can avoid things like [bazel from Go](https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/go/gen.go#L1) but I'm not sure if this is the best approach.\n\nI would be really grateful if you could please take a look at the code and let me know your thoughts, what I'm missing, how can I improve it, etc.\n\nThanks!\n\nRelated ticket: #10 \n",
    "timestamp": "2025-05-06 01:27:13"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/38968",
    "comments": [
      "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_sender_cla -->",
      "@kushanam Thank you for your contribution. Can you please sign CLA? Thanks!",
      "@gbaned it's been done.\r\n\r\n",
      "Also added @anj-s as reviewer as she has the most familiarity with the distributed input iterators etc",
      "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\r\n\r\n@googlebot I signed it!",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->",
      "@kushanam it still shows CLA is pending , can you use please make sure to use same GitHub username and email-id associated with it.",
      "@googlebot I fixed it.",
      "@kushanam is this supposed to work for TF2 as well? Or just TF1?",
      "hi @kushanam  - any updates on this?",
      "Hi @guptapriya, Thanks for the follow up. yes I will submit the changes\nthis week...\n\nOn Sat, Jun 13, 2020 at 10:26 PM guptapriya <notifications@github.com>\nwrote:\n\n> hi @kushanam <https://github.com/kushanam> - any updates on this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38968#issuecomment-643720700>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKDMBKKME7EYXGKJ54TPW53RWRNRNANCNFSM4MSQJF7Q>\n> .\n>\n",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- cla_yes -->",
      "@kushanam Can you please check @anj-s's comments and resolve conflicts?. Thanks!",
      "@kushanam Can you please resolve conflicts? Thanks!",
      "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_consent -->",
      "> \r\n> \r\n> All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\r\n> \r\n> We need to confirm that all authors are ok with their commits being contributed to this project. Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\r\n> \r\n> _Note to project maintainer:_ There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent. In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\r\n\r\n@googlebot I consent",
      "@kushanam Can you please check @mihaimaruseac's comments and resolve conflicts?. Thanks!",
      "This seems like a broken rebase, bringing in commits from other PRs.",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->",
      "@kushanam Can you please sign CLA. Thanks!\r\n",
      "> \r\n> \r\n> @kushanam Can you please sign CLA. Thanks!\r\n\r\n@gbaned Have signed it before and it shows in my contributor agreement!",
      "You have at least one commit using an `@ngvpn01-165-85.dyn.scz.us.nvidia.com` address. That one does not have CLA signed. Seems it was a misconfigured `git.email` address, maybe a `git amend` can help?",
      "We still need the CLA fix",
      "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- ok -->",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->",
      "> \r\n> \r\n> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\r\n@googlebot I fixed it.\r\n",
      "@kushanam Can you please sign CLA. Thanks!",
      "@kushanam  It still shows CLA is pending , can you please sign CLA. Thanks!",
      "> \r\n> \r\n> @kushanam It still shows CLA is pending , can you please sign CLA. Thanks!\r\n\r\n@gbaned Could you let me know again which email address/commit is giving the CLA issue? I can see the agreement signed on my ttps://cla.developers.google.com/clas page and last time the CLA was verified: https://github.com/tensorflow/tensorflow/pull/38968#issuecomment-664512078",
      "> @googlebot I fixed it.\r\n\r\n@googlebot I fixed it."
    ],
    "review_comments": [
      {
        "body": "Add the new argument in the `Args` section.",
        "diff_hunk": "@@ -872,8 +881,13 @@ def replica_fn_with_signature(inputs):\n       A \"distributed `Dataset`\", which acts like a `tf.data.Dataset` except\n       it produces \"per-replica\" values.\n     \"\"\"\n-    return self._extended._experimental_distribute_datasets_from_function(  # pylint: disable=protected-access\n+    if replication_mode == InputReplicationMode.PER_WORKER:"
      },
      {
        "body": "Also, in the doc string above, add an example that shows the usage of the new arg",
        "diff_hunk": "@@ -872,8 +881,13 @@ def replica_fn_with_signature(inputs):\n       A \"distributed `Dataset`\", which acts like a `tf.data.Dataset` except\n       it produces \"per-replica\" values.\n     \"\"\"\n-    return self._extended._experimental_distribute_datasets_from_function(  # pylint: disable=protected-access\n+    if replication_mode == InputReplicationMode.PER_WORKER:"
      },
      {
        "body": "Any reason not to re-use the same method `self._extended._experimental_distribute_datasets_from_function` with `replication_mode` arg? Since the code is quite similar between the two. It will prevent them from getting out of sync w each other.",
        "diff_hunk": "@@ -872,8 +881,13 @@ def replica_fn_with_signature(inputs):\n       A \"distributed `Dataset`\", which acts like a `tf.data.Dataset` except\n       it produces \"per-replica\" values.\n     \"\"\"\n-    return self._extended._experimental_distribute_datasets_from_function(  # pylint: disable=protected-access\n+    if replication_mode == InputReplicationMode.PER_WORKER:\n+      return self._extended._experimental_distribute_datasets_from_function(  # pylint: disable=protected-access"
      },
      {
        "body": "Please also implement this functionality for the other strategies: \r\n- Default strategy (in this file)\r\n- OneDeviceStrategy\r\n- CollectiveAllReduceStrategy (although I think the current version of this for PER_WORKER may be broken.. because it inherits from `MirroredStrategy`.)\r\n- PS Strategy / CentralStorageStrategy.\r\n\r\nYou can skip TPUStrategy but please add a \"not supported\" error message. \r\n\r\n",
        "diff_hunk": "@@ -872,8 +881,13 @@ def replica_fn_with_signature(inputs):\n       A \"distributed `Dataset`\", which acts like a `tf.data.Dataset` except\n       it produces \"per-replica\" values.\n     \"\"\"\n-    return self._extended._experimental_distribute_datasets_from_function(  # pylint: disable=protected-access\n+    if replication_mode == InputReplicationMode.PER_WORKER:\n+      return self._extended._experimental_distribute_datasets_from_function(  # pylint: disable=protected-access\n         dataset_fn)\n+    elif replication_mode == InputReplicationMode.PER_REPLICA:"
      },
      {
        "body": "I don't understand this error message. What are you trying to say here?",
        "diff_hunk": "@@ -135,6 +135,28 @@ def get_distributed_datasets_from_function(dataset_fn,\n         strategy)\n \n \n+def get_distributed_datasets_from_function_per_replica(\n+  dataset_fn, input_workers, input_contexts, strategy):\n+    \"\"\"Returns a distributed dataset from the given input function.\n+\n+  This is a common function that is used by all strategies to return a\n+  per replica dataset\n+  Returns:\n+    A distributed dataset instance.\n+  \"\"\"\n+  if tf2.enabled():\n+    return DistributedDatasetsFromFunctionForReplicas(\n+          dataset_fn,\n+          input_workers,\n+          input_contexts,\n+          strategy)\n+  else:\n+    raise ValueError(\n+      \"executing_eagerly_outside_functions is not implemented for: %r\" "
      },
      {
        "body": "please add an args section. (also, perhaps extend `get_distributed_datasets_from_function` instead of a new function?)",
        "diff_hunk": "@@ -135,6 +135,28 @@ def get_distributed_datasets_from_function(dataset_fn,\n         strategy)\n \n \n+def get_distributed_datasets_from_function_per_replica(\n+  dataset_fn, input_workers, input_contexts, strategy):\n+    \"\"\"Returns a distributed dataset from the given input function.\n+\n+  This is a common function that is used by all strategies to return a\n+  per replica dataset\n+  Returns:\n+    A distributed dataset instance."
      },
      {
        "body": "lint nit: 2 lines instead of 3",
        "diff_hunk": "@@ -135,6 +135,28 @@ def get_distributed_datasets_from_function(dataset_fn,\n         strategy)\n \n \n+def get_distributed_datasets_from_function_per_replica(\n+  dataset_fn, input_workers, input_contexts, strategy):\n+    \"\"\"Returns a distributed dataset from the given input function.\n+\n+  This is a common function that is used by all strategies to return a\n+  per replica dataset\n+  Returns:\n+    A distributed dataset instance.\n+  \"\"\"\n+  if tf2.enabled():\n+    return DistributedDatasetsFromFunctionForReplicas(\n+          dataset_fn,\n+          input_workers,\n+          input_contexts,\n+          strategy)\n+  else:\n+    raise ValueError(\n+      \"executing_eagerly_outside_functions is not implemented for: %r\" \n+        % replication_mode)\n+\n+\n+"
      },
      {
        "body": "Can we also verify the values returned?",
        "diff_hunk": "@@ -1111,5 +1111,67 @@ def dataset_fn(ctx):\n           strategy,\n           sess=sess)\n \n+def dali_const_dataset(batch_size, sample_size, device_id):\n+  import tensorflow as tf\n+  import nvidia.dali.fn as fn\n+  from nvidia.dali.pipeline import Pipeline\n+  import nvidia.dali.plugin.tf as dali_tf\n+\n+  pipeline = Pipeline(batch_size, 4, device_id)\n+  const = fn.constant(device = 'gpu', fdata = sample_size * [1.])\n+  pipeline.set_outputs(const)\n+\n+  dali_dataset = dali_tf.DALIDataset(\n+      pipeline=pipeline,\n+      batch_size=batch_size,\n+      output_shapes=((batch_size, sample_size)),\n+      output_dtypes=(tf.float32),\n+      device_id=device_id)\n+\n+  options = tf.data.Options()\n+  options.experimental_optimization.apply_default_optimizations = False\n+  options.experimental_optimization.autotune = False\n+\n+  return dali_dataset.with_options(options)\n+\n+\n+class InputTypeSpecAndDevicePerReplicaTest(test.TestCase, parameterized.TestCase):\n+\n+  @combinations.generate(\n+      combinations.combine(\n+          mode=[\"eager\"],\n+          distribution=[\n+              strategy_combinations.mirrored_strategy_with_two_gpus]))\n+  def testInputSignatureForPerReplicaValues(self, distribution):\n+    with distribution.scope():\n+      def dataset_fn(input_context):\n+        return dali_const_dataset(4, 4, input_context.input_pipeline_id)\n+\n+      ds = distribution.experimental_distribute_datasets_from_function(\n+          dataset_fn, distribute_lib.InputReplicationMode.PER_REPLICA)\n+\n+      iterator = iter(ds)\n+      type_spec = iterator.element_spec\n+\n+      @def_function.function(input_signature=[type_spec])\n+      def process_inputs(inputs):\n+        distribution.run(lambda inputs: inputs, args=(inputs,))\n+\n+      for x in ds:\n+        process_inputs(x)"
      },
      {
        "body": "I don't think this is possible as we cannot have these dependencies in TF, even in tests. I think for the tests here, you can simply make an input pipeline using basic tf.data. \r\nOnce this is submitted, you can add a DALI test case in the DALI repo? or another suitable place.\r\n\r\nThis is good to know that this works though.",
        "diff_hunk": "@@ -1111,5 +1111,67 @@ def dataset_fn(ctx):\n           strategy,\n           sess=sess)\n \n+def dali_const_dataset(batch_size, sample_size, device_id):\n+  import tensorflow as tf\n+  import nvidia.dali.fn as fn\n+  from nvidia.dali.pipeline import Pipeline"
      },
      {
        "body": "this should not be needed once you remove the DALI dependency.",
        "diff_hunk": "@@ -1111,5 +1111,67 @@ def dataset_fn(ctx):\n           strategy,\n           sess=sess)\n \n+def dali_const_dataset(batch_size, sample_size, device_id):\n+  import tensorflow as tf\n+  import nvidia.dali.fn as fn\n+  from nvidia.dali.pipeline import Pipeline\n+  import nvidia.dali.plugin.tf as dali_tf\n+\n+  pipeline = Pipeline(batch_size, 4, device_id)\n+  const = fn.constant(device = 'gpu', fdata = sample_size * [1.])\n+  pipeline.set_outputs(const)\n+\n+  dali_dataset = dali_tf.DALIDataset(\n+      pipeline=pipeline,\n+      batch_size=batch_size,\n+      output_shapes=((batch_size, sample_size)),\n+      output_dtypes=(tf.float32),\n+      device_id=device_id)\n+\n+  options = tf.data.Options()\n+  options.experimental_optimization.apply_default_optimizations = False\n+  options.experimental_optimization.autotune = False\n+\n+  return dali_dataset.with_options(options)\n+\n+\n+class InputTypeSpecAndDevicePerReplicaTest(test.TestCase, parameterized.TestCase):\n+\n+  @combinations.generate(\n+      combinations.combine(\n+          mode=[\"eager\"],\n+          distribution=[\n+              strategy_combinations.mirrored_strategy_with_two_gpus]))\n+  def testInputSignatureForPerReplicaValues(self, distribution):\n+    with distribution.scope():\n+      def dataset_fn(input_context):\n+        return dali_const_dataset(4, 4, input_context.input_pipeline_id)\n+\n+      ds = distribution.experimental_distribute_datasets_from_function(\n+          dataset_fn, distribute_lib.InputReplicationMode.PER_REPLICA)\n+\n+      iterator = iter(ds)\n+      type_spec = iterator.element_spec\n+\n+      @def_function.function(input_signature=[type_spec])\n+      def process_inputs(inputs):\n+        distribution.run(lambda inputs: inputs, args=(inputs,))\n+\n+      for x in ds:\n+        process_inputs(x)\n+        self.assertEqual(\n+          x[0].values[0].device,\n+          distribution.extended.worker_devices[0])\n+        self.assertEqual(\n+          x[0].values[0].backing_device,\n+          distribution.extended.worker_devices[0])\n+        self.assertEqual(\n+          x[0].values[1].device,\n+          distribution.extended.worker_devices[1])\n+        self.assertEqual(\n+          x[0].values[1].backing_device,\n+          distribution.extended.worker_devices[1])\n+        break               # DALI dataset is infinite"
      },
      {
        "body": "Would be good to test with an input function where the different replicas functions have different number of inputs (to check the last partial batch logic)",
        "diff_hunk": "@@ -1111,5 +1111,67 @@ def dataset_fn(ctx):\n           strategy,\n           sess=sess)\n \n+def dali_const_dataset(batch_size, sample_size, device_id):\n+  import tensorflow as tf\n+  import nvidia.dali.fn as fn\n+  from nvidia.dali.pipeline import Pipeline\n+  import nvidia.dali.plugin.tf as dali_tf\n+\n+  pipeline = Pipeline(batch_size, 4, device_id)\n+  const = fn.constant(device = 'gpu', fdata = sample_size * [1.])\n+  pipeline.set_outputs(const)\n+\n+  dali_dataset = dali_tf.DALIDataset(\n+      pipeline=pipeline,\n+      batch_size=batch_size,\n+      output_shapes=((batch_size, sample_size)),\n+      output_dtypes=(tf.float32),\n+      device_id=device_id)\n+\n+  options = tf.data.Options()\n+  options.experimental_optimization.apply_default_optimizations = False\n+  options.experimental_optimization.autotune = False\n+\n+  return dali_dataset.with_options(options)\n+\n+\n+class InputTypeSpecAndDevicePerReplicaTest(test.TestCase, parameterized.TestCase):\n+\n+  @combinations.generate(\n+      combinations.combine(\n+          mode=[\"eager\"],\n+          distribution=[\n+              strategy_combinations.mirrored_strategy_with_two_gpus]))\n+  def testInputSignatureForPerReplicaValues(self, distribution):\n+    with distribution.scope():\n+      def dataset_fn(input_context):\n+        return dali_const_dataset(4, 4, input_context.input_pipeline_id)"
      },
      {
        "body": "Hm. doesn't input_workers need to be adjusted? the number of \"input workers\" is supposed to the number of input pipelines: https://github.com/tensorflow/tensorflow/blob/308bc077374544b3e27597b604d5b5e484de48a6/tensorflow/python/distribute/input_lib.py#L147\r\n\r\nIn this case, each input device will feed one device.\r\n\r\nI think if you fix this, you can pretty much completely share the implementation w the PER_WORKER method.",
        "diff_hunk": "@@ -501,6 +501,22 @@ def _experimental_distribute_datasets_from_function(self, dataset_fn):\n         input_contexts,\n         self._container_strategy())\n \n+  def _experimental_distribute_datasets_from_function_per_replica(self, dataset_fn):\n+    input_contexts = []\n+    num_replicas = self.worker_devices\n+    for i in range(len(num_replicas)):\n+      input_contexts.append(distribute_lib.InputContext(\n+          num_input_pipelines=len(num_replicas),\n+          input_pipeline_id=i,\n+          num_replicas_in_sync=self._num_replicas_in_sync))\n+\n+    return input_lib.get_distributed_datasets_from_function_per_replica(\n+        dataset_fn,\n+        self._input_workers,"
      },
      {
        "body": "num_input_replicas = len(self.worker_devices)\r\n\r\nalthough, once you fix `self._input_workers` this should just be `self._input_workers.num_workers`\r\n\r\n",
        "diff_hunk": "@@ -501,6 +501,22 @@ def _experimental_distribute_datasets_from_function(self, dataset_fn):\n         input_contexts,\n         self._container_strategy())\n \n+  def _experimental_distribute_datasets_from_function_per_replica(self, dataset_fn):\n+    input_contexts = []\n+    num_replicas = self.worker_devices"
      },
      {
        "body": "`input_workers.compute_devices_for_worker(0)` does not look right.. this should be `input_workers.worker_devices`.\r\ncompute devices is the fed devices. here you want to be using the worker device for placing the input pipeline, and compute device for `_SingleReplicaDatasetIterator`",
        "diff_hunk": "@@ -1363,6 +1452,21 @@ def initialize(self):\n     return []\n \n \n+def _create_iterators_per_replica_with_input_context(input_contexts,\n+                                                    input_workers,\n+                                                    dataset_fn):\n+  \"\"\"Create a multidevice iterator per workers given a dataset function.\"\"\"\n+  iterators = []\n+  for ctx, device in zip(input_contexts, input_workers.compute_devices_for_worker(0)):"
      },
      {
        "body": "Does placing on the GPU work only w DALI? Would it work with non DALI input pipelines?\r\nI think as long as inside the `dataset_fn` you place the input ops etc on the GPU devices, things should work, even if there is a `with tf.device(\"cpu\")` wrapper here - which works in the general non DALI cases. \r\n",
        "diff_hunk": "@@ -1363,6 +1452,21 @@ def initialize(self):\n     return []\n \n \n+def _create_iterators_per_replica_with_input_context(input_contexts,\n+                                                    input_workers,\n+                                                    dataset_fn):\n+  \"\"\"Create a multidevice iterator per workers given a dataset function.\"\"\"\n+  iterators = []\n+  for ctx, device in zip(input_contexts, input_workers.compute_devices_for_worker(0)):\n+    with ops.device(device):"
      },
      {
        "body": "Do all other methods work as expected if `devices=[]`?",
        "diff_hunk": "@@ -1330,6 +1408,17 @@ def output_types(self):\n     return dataset_ops.get_legacy_output_types(self._iterator)\n \n \n+class _SingleReplicaDatasetIterator(_SingleWorkerDatasetIterator):\n+  def __init__(self, dataset, device):\n+    super(_SingleReplicaDatasetIterator, self).__init__(dataset, device, [])"
      },
      {
        "body": "hm, this is overridden because you don't want to use the `MultiDeviceIterator`? (now `OwnedMultiDeviceIterator`). Does it break things to use that with DALI?\r\n\r\nIn general, we say that dist strat will take care of prefetching the input to the GPUs etc. We do this for the PER_WORKER case (and distribute_dataset case). To not do that for `PER_REPLICA` case will introduce inconsistencies.\r\nIf this is absolutely not possible with DALI, then let me know and I will think of alternatives. \r\n\r\n",
        "diff_hunk": "@@ -1330,6 +1408,17 @@ def output_types(self):\n     return dataset_ops.get_legacy_output_types(self._iterator)\n \n \n+class _SingleReplicaDatasetIterator(_SingleWorkerDatasetIterator):\n+  def __init__(self, dataset, device):\n+    super(_SingleReplicaDatasetIterator, self).__init__(dataset, device, [])\n+\n+  def _make_iterator(self):\n+    \"\"\"Make appropriate iterator on the dataset.\"\"\"\n+    with ops.device(self._worker):\n+      self._iterator = iter(self._dataset)"
      },
      {
        "body": "This seems to be assuming that the shapes will always be static. Why is that the case? \r\n",
        "diff_hunk": "@@ -376,6 +398,21 @@ def out_of_range_fn(worker_index, device):\n \n     return values.regroup(replicas)\n \n+class DistributedIteratorForReplicas(DistributedIterator):\n+  \"\"\"Input Iterator for a distributed dataset on replicas.\"\"\"\n+  def __init__(self, input_workers, iterators, strategy):\n+    super(DistributedIteratorForReplicas, self).__init__(input_workers, iterators, strategy)\n+\n+  def get_next(self, name=None):\n+    \"\"\"Returns the next input from the iterator for all replicas.\"\"\"\n+    if not self._enable_get_next_as_optional:"
      },
      {
        "body": "why are you passing the worker here? ",
        "diff_hunk": "@@ -376,6 +398,21 @@ def out_of_range_fn(worker_index, device):\n \n     return values.regroup(replicas)\n \n+class DistributedIteratorForReplicas(DistributedIterator):\n+  \"\"\"Input Iterator for a distributed dataset on replicas.\"\"\"\n+  def __init__(self, input_workers, iterators, strategy):\n+    super(DistributedIteratorForReplicas, self).__init__(input_workers, iterators, strategy)\n+\n+  def get_next(self, name=None):\n+    \"\"\"Returns the next input from the iterator for all replicas.\"\"\"\n+    if not self._enable_get_next_as_optional:\n+      replicas = []\n+      for iterator in self._iterators:\n+        with ops.device(iterator._worker):\n+          next_out = iterator.get_next_as_list_static_shapes(iterator._worker)"
      },
      {
        "body": "should this be indented?",
        "diff_hunk": "@@ -864,6 +869,22 @@ def replica_fn_with_signature(inputs):\n           args=(next(iterator),))\n     ```\n \n+    In the case where you want to specify datasets `PER_REPLICA`, that is having \n+    a separate dataset per each device, you can specify as follows.\n+\n+    ```python\n+    train_dist_dataset = strategy.experimental_distribute_datasets_from_function(\n+        train_dataset_fn,\n+        distribute_lib.InputReplicationMode.PER_REPLICA)\n+\n+    train_dist_iterator = iter(train_dist_dataset)\n+        for epoch in range(NUM_EPOCHS):"
      },
      {
        "body": "why do we need this property on the strategy?",
        "diff_hunk": "@@ -681,6 +689,10 @@ def _num_replicas_in_sync(self):\n   @property\n   def worker_devices(self):\n     return self._devices\n+  \n+  @property\n+  def replication_mode(self):"
      },
      {
        "body": "This is intended to address Priya's ask @ https://github.com/tensorflow/tensorflow/pull/38968#pullrequestreview-404701790 ",
        "diff_hunk": "@@ -681,6 +689,10 @@ def _num_replicas_in_sync(self):\n   @property\n   def worker_devices(self):\n     return self._devices\n+  \n+  @property\n+  def replication_mode(self):"
      },
      {
        "body": "Ah, @kushanam I didn't mean to say you should add the replication_mode to the strategy constructor. Can't we keep it on the `experimental_distribute_datasets_from_function` method like before?\r\n\r\nDid you move this in order to adjust the strategy.input_workers attribute? I think currently we store it as an attribute because it never changes.\r\n\r\nBut once you have 2 modes, it can be different everytime one calls `experimental_distribute_datasets_from_function`, so we should no longer have it as a property on the strategy but compute it every time. \r\n\r\nWhat do you think? \r\n\r\n\r\nIn fact, there has been a [recent change](https://github.com/tensorflow/tensorflow/commit/4cdc8d04f306d54c6b992767ad6128a49dfdad52) where we are computing the input workers on the fly. \r\n\r\n\r\n\r\n\r\n",
        "diff_hunk": "@@ -264,9 +264,11 @@ def replica_fn(input):\n       the particular hardware is available.\n   \"\"\"\n \n-  def __init__(self, devices=None, cross_device_ops=None):\n-    extended = MirroredExtended(\n-        self, devices=devices, cross_device_ops=cross_device_ops)\n+  def __init__(self, devices=None, cross_device_ops=None,\n+               replication_mode=distribute_lib.InputReplicationMode.PER_WORKER):"
      },
      {
        "body": "this is not exactly what i was asking for :) see my other comment",
        "diff_hunk": "@@ -681,6 +689,10 @@ def _num_replicas_in_sync(self):\n   @property\n   def worker_devices(self):\n     return self._devices\n+  \n+  @property\n+  def replication_mode(self):"
      },
      {
        "body": "the input replication mode should be here (and also the docstring is now outdated)",
        "diff_hunk": "@@ -805,7 +809,8 @@ def train_step(inputs):\n     \"\"\"\n     return self._extended._experimental_distribute_dataset(dataset)  # pylint: disable=protected-access\n \n-  def experimental_distribute_datasets_from_function(self, dataset_fn):\n+  def experimental_distribute_datasets_from_function(self, \n+                                                     dataset_fn):"
      },
      {
        "body": "i am a little confused as to which changes in input_lib are still needed after changes in this file to adjust the input workers. \r\n\r\nlet's add tests for these changes. \r\n\r\n\r\n",
        "diff_hunk": "@@ -331,8 +335,12 @@ def _initialize_strategy(self, devices):\n   def _initialize_single_worker(self, devices):\n     \"\"\"Initializes the object for single-worker training.\"\"\"\n     self._devices = tuple(device_util.canonicalize(d) for d in devices)\n-    self._input_workers = input_lib.InputWorkers(\n-        ((device_util.canonicalize(\"/device:CPU:0\", devices[0]), devices),))\n+    if self._replication_mode == distribute_lib.InputReplicationMode.PER_WORKER:"
      },
      {
        "body": "Sorry I am confused. This is to address https://github.com/tensorflow/tensorflow/pull/38968#pullrequestreview-404701790 to have a consistant input_worker per your request: \r\nPER_WORKER: input_workers = [ (\"cpu\", (\"gpu:0\", \"gpu:1\")]\r\nPER_REPLICA: input_workers = [ (\"cpu\", \"gpu:0\"), (\"cpu\": \"gpu:1\")]\r\nCould you elaborate?",
        "diff_hunk": "@@ -264,9 +264,11 @@ def replica_fn(input):\n       the particular hardware is available.\n   \"\"\"\n \n-  def __init__(self, devices=None, cross_device_ops=None):\n-    extended = MirroredExtended(\n-        self, devices=devices, cross_device_ops=cross_device_ops)\n+  def __init__(self, devices=None, cross_device_ops=None,\n+               replication_mode=distribute_lib.InputReplicationMode.PER_WORKER):"
      },
      {
        "body": "@guptapriya  Moved the replication_mode to 'experimental_distribute_datasets_from_function' and removed it from the MirroredStrategy's constructor. Could you take a look please and let me know what you think?",
        "diff_hunk": "@@ -264,9 +264,11 @@ def replica_fn(input):\n       the particular hardware is available.\n   \"\"\"\n \n-  def __init__(self, devices=None, cross_device_ops=None):\n-    extended = MirroredExtended(\n-        self, devices=devices, cross_device_ops=cross_device_ops)\n+  def __init__(self, devices=None, cross_device_ops=None,\n+               replication_mode=distribute_lib.InputReplicationMode.PER_WORKER):"
      },
      {
        "body": "looks like changes from a bad rebase? this and the other tf lite file",
        "diff_hunk": "@@ -121,6 +121,8 @@ TFLITE_ATTRIBUTE_WEAK Interpreter::TfLiteDelegatePtr AcquireFlexDelegate() {\n   const char* filename_pywrap_tensorflow_internal =\n #if defined(_WIN32)\n       \"_pywrap_tensorflow_internal.pyd\";\n+#elif defined(__APPLE__)"
      },
      {
        "body": "this seems added to the wrong \"Args\" section? Should be added to args of `experimental_distribute_datasets_from_function` instead",
        "diff_hunk": "@@ -1058,6 +1062,7 @@ def experimental_distribute_dataset(self, dataset, options=None):\n     Args:\n       dataset: `tf.data.Dataset` that will be sharded across all replicas using\n         the rules stated above.\n+      replication_mode: Replication mode for the input function."
      }
    ],
    "body": "",
    "timestamp": "2025-05-06 01:27:15"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/16253",
    "comments": [
      "@samikama \r\nThanks for the nice work.  \r\nActually our team are working on the same task of integrating TensorRT with TensorFlow with the same design idea even with some similar function/variable name:). it looks that NVIDIA team run a little bit faster.\r\n\r\nMay I know have you made any benchmark with current implementation? Since with this integration TensorRT actually could leverage existing TF ops for its functionality completeness which is actually a headache for traditional TensorRT execution solution. \r\nBut there may be some potential performance overhead between the switch of TensorRT of TF(around several hundred us to 1ms observed in our scenarios), also if there are multiple switches between TensorRT and TF,  we need to be more careful about its overhead(considering TRT->TF->TRT execution plan).\r\n\r\nSo any performance number sharing will be really helpful.\r\n\r\nIf the work is solid enough, we would like to contribute more enhancement based on this PR and apply it into our production environment and I think this will be a win-win for community, NV and Alibaba.",
      "@yangjunpro \r\nThanks,\r\nWe have some preliminary measurements(@jjsjann123) for ourselves to see whether everything is working or not and we see a nice improvement. But we didn't go through performance in detail yet so we expect some more improvement soon. We can try to do some more measurements and get back to you.",
      "We've committed 76f6938bafeb81a4ca41b8dac2b9c83e1286fa95 for the build config and I'm merging the conflicts. Some difference between that commit and this PR:\r\n\r\n1. We need to use macro GOOGLE_TENSORRT to guard all c++ files, see [tensorrt_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/tensorrt_test.cc#L20) for an example.\r\n2. Bazel macro 'if_trt' now becomes 'if_tensorrt'\r\n3. The 'tensorrt' lib is renamed to 'nv_infer'\r\n\r\nMeanwhile I'll try to pull in this PR locally to make sure it works, thanks.",
      "I merged this PR locally with tensorflow master branch, and tried to build it with configure \"build tensorflow with tensorRT support: [y/N] y\"\r\nWhile building tensorflow from source, I got this error msg: \r\nTensorRT library version detected from /mnt/evanlu.lyf/tensorRT/TensorRT-3.0.0/include/NvInfer.h (4           //!<.0          //!<.0          //!<) does not match TF_TENSORRT_VERSION (4.0.0). To fix this rerun configure again.\r\nIs this PR is currently only internal for NV developers using tensorRT 4.0 version ?\r\n\r\nI just wondering, if it is possible to use tensorRT 3.0 instead? since v3.0 is released, then I could test this new feature locally.",
      "@evanluyifan - Thanks for the feedback.  No, this is not only for internal NV development.  The libnvinfer version numbering is simply done independently of the main TensorRT version number.  By any chance do you have the TRT 3.0.0 RC version instead of the more recent TRT 3.0.1 GA release?",
      "@evanluyifan the PR head is also failing on me after the master merge. As @cliffwoolley said, we are using the public TensorRT GA.\r\nIf you want to use it, try checkout commit 9384314 ",
      "Hi @aaroey,\r\n\r\n76f6938 seems to break our builds as well. I believe there could be some improvements to it. Does it work for you internally?\r\n\r\nThanks,\r\nSami\r\n",
      "@samikama I'm working on the fix.",
      "@aaroey Thanks!\r\nI pushed some updates addressing the review comments. Will test it with the merge once I saw your fix :)",
      "Hi @aaroey,\r\n\r\nI pushed some changes that fixes the build  for me.  Do these work for you as well?",
      "@samikama Thanks, I also pushed some fixes. It looks like your changes conflict with mine. Would you sync to head and retry?",
      "Hi @aaroey,\r\n\r\nI don't see any changes since 8e03944 . It was failing for me.",
      "@samikama Let me fix the problem in logger. It looks like the problem is in merging: https://github.com/tensorflow/tensorflow/pull/16253/files/ae740a67bdc01b991ead6ac047c774bff4d7bc8f..fe4260f71479374aee8340b8dacdbec8102d18c3#diff-39a0a00c611f917b0c3bbc776ddd6e01R31",
      "@samikama please check again.",
      "@aaroey,\r\nDone, sorry for the missed merge issue.\r\n",
      "@aaroey \r\nJust pushed another commit, removing the debug logs and replacing them with vlogs",
      "Build works for me.\r\nBut cannot import tensorrt in python\r\n```\r\n\r\nimport tensorflow.contrib.tensorrt\r\n2018-01-29 16:29:45.638178: F tensorflow/core/framework/op.cc:54] Non-OK-status: RegisterAlreadyLocked(op_data_factory) status: Already exists: Op with name _Arg\r\nAborted (core dumped)\r\n\r\n```",
      "@aaroey, @wujingyue \r\nIs there anything else left?",
      "It looks like, core/ops/function_ops.cc seems to be included in python bindings we create which cause static constructors to be included in our .so which in turn is causing our import to cause an abort due to multiple registration of \"_Arg\" system op, as observed by @jjsjann123 . At least this is my understanding. This file wasn't included before, @aaroey do you know what could have caused this change?",
      "@cliffwoolley  @jjsjann123   Thansk for your response,  have you fixed the issue on building with master branch ? \r\nI' currently using tensorRT 3.0 RC release.  Do I need to upgrade this to 3.0.1 ?\r\nAnd since this PR is based on the Public GA, why there is a TRT version check? which indicates we need tensorRT version >= 4.0 ?\r\n\r\nBTW, I think what this PR doing is adding tensorRT support into tensorflow, is there any schedule for this?\r\n\r\nPersonally I do agree that tensorflow with tensorRT native support is a great idea. On the other hand, we also have some guys working the XLA side. I just wondering, for the inference part, especially the CNN networks (I know TRT currently could only fully support CNN networks), what would be the future of \"XLA optimaztion\" vs \"tensorRT support\"?  ",
      "@evanluyifan - Please do update to TensorRT 3.0 GA, available from https://developer.nvidia.com/tensorrt .  The most recent build posted there is numbered 3.0.2.\r\n\r\nAgain you don't need a \"4.0\" version of TensorRT - there's no such thing.  TensorRT 3 will include libnvinfer4 out of the box.\r\n\r\nAnd yes this PR does provide integration of TRT into TF.  We have several future steps in mind to incrementally improve this integration and better leverage internal TF mechanisms; this is just the first step.  I cannot comment on a timeline, however.\r\n\r\nThanks,\r\nCliff",
      "@evanluyifan I think this should work with any trt version, if it doesn't, please let me know and provide a reliable repro case. There is still some issues with the build, while we're working on the fix,  this PR should be in within a few days.",
      "Looks like some of the later changes over-write benoitsteiner's fix on the grappler thing:\r\n\r\n```\r\nTurn the op_performance_data proto lib into a header only library by \u2026\r\n\u2026default\r\n\r\nPiperOrigin-RevId: 182621348\r\n\r\n```\r\n\r\nNot sure if applying it would break other TF stuff though.",
      "@samikama @jjsjann123 I'm working on fixing the build dependencies, and hopefully it would solve the problem of double linking. Just FYI.",
      "@samikama @jjsjann123 the import error is caused by grappler now:\r\n\r\n$ bazel query 'somepath(//tensorflow/contrib/tensorrt:init_py, //tensorflow/core:functional_ops_op_lib)'\r\n//tensorflow/contrib/tensorrt:init_py\r\n//tensorflow/contrib/tensorrt:trt_convert_py\r\n//tensorflow/contrib/tensorrt:wrap_conversion\r\n//tensorflow/contrib/tensorrt:_wrap_conversion.so\r\n//tensorflow/contrib/tensorrt:trt_conversion\r\n//tensorflow/core/grappler/costs:graph_properties\r\n//tensorflow/core:core_cpu_base\r\n//tensorflow/core:functional_ops_op_lib\r\n\r\nWould you help to fix that?",
      "Verified that the patch from Benoitsteiner is indeed in the code (making op_performance_data header only). \r\n\r\nDo remember hearing from him about the fix and another patch that is supposed to fix the dependency on grappler/clusters:single_machine.\r\nMaybe the second patch broke the first one ? Should we ping Benoitsteiner, unless @samikama had a fix for this.",
      "If the fix is going to take some time, I could revert the code back and use grappler from python side. That would get rid of the dependency. @aaroey ",
      "@jjsjann123 I fixed it and testing it locally after merging with changes from @aaroey. Will push it shortly\r\n",
      "@zheng-xq @aaroey ,\r\nI changed logging statements to VLOG but it seems VLOG logic is inverted. In order to see WARNING messages, INFO messages are also printed, to see FATALs all levels are displayed. \r\n`\r\n#define VLOG_IS_ON(lvl) \\\r\n  ((lvl) <= ::tensorflow::internal::LogMessage::MinVLogLevel())\r\n#endif\r\n`\r\nThis is counter-intuitive for me, also inverted logic wrt LOG. See below,\r\n`\r\nLogMessage::~LogMessage() {\r\n  // Read the min log level once during the first call to logging.\r\n  static int64 min_log_level = MinLogLevelFromEnv();\r\n  if (TF_PREDICT_TRUE(severity_ >= min_log_level)) GenerateLogMessage();\r\n}\r\n`\r\nIs this on purpose or is this a bug?\r\n\r\nShould I change it? If this is on purpose we need to invert VLOG severities in the code.  \r\n\r\nThanks,\r\nSami\r\n",
      "Hi @cliffwoolley , I have a simple question, does this change including the tensorRT optimizer? normally tensorRT save the optimized graph as a plan, while in this PR, I think the engine plan is stored in the cache, right?\r\nSo, could you show me where we do the optimize job and create a plan for trt_engine_op to load?"
    ],
    "review_comments": [
      {
        "body": "You can also use https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.h to infer shapes. It works directly on the GraphDef, so you don't have to create a Graph, and it's much more accurate than the ShapeRefiner",
        "diff_hunk": "@@ -0,0 +1,125 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/tensorrt/convert/inferShapes.h\"\n+#include <functional>\n+#include \"tensorflow/core/common_runtime/shape_refiner.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.pb_text.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+\n+namespace tensorflow {\n+namespace trt {\n+std::vector<tensorflow::DataType> getTypes(const tensorflow::OpDef& op,\n+                                           const tensorflow::NodeDef& nd,\n+                                           bool inp = true) {\n+  const auto& attrMap = nd.attr();\n+  auto getType = [&attrMap](decltype(\n+                     op.input_arg(0)) a) -> std::vector<tensorflow::DataType> {\n+    std::vector<tensorflow::DataType> tvec;\n+    if (!a.type_list_attr().empty()) {  // get the list types\n+      const auto& tl = attrMap.at(a.type_list_attr()).list();\n+      int tsize = tl.type_size();\n+      tvec.reserve(tsize);\n+      for (int t = 0; t < tsize; t++) {\n+        tvec.push_back(tl.type(t));\n+      }\n+      return tvec;\n+    }\n+    tensorflow::DataType cType = tensorflow::DT_INVALID;\n+    if (a.type() != tensorflow::DT_INVALID) {  // get defined types\n+      cType = a.type();\n+    } else if (!a.type_attr().empty()) {\n+      cType = attrMap.at(a.type_attr()).type();\n+    }\n+    if (!a.number_attr().empty()) {  // numbertypes\n+      int64 nTensors = attrMap.at(a.number_attr()).i();\n+      tvec = std::vector<tensorflow::DataType>(nTensors, cType);\n+      return tvec;\n+    }\n+    tvec.push_back(cType);\n+    return tvec;\n+  };\n+  std::vector<tensorflow::DataType> types;\n+  if (inp) {\n+    int n_inputs = op.input_arg_size();\n+    for (int i = 0; i < n_inputs; i++) {\n+      auto tout = getType(op.input_arg(i));\n+      LOG(DEBUG) << \"Node= \" << nd.name() << \" #inputs\" << tout.size();\n+      types.insert(types.end(), tout.begin(), tout.end());\n+    }\n+  } else {\n+    int n_outputs = op.output_arg_size();\n+    // types.resize(n_outputs);\n+    for (int i = 0; i < n_outputs; i++) {\n+      auto tout = getType(op.output_arg(i));\n+      LOG(DEBUG) << \"Node= \" << nd.name() << \" #outputs\" << tout.size();\n+      types.insert(types.end(), tout.begin(), tout.end());\n+    }\n+  }\n+  return types;\n+}\n+\n+tensorflow::Status inferShapes(const tensorflow::GraphDef& graph_def,\n+                               const std::vector<std::string>& output_names,\n+                               ShapeMap& shapes) {\n+  tensorflow::Graph g(OpRegistry::Global());\n+  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph("
      },
      {
        "body": "s/2015/2018/",
        "diff_hunk": "@@ -0,0 +1,203 @@\n+Copyright 2015 The TensorFlow Authors.  All rights reserved."
      },
      {
        "body": "Thanks @benoitsteiner ,\r\n\r\nWe had some issues with grappler but they are resolved in recent commits. We have it implemented in our working branch and will switch using grappler in the next PR.",
        "diff_hunk": "@@ -0,0 +1,125 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/tensorrt/convert/inferShapes.h\"\n+#include <functional>\n+#include \"tensorflow/core/common_runtime/shape_refiner.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.pb_text.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+\n+namespace tensorflow {\n+namespace trt {\n+std::vector<tensorflow::DataType> getTypes(const tensorflow::OpDef& op,\n+                                           const tensorflow::NodeDef& nd,\n+                                           bool inp = true) {\n+  const auto& attrMap = nd.attr();\n+  auto getType = [&attrMap](decltype(\n+                     op.input_arg(0)) a) -> std::vector<tensorflow::DataType> {\n+    std::vector<tensorflow::DataType> tvec;\n+    if (!a.type_list_attr().empty()) {  // get the list types\n+      const auto& tl = attrMap.at(a.type_list_attr()).list();\n+      int tsize = tl.type_size();\n+      tvec.reserve(tsize);\n+      for (int t = 0; t < tsize; t++) {\n+        tvec.push_back(tl.type(t));\n+      }\n+      return tvec;\n+    }\n+    tensorflow::DataType cType = tensorflow::DT_INVALID;\n+    if (a.type() != tensorflow::DT_INVALID) {  // get defined types\n+      cType = a.type();\n+    } else if (!a.type_attr().empty()) {\n+      cType = attrMap.at(a.type_attr()).type();\n+    }\n+    if (!a.number_attr().empty()) {  // numbertypes\n+      int64 nTensors = attrMap.at(a.number_attr()).i();\n+      tvec = std::vector<tensorflow::DataType>(nTensors, cType);\n+      return tvec;\n+    }\n+    tvec.push_back(cType);\n+    return tvec;\n+  };\n+  std::vector<tensorflow::DataType> types;\n+  if (inp) {\n+    int n_inputs = op.input_arg_size();\n+    for (int i = 0; i < n_inputs; i++) {\n+      auto tout = getType(op.input_arg(i));\n+      LOG(DEBUG) << \"Node= \" << nd.name() << \" #inputs\" << tout.size();\n+      types.insert(types.end(), tout.begin(), tout.end());\n+    }\n+  } else {\n+    int n_outputs = op.output_arg_size();\n+    // types.resize(n_outputs);\n+    for (int i = 0; i < n_outputs; i++) {\n+      auto tout = getType(op.output_arg(i));\n+      LOG(DEBUG) << \"Node= \" << nd.name() << \" #outputs\" << tout.size();\n+      types.insert(types.end(), tout.begin(), tout.end());\n+    }\n+  }\n+  return types;\n+}\n+\n+tensorflow::Status inferShapes(const tensorflow::GraphDef& graph_def,\n+                               const std::vector<std::string>& output_names,\n+                               ShapeMap& shapes) {\n+  tensorflow::Graph g(OpRegistry::Global());\n+  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph("
      },
      {
        "body": "Cherry picked the grappler fix.\r\nUpdated the shape inference as well as optimization pass. Pushed to the public repo (68e17d4).",
        "diff_hunk": "@@ -0,0 +1,125 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/tensorrt/convert/inferShapes.h\"\n+#include <functional>\n+#include \"tensorflow/core/common_runtime/shape_refiner.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.pb_text.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+\n+namespace tensorflow {\n+namespace trt {\n+std::vector<tensorflow::DataType> getTypes(const tensorflow::OpDef& op,\n+                                           const tensorflow::NodeDef& nd,\n+                                           bool inp = true) {\n+  const auto& attrMap = nd.attr();\n+  auto getType = [&attrMap](decltype(\n+                     op.input_arg(0)) a) -> std::vector<tensorflow::DataType> {\n+    std::vector<tensorflow::DataType> tvec;\n+    if (!a.type_list_attr().empty()) {  // get the list types\n+      const auto& tl = attrMap.at(a.type_list_attr()).list();\n+      int tsize = tl.type_size();\n+      tvec.reserve(tsize);\n+      for (int t = 0; t < tsize; t++) {\n+        tvec.push_back(tl.type(t));\n+      }\n+      return tvec;\n+    }\n+    tensorflow::DataType cType = tensorflow::DT_INVALID;\n+    if (a.type() != tensorflow::DT_INVALID) {  // get defined types\n+      cType = a.type();\n+    } else if (!a.type_attr().empty()) {\n+      cType = attrMap.at(a.type_attr()).type();\n+    }\n+    if (!a.number_attr().empty()) {  // numbertypes\n+      int64 nTensors = attrMap.at(a.number_attr()).i();\n+      tvec = std::vector<tensorflow::DataType>(nTensors, cType);\n+      return tvec;\n+    }\n+    tvec.push_back(cType);\n+    return tvec;\n+  };\n+  std::vector<tensorflow::DataType> types;\n+  if (inp) {\n+    int n_inputs = op.input_arg_size();\n+    for (int i = 0; i < n_inputs; i++) {\n+      auto tout = getType(op.input_arg(i));\n+      LOG(DEBUG) << \"Node= \" << nd.name() << \" #inputs\" << tout.size();\n+      types.insert(types.end(), tout.begin(), tout.end());\n+    }\n+  } else {\n+    int n_outputs = op.output_arg_size();\n+    // types.resize(n_outputs);\n+    for (int i = 0; i < n_outputs; i++) {\n+      auto tout = getType(op.output_arg(i));\n+      LOG(DEBUG) << \"Node= \" << nd.name() << \" #outputs\" << tout.size();\n+      types.insert(types.end(), tout.begin(), tout.end());\n+    }\n+  }\n+  return types;\n+}\n+\n+tensorflow::Status inferShapes(const tensorflow::GraphDef& graph_def,\n+                               const std::vector<std::string>& output_names,\n+                               ShapeMap& shapes) {\n+  tensorflow::Graph g(OpRegistry::Global());\n+  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph("
      },
      {
        "body": "One discussion we are having internally is whether we should have a separate \"--config=tensorrt\" here. Since users have to explicitly choose to enable tensorrt already.",
        "diff_hunk": "@@ -0,0 +1,42 @@\n+Using TensorRT in TensorFlow\n+============================\n+\n+This module provides necessary bindings and introduces TRT_engine_op\n+operator that wraps a subgraph in TensorRT.\n+\n+Compilation\n+-----------\n+\n+In order to compile the module, you need to have a local TensorRT\n+installation (libnvinfer.so and respective include files). During the\n+configuration step, TensorRT should be enabled and installation path\n+should be set. If installed through package managers (deb,rpm),\n+configure script should find the necessary components from the system\n+automatically. If installed from tar packages, user has to set path to\n+location where the library is installed during configuration.\n+\n+In order to enable TensorRT support, user has to add `--config=tensorrt` to\n+the build flags during the compilation such as\n+\n+```\n+bazel build --config=cuda --config=opt --config=tensorrt //tensorflow/tools/pip_package:build_pip_package"
      },
      {
        "body": "TF conventions seem to be accept tensor as well. You can have both names and tensors accepted.",
        "diff_hunk": "@@ -0,0 +1,42 @@\n+Using TensorRT in TensorFlow\n+============================\n+\n+This module provides necessary bindings and introduces TRT_engine_op\n+operator that wraps a subgraph in TensorRT.\n+\n+Compilation\n+-----------\n+\n+In order to compile the module, you need to have a local TensorRT\n+installation (libnvinfer.so and respective include files). During the\n+configuration step, TensorRT should be enabled and installation path\n+should be set. If installed through package managers (deb,rpm),\n+configure script should find the necessary components from the system\n+automatically. If installed from tar packages, user has to set path to\n+location where the library is installed during configuration.\n+\n+In order to enable TensorRT support, user has to add `--config=tensorrt` to\n+the build flags during the compilation such as\n+\n+```\n+bazel build --config=cuda --config=opt --config=tensorrt //tensorflow/tools/pip_package:build_pip_package\n+bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/\n+```\n+\n+After the installation of tensorflow package, TensorRT transformation\n+will be available. An example use is shown below.\n+\n+```python\n+import tensorflow as tf\n+import tensorflow.contrib.tensorrt as trt\n+#... create and train or load model\n+gdef=sess.graph.as_graph_def()\n+trt_gdef=trt.CreateInferenceGraph(gdef, #original graph_def\n+\t\t\t\t  [\"output\"], #name of output node(s)"
      },
      {
        "body": "If it is possible to add more configuration arguments later, maybe we should encourage \"max_batch_size=max_batch_size, max_workspace_size=max_workspace_size\" here. Or add a configuration object.",
        "diff_hunk": "@@ -0,0 +1,42 @@\n+Using TensorRT in TensorFlow\n+============================\n+\n+This module provides necessary bindings and introduces TRT_engine_op\n+operator that wraps a subgraph in TensorRT.\n+\n+Compilation\n+-----------\n+\n+In order to compile the module, you need to have a local TensorRT\n+installation (libnvinfer.so and respective include files). During the\n+configuration step, TensorRT should be enabled and installation path\n+should be set. If installed through package managers (deb,rpm),\n+configure script should find the necessary components from the system\n+automatically. If installed from tar packages, user has to set path to\n+location where the library is installed during configuration.\n+\n+In order to enable TensorRT support, user has to add `--config=tensorrt` to\n+the build flags during the compilation such as\n+\n+```\n+bazel build --config=cuda --config=opt --config=tensorrt //tensorflow/tools/pip_package:build_pip_package\n+bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/\n+```\n+\n+After the installation of tensorflow package, TensorRT transformation\n+will be available. An example use is shown below.\n+\n+```python\n+import tensorflow as tf\n+import tensorflow.contrib.tensorrt as trt\n+#... create and train or load model\n+gdef=sess.graph.as_graph_def()\n+trt_gdef=trt.CreateInferenceGraph(gdef, #original graph_def\n+\t\t\t\t  [\"output\"], #name of output node(s)\n+\t\t\t\t  max_batch_size, #maximum batch size to run the inference\n+\t\t\t\t  max_workspace_size # max memory for TensorRT to use "
      },
      {
        "body": "This is fine in this PR. But since the op name may change. For example: BiasAdd is after BiasAddV1. Newer versions may be added and this would easily get out of sync.\r\n\r\nIt's a good idea to split this into a registration mechanism. And then we can register the kernel to have this attribute in the kernel implementation.\r\n\r\nA TODO item is good enough.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\","
      },
      {
        "body": "TF convention is \"const Graph& graph\"",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\",\n+      \"Add\",      \"Mul\",   \"Sub\",    \"Rsqrt\",   \"Pad\"  // \"Placeholder\" ,\"Mean\"\n+                                                       // TODO(ben,jie): ...\n+  };\n+  if (output_nodes.count(node_def.name())) return false;\n+  return candidate_ops.count(node_def.op());\n+}\n+\n+void GetSubGraphIncomingEdges(tensorflow::Graph const& graph,"
      },
      {
        "body": "Sure.\r\nFor this PR, current segmentation contains only the bare minimum to get basic stuff working. We need more than op type to determine whether it is TRT-compatible or not.\r\nI'll put a TODO here and maybe we can get back to it later.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\","
      },
      {
        "body": "Remove?",
        "diff_hunk": "@@ -0,0 +1,42 @@\n+# -*- python -*-\n+# Description:\n+#   provide tensorrt information\n+\n+#TODO(Sami) these needs to be defined "
      },
      {
        "body": "This seems to be duplicate of the \"trt_enabled\" config settings defined in third_party/tensorrt/BUILD.tpl? And it's not used anywhere?",
        "diff_hunk": "@@ -358,6 +358,14 @@ config_setting(\n     },\n )\n \n+config_setting("
      },
      {
        "body": "s/2015/2018",
        "diff_hunk": "@@ -0,0 +1,19 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved."
      },
      {
        "body": "s/2017/2018, I think this applies to other files as well.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved."
      },
      {
        "body": "Remove?",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\""
      },
      {
        "body": "Seems not being used. Remove? Same in other files.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)"
      },
      {
        "body": "The TF convention is to add a 'namespace tensorflow' as the outermost namespace. And please fix other c++ files as well.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------"
      },
      {
        "body": "nit: we can add a 'using std::set' above inside the namespace to avoid typing std:: multiple times, and same for string.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\",\n+      \"Add\",      \"Mul\",   \"Sub\",    \"Rsqrt\",   \"Pad\"  // \"Placeholder\" ,\"Mean\"\n+                                                       // TODO(ben,jie): ...\n+  };\n+  if (output_nodes.count(node_def.name())) return false;\n+  return candidate_ops.count(node_def.op());\n+}\n+\n+void GetSubGraphIncomingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,"
      },
      {
        "body": "Remove this? And other commented lines as well. If some are for TODOs, add a TODO instead.",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\",\n+      \"Add\",      \"Mul\",   \"Sub\",    \"Rsqrt\",   \"Pad\"  // \"Placeholder\" ,\"Mean\"\n+                                                       // TODO(ben,jie): ...\n+  };\n+  if (output_nodes.count(node_def.name())) return false;\n+  return candidate_ops.count(node_def.op());\n+}\n+\n+void GetSubGraphIncomingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,\n+                              tensorflow::EdgeSet* incoming_edges) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node const* node = graph.FindNodeId(node_id);\n+    LOG(DEBUG) << node->name() << \" has incoming edges: \";\n+    for (tensorflow::Edge const* edge : node->in_edges()) {\n+      if (!subgraph_node_ids.count(edge->src()->id()) &&\n+          !edge->src()->IsSource()) {\n+        LOG(DEBUG) << edge->src()->name() << \", \";\n+        incoming_edges->insert(edge);\n+      }\n+    }\n+  }\n+}\n+\n+void GetSubGraphOutgoingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,\n+                              tensorflow::EdgeSet* outgoing_edges) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node const* node = graph.FindNodeId(node_id);\n+    LOG(DEBUG) << node->name() << \" has outgoing edges: \";\n+    for (tensorflow::Edge const* edge : node->out_edges()) {\n+      if (!subgraph_node_ids.count(edge->dst()->id()) &&\n+          !edge->dst()->IsSink()) {\n+        outgoing_edges->insert(edge);\n+      }\n+    }\n+  }\n+}\n+\n+std::pair<std::string, int> ParseTensorName(std::string name,\n+                                            int default_idx = 0) {\n+  int idx = default_idx;\n+  size_t sep = name.find_last_of(':');\n+  if (sep != std::string::npos) {\n+    name = name.substr(0, sep);\n+    idx = std::stoi(name.substr(sep + 1));\n+  }\n+  return std::make_pair(name, idx);\n+}\n+\n+std::unordered_map<std::string, std::vector<int>> BuildTensorNameMap(\n+    const std::vector<std::string>& tensor_names) {\n+  std::unordered_map<std::string, std::vector<int>> result;\n+  for (std::string const& tensor_name : tensor_names) {\n+    std::string node_name;\n+    int index;\n+    std::tie(node_name, index) = ParseTensorName(tensor_name);\n+    result[node_name].push_back(index);\n+  }\n+  return result;\n+}\n+\n+tensorflow::Status ConvertSubGraphToTensorRT(\n+    tensorflow::Graph& graph, const std::vector<std::string>& output_names,\n+    const std::set<int>& subgraph_node_ids, size_t max_batch_size,\n+    size_t max_workspace_size,\n+    const tensorflow::grappler::GraphProperties& graph_properties) {\n+  tensorflow::EdgeSet subgraph_incoming_edges;\n+  GetSubGraphIncomingEdges(graph, subgraph_node_ids, &subgraph_incoming_edges);\n+\n+  std::vector<std::pair<int, int>> subgraph_inputs;\n+\n+\n+  // Collect inputs by looking for incoming edges\n+  for (tensorflow::Edge const* edge : subgraph_incoming_edges) {\n+    subgraph_inputs.push_back({edge->src()->id(), edge->src_output()});\n+  }\n+  std::set<std::pair<int, int>> subgraph_outputs_set;\n+  // Collect outputs referenced from output_names\n+  auto output_name_to_index_map = BuildTensorNameMap(output_names);\n+  // for (int node_id : subgraph_node_ids_no_placeholder) {"
      },
      {
        "body": "We can use EnumToDataType to get the size: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.h#L377",
        "diff_hunk": "@@ -0,0 +1,1761 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+\n+#include <algorithm>\n+#include <fstream>\n+#include <list>\n+#include <map>\n+#include <memory>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <utility>\n+#include <vector>\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+//  Check if the types are equal. Cast to int first so that failure log message\n+//  would work!\n+#define CHECK_EQ_TYPE(val1, val2) CHECK_EQ((int)val1, (int)val2)\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+inline int get_dtype_size(nvinfer1::DataType trt_dtype) {\n+  switch (trt_dtype) {\n+    case nvinfer1::DataType::kFLOAT:\n+      return 4;\n+    case nvinfer1::DataType::kINT8:\n+      return 1;\n+    case nvinfer1::DataType::kHALF:\n+      return 2;\n+    default:\n+      return -1;\n+  }\n+}\n+\n+inline int get_dtype_size(tensorflow::DataType trt_dtype) {\n+  switch (trt_dtype) {"
      },
      {
        "body": "The TF convention is to use the double slash form ('//') for comments.",
        "diff_hunk": "@@ -0,0 +1,1761 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+\n+#include <algorithm>\n+#include <fstream>\n+#include <list>\n+#include <map>\n+#include <memory>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <utility>\n+#include <vector>\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+//  Check if the types are equal. Cast to int first so that failure log message\n+//  would work!\n+#define CHECK_EQ_TYPE(val1, val2) CHECK_EQ((int)val1, (int)val2)\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+inline int get_dtype_size(nvinfer1::DataType trt_dtype) {\n+  switch (trt_dtype) {\n+    case nvinfer1::DataType::kFLOAT:\n+      return 4;\n+    case nvinfer1::DataType::kINT8:\n+      return 1;\n+    case nvinfer1::DataType::kHALF:\n+      return 2;\n+    default:\n+      return -1;\n+  }\n+}\n+\n+inline int get_dtype_size(tensorflow::DataType trt_dtype) {\n+  switch (trt_dtype) {\n+    case tensorflow::DataType::DT_FLOAT:\n+      return 4;\n+    case tensorflow::DataType::DT_INT8:\n+      return 1;\n+    case tensorflow::DataType::DT_HALF:\n+      return 2;\n+    case tensorflow::DataType::DT_INT32:\n+      return 4;\n+    default:\n+      return -1;\n+  }\n+}\n+\n+inline tensorflow::Status convert_dtype(tensorflow::DataType tf_dtype,\n+                                        nvinfer1::DataType* trt_dtype) {\n+  switch (tf_dtype) {\n+    case tensorflow::DataType::DT_FLOAT:\n+      *trt_dtype = nvinfer1::DataType::kFLOAT;\n+      break;\n+    case tensorflow::DataType::DT_INT8:\n+      *trt_dtype = nvinfer1::DataType::kINT8;\n+      break;\n+    case tensorflow::DataType::DT_HALF:\n+      *trt_dtype = nvinfer1::DataType::kHALF;\n+      break;\n+    default:\n+      return tensorflow::errors::InvalidArgument(\"Unsupported data type\");\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+inline nvinfer1::Dims get_tensor_shape(const tensorflow::Tensor& tensor) {\n+  nvinfer1::Dims dims;\n+  dims.nbDims = tensor.dims();\n+  for (int i = 0; i < dims.nbDims; i++) {\n+    dims.d[i] = tensor.dim_size(i);\n+  }\n+  return dims;\n+}\n+\n+inline int64_t get_shape_size(nvinfer1::Dims shape) {\n+  // Returns total number of elements in shape\n+  int64_t count = 1;\n+  for (int d = 0; d < shape.nbDims; ++d) {\n+    count *= shape.d[d];\n+  }\n+  return count;\n+}\n+\n+static std::vector<std::pair<int, int>> createSamePadding(\n+    nvinfer1::DimsHW& stride, nvinfer1::DimsHW& kernel,\n+    std::vector<int64_t> inputDims) {\n+  std::vector<std::pair<int, int>> padding(inputDims.size());\n+  CHECK_EQ((size_t)stride.nbDims, inputDims.size());  // TODO(jie): N+C? NC+?\n+\n+  for (size_t i = 0; i < inputDims.size(); ++i) {\n+    /* formula to calculate the padding */"
      },
      {
        "body": "Yes, I think we need one runtime per device? And is deserializeCudaEngine() thread-safe? If yes, you may see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_using_gemm.cc#L314 for an example, and we can get the device from the context and use the gpu id as the key: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.h#L91. ",
        "diff_hunk": "@@ -0,0 +1,183 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/tensorrt/kernels/trt_engine_op.h\"\n+#include <cuda_runtime_api.h>\n+#include <sstream>\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+// Use TF logging f\n+\n+\n+namespace tensorflow {\n+static ::tensorflow::tensorrt::Logger gLogger;\n+\n+using namespace nvinfer1;\n+\n+namespace tensorrt {\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+  // char *gieModelStream{nullptr};\n+  // size_t size{0};\n+\n+  // read serialized_engine\n+  std::string serialized_engine;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"serialized_engine\", &serialized_engine));\n+\n+  // register input output node name in trt_sub_graph\n+  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+\n+  // TODO(samikama) runtime should be taken from a resourcemanager as well."
      },
      {
        "body": "Do we really need these logs? If yes we can wrap them in a separate function and invoke it here.",
        "diff_hunk": "@@ -0,0 +1,183 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/tensorrt/kernels/trt_engine_op.h\"\n+#include <cuda_runtime_api.h>\n+#include <sstream>\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+// Use TF logging f\n+\n+\n+namespace tensorflow {\n+static ::tensorflow::tensorrt::Logger gLogger;\n+\n+using namespace nvinfer1;\n+\n+namespace tensorrt {\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+  // char *gieModelStream{nullptr};\n+  // size_t size{0};\n+\n+  // read serialized_engine\n+  std::string serialized_engine;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"serialized_engine\", &serialized_engine));\n+\n+  // register input output node name in trt_sub_graph\n+  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+\n+  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n+  //  Only engine should be in the op and context and runtime should be taken\n+  //  from resourcemanager\n+  IRuntime* infer = createInferRuntime(gLogger);\n+  trt_engine_ptr_.reset(infer->deserializeCudaEngine(\n+      serialized_engine.c_str(), serialized_engine.size(), nullptr));\n+\n+  trt_context_ptr_.reset(trt_engine_ptr_->createExecutionContext());\n+  // runtime is safe to delete after engine creation\n+  infer->destroy();\n+  std::stringstream oss;\n+  // debug iterate through all binding instances\n+  for (int i = 0; i < trt_engine_ptr_->getNbBindings(); i++) {"
      },
      {
        "body": "For graph with 4 nodes A, B, C, D where A, B, D are trt compatible but C is not, and edges set {A->B, A->C, B->D, C->D}, the current solution seems to prefer contracting C->D comparing to A->C. Can we document this behavior? Is there an easy way to use some heuristics to determine which option is better?",
        "diff_hunk": "@@ -0,0 +1,259 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+\n+#include <set>\n+#include <string>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"tensorflow/contrib/tensorrt/segment/union_find.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace segment {\n+\n+//------------------------------------------------------------------------------\n+namespace {\n+\n+//------------------------------------------------------------------------------\n+bool CanContractEdge(const tensorflow::Edge* edge,\n+                     const tensorflow::Graph& graph) {\n+  const tensorflow::Node* src = edge->src();\n+  const tensorflow::Node* dst = edge->dst();\n+\n+  // Can't contract edge if doing so would cause a cycle in the\n+  // graph. So, if there is a directed path from 'src' to 'dst', other\n+  // than 'edge' (or any other direct edge from 'src' to 'dst'), then\n+  // combining 'src' and 'dst' will cause a cycle along that path.\n+  //\n+  // In practice, to avoid modifying the graph and to take advantage\n+  // of existing graph functions, we perform an equivalent.\n+  //   1. Get all nodes incoming to 'dst', excluding 'src'\n+  //   2. Reverse DFS from those nodes\n+  //   3. If reverse DFS reaches 'src' then we have a cycle\n+  std::vector<tensorflow::Node*> dfs_start_nodes;\n+  for (tensorflow::Node* node : dst->in_nodes()) {\n+    if (node != src) {\n+      dfs_start_nodes.push_back(node);\n+    }\n+  }\n+\n+  bool is_cycle = false;\n+  if (!dfs_start_nodes.empty()) {\n+    tensorflow::ReverseDFSFrom(graph, dfs_start_nodes, {},\n+                               [&is_cycle, src](tensorflow::Node* node) {\n+                                 if (node == src) {\n+                                   is_cycle = true;\n+                                 }\n+                               });\n+  }\n+\n+  return !is_cycle;\n+}\n+\n+//------------------------------------------------------------------------------\n+void ContractEdge(tensorflow::Edge* edge, tensorflow::Graph* graph,\n+                  std::vector<const tensorflow::Edge*>* remove_edges) {\n+  // Transfer all inputs and outputs of 'dst' to 'src' except edges\n+  // connecting the two.\n+  tensorflow::Node* src = edge->src();\n+  tensorflow::Node* dst = edge->dst();\n+\n+  // We can use '0' for input/output index because we don't need them\n+  // to be accurate for the way we are using the graph.\n+  std::vector<const tensorflow::Edge*> in_edges(dst->in_edges().begin(),\n+                                                dst->in_edges().end());\n+  for (const tensorflow::Edge* in_edge : in_edges) {\n+    if (in_edge->src() != src) {\n+      tensorflow::Edge* e = const_cast<tensorflow::Edge*>(in_edge);\n+      if (e->src() == graph->source_node()) {\n+        graph->AddEdge(e->src(), e->src_output(), src,\n+                       tensorflow::Graph::kControlSlot);\n+      } else {\n+        graph->AddEdge(e->src(), e->src_output(), src, 0 /* input index */);\n+      }\n+    }\n+  }\n+\n+  std::vector<const tensorflow::Edge*> out_edges(dst->out_edges().begin(),\n+                                                 dst->out_edges().end());\n+  for (const tensorflow::Edge* out_edge : out_edges) {\n+    tensorflow::Edge* e = const_cast<tensorflow::Edge*>(out_edge);\n+    if (e->dst() == graph->sink_node()) {\n+      graph->AddEdge(src, tensorflow::Graph::kControlSlot, e->dst(),\n+                     e->dst_input());\n+    } else {\n+      graph->AddEdge(src, 0 /* output index */, e->dst(), e->dst_input());\n+    }\n+  }\n+\n+  // Return the edges that must be removed to disconnect 'dst' from\n+  // the graph. We don't actually remove 'dst' since the caller holds\n+  // references to all the nodes.\n+  for (const auto& in_edge : dst->in_edges()) {\n+    remove_edges->push_back(in_edge);\n+  }\n+  for (const auto& out_edge : dst->out_edges()) {\n+    remove_edges->push_back(out_edge);\n+  }\n+}\n+\n+}  // namespace\n+\n+//------------------------------------------------------------------------------\n+tensorflow::Status SegmentGraph(\n+    const tensorflow::GraphDef& gdef,\n+    const std::function<bool(const tensorflow::NodeDef&)>& candidate_fn,\n+    const SegmentOptions& options, SegmentNodesVector* segments) {\n+  // Create a Graph representation of the GraphDef.\n+  tensorflow::FunctionLibraryDefinition flib(tensorflow::OpRegistry::Global(),\n+                                             gdef.library());\n+  tensorflow::Graph graph(flib);\n+  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph(\n+      tensorflow::GraphConstructorOptions(), gdef, &graph));\n+\n+  // tensorflow::DumpGraph(\"Pre-Segment\", &graph);\n+\n+  // Use a union-find to collect the nodes that belong to the same\n+  // segment. A node value of nullptr indicates that the node is not a\n+  // candidate for TRT.\n+  std::vector<UnionFind<tensorflow::Node*>> node_segments;\n+  for (int i = 0; i < graph.num_node_ids(); ++i) {\n+    tensorflow::Node* node = graph.FindNodeId(i);\n+    if (!candidate_fn(node->def())) {\n+      node = nullptr;\n+    }\n+    node_segments.emplace_back(node);\n+  }\n+\n+  // Visit nodes in reverse topological order and use edge\n+  // contraction to merge candidate nodes.\n+  std::vector<tensorflow::Node*> order;\n+  tensorflow::GetPostOrder(graph, &order);\n+\n+  for (const tensorflow::Node* node : order) {\n+    // All output nodes of 'node' have been visited...\n+    VLOG(2) << \"Trying node \" << node->name();\n+\n+    // 'node' must be a TRT candidate...\n+    if (node_segments[node->id()].Value() == nullptr) {\n+      VLOG(2) << \"... not a TRT candidate\";\n+      continue;\n+    }\n+\n+    // Contract output edges to combine 'node' with output\n+    // nodes. Iterate since combining two nodes may unblock other\n+    // combining.\n+    while (true) {\n+      std::set<const tensorflow::Edge*> contract_edges;\n+      for (const tensorflow::Edge* out_edge : node->out_edges()) {\n+        VLOG(2) << \"... out node \" << out_edge->dst()->name();\n+\n+        // Out node must be TRT candidate...\n+        if (node_segments[out_edge->dst()->id()].Value() == nullptr) {\n+          VLOG(2) << \"... ... not a TRT candidate\";\n+          continue;\n+        }\n+\n+        if (CanContractEdge(out_edge, graph)) {\n+          VLOG(2) << \"... ... can contract\";\n+          contract_edges.insert(out_edge);"
      },
      {
        "body": "Several changes in this file don't follow [TF coding style](https://www.tensorflow.org/community/style_guide). ",
        "diff_hunk": "@@ -382,13 +384,12 @@ def set_build_var(environ_cp, var_name, query_item, option_name,\n \n   var = str(int(get_var(environ_cp, var_name, query_item, enabled_by_default)))\n   environ_cp[var_name] = var\n-  if var == '1':\n-    write_to_bazelrc('build --define %s=true' % option_name)\n-  elif bazel_config_name is not None:\n-    # TODO(mikecase): Migrate all users of configure.py to use --config Bazel\n-    # options and not to set build configs through environment variables.\n-    write_to_bazelrc('build:%s --define %s=true'\n-                     % (bazel_config_name, option_name))\n+  # TODO(mikecase): Migrate all users of configure.py to use --config Bazel\n+  # options and not to set build configs through environment variables.\n+  if var=='1':"
      },
      {
        "body": "It'd be really great to add a IfChangeThenChange lint check. This will make sure you change convert_nodes.cc when this list is changed. For example, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto#L9",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\","
      },
      {
        "body": "This PR contains much more than the title \"introducing TensorRT operator\". I'd like to see a more detailed description listing specific changes so readers know what components you add or change and how they are related. For example, \r\n\r\n- Add a TRTEngineOp that encapsulates a TensorRT executable. \r\n- Add CreateInferenceGraph to contract a TensorRT-compilable subgraph to a TRTEngineOp. \r\n- Add configurations that blah blah. ",
        "diff_hunk": "@@ -0,0 +1,42 @@\n+Using TensorRT in TensorFlow"
      },
      {
        "body": "Remove all code commented out. ",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\",\n+      \"Add\",      \"Mul\",   \"Sub\",    \"Rsqrt\",   \"Pad\"  // \"Placeholder\" ,\"Mean\"\n+                                                       // TODO(ben,jie): ...\n+  };\n+  if (output_nodes.count(node_def.name())) return false;\n+  return candidate_ops.count(node_def.op());\n+}\n+\n+void GetSubGraphIncomingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,\n+                              tensorflow::EdgeSet* incoming_edges) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node const* node = graph.FindNodeId(node_id);\n+    LOG(DEBUG) << node->name() << \" has incoming edges: \";\n+    for (tensorflow::Edge const* edge : node->in_edges()) {\n+      if (!subgraph_node_ids.count(edge->src()->id()) &&\n+          !edge->src()->IsSource()) {\n+        LOG(DEBUG) << edge->src()->name() << \", \";\n+        incoming_edges->insert(edge);\n+      }\n+    }\n+  }\n+}\n+\n+void GetSubGraphOutgoingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,\n+                              tensorflow::EdgeSet* outgoing_edges) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node const* node = graph.FindNodeId(node_id);\n+    LOG(DEBUG) << node->name() << \" has outgoing edges: \";\n+    for (tensorflow::Edge const* edge : node->out_edges()) {\n+      if (!subgraph_node_ids.count(edge->dst()->id()) &&\n+          !edge->dst()->IsSink()) {\n+        outgoing_edges->insert(edge);\n+      }\n+    }\n+  }\n+}\n+\n+std::pair<std::string, int> ParseTensorName(std::string name,\n+                                            int default_idx = 0) {\n+  int idx = default_idx;\n+  size_t sep = name.find_last_of(':');\n+  if (sep != std::string::npos) {\n+    name = name.substr(0, sep);\n+    idx = std::stoi(name.substr(sep + 1));\n+  }\n+  return std::make_pair(name, idx);\n+}\n+\n+std::unordered_map<std::string, std::vector<int>> BuildTensorNameMap(\n+    const std::vector<std::string>& tensor_names) {\n+  std::unordered_map<std::string, std::vector<int>> result;\n+  for (std::string const& tensor_name : tensor_names) {\n+    std::string node_name;\n+    int index;\n+    std::tie(node_name, index) = ParseTensorName(tensor_name);\n+    result[node_name].push_back(index);\n+  }\n+  return result;\n+}\n+\n+tensorflow::Status ConvertSubGraphToTensorRT(\n+    tensorflow::Graph& graph, const std::vector<std::string>& output_names,\n+    const std::set<int>& subgraph_node_ids, size_t max_batch_size,\n+    size_t max_workspace_size,\n+    const tensorflow::grappler::GraphProperties& graph_properties) {\n+  tensorflow::EdgeSet subgraph_incoming_edges;\n+  GetSubGraphIncomingEdges(graph, subgraph_node_ids, &subgraph_incoming_edges);\n+\n+  std::vector<std::pair<int, int>> subgraph_inputs;\n+\n+\n+  // Collect inputs by looking for incoming edges\n+  for (tensorflow::Edge const* edge : subgraph_incoming_edges) {\n+    subgraph_inputs.push_back({edge->src()->id(), edge->src_output()});\n+  }\n+  std::set<std::pair<int, int>> subgraph_outputs_set;\n+  // Collect outputs referenced from output_names\n+  auto output_name_to_index_map = BuildTensorNameMap(output_names);\n+  // for (int node_id : subgraph_node_ids_no_placeholder) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node* node = graph.FindNodeId(node_id);\n+    if (output_name_to_index_map.count(node->name())) {\n+      for (int index : output_name_to_index_map.at(node->name())) {\n+        subgraph_outputs_set.insert({node_id, index});\n+      }\n+    }\n+  }\n+  // Collect outputs referenced from outgoing edges\n+  tensorflow::EdgeSet subgraph_outgoing_edges;\n+  // GetSubGraphOutgoingEdges(graph, subgraph_node_ids_no_placeholder,\n+  //  &subgraph_outgoing_edges);\n+  GetSubGraphOutgoingEdges(graph, subgraph_node_ids, &subgraph_outgoing_edges);\n+  for (tensorflow::Edge const* edge : subgraph_outgoing_edges) {\n+    subgraph_outputs_set.insert({edge->src()->id(), edge->src_output()});\n+  }\n+  // Impose an ordering on the outputs\n+  std::vector<std::pair<int, int>> subgraph_outputs(\n+      subgraph_outputs_set.begin(), subgraph_outputs_set.end());\n+  // Build TensorRT node and add it to the graph\n+  tensorflow::NodeDef trt_node_def;\n+  TF_RETURN_IF_ERROR(ConvertSubGraphToTensorRTNodeDef(\n+      graph, subgraph_node_ids, subgraph_inputs, subgraph_outputs,\n+      max_batch_size, max_workspace_size, graph_properties, &trt_node_def));\n+  tensorflow::Status status;\n+  tensorflow::Node* trt_node = graph.AddNode(trt_node_def, &status);\n+\n+  TF_RETURN_IF_ERROR(status);\n+\n+  // Re-map outgoing edges to use the new TRT node instead of the orig subgraph\n+  std::map<std::pair<int, int>, int> subgraph_edge_to_output_map;\n+  for (size_t i = 0; i < subgraph_outputs.size(); ++i) {\n+    subgraph_edge_to_output_map.insert({subgraph_outputs.at(i), i});\n+  }\n+  TF_RETURN_IF_ERROR(status);\n+  for (tensorflow::Edge const* edge : subgraph_outgoing_edges) {\n+    std::pair<int, int> old_src = {edge->src()->id(), edge->src_output()};\n+    int new_src_output = subgraph_edge_to_output_map.at(old_src);\n+    graph.UpdateEdge(trt_node, new_src_output, edge->dst(), edge->dst_input());\n+  }\n+  // Remove the original subgraph\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node* node = graph.FindNodeId(node_id);\n+    // Don't remove the input placeholders\n+    if (node->type_string() == \"Placeholder\") {\n+      continue;\n+    }\n+    graph.RemoveNode(node);\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status BuildNodeMap(\n+    const tensorflow::Graph& graph,\n+    std::unordered_map<std::string, tensorflow::Node*>* node_map) {\n+  for (auto* node : graph.op_nodes()) {\n+    if (!node_map->insert({node->name(), node}).second) {\n+      return tensorflow::errors::AlreadyExists(\n+          \"Node name is not unique in graph: \" + node->name());\n+    }\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+}  // namespace\n+\n+tensorflow::Status ConvertGraphDefToTensorRT(\n+    const tensorflow::GraphDef& graph_def,\n+    const std::vector<std::string>& output_names, size_t max_batch_size,\n+    size_t max_workspace_size, tensorflow::GraphDef* new_graph_def) {\n+\n+  // optimization pass\n+  tensorflow::grappler::GrapplerItem item;\n+  item.fetch = output_names;\n+  tensorflow::GraphDef gdef;\n+\n+  // layout optimization\n+  item.graph = graph_def;\n+  tensorflow::grappler::LayoutOptimizer optimizer;\n+  tensorflow::grappler::Cluster* gCluster;\n+\n+  // virtual cluster\n+  tensorflow::DeviceProperties device_properties;\n+  device_properties.set_type(\"GPU\");\n+  device_properties.mutable_environment()->insert({\"architecture\", \"6\"});\n+  gCluster =\n+    new tensorflow::grappler::VirtualCluster({{\"/GPU:0\", device_properties}});\n+\n+  // single machine\n+  int num_cpu_cores = tensorflow::grappler::GetNumAvailableLogicalCPUCores();\n+  int num_gpus = tensorflow::grappler::GetNumAvailableGPUs();\n+  LOG(DEBUG) << \"cpu_cores: \" << num_cpu_cores;\n+  LOG(DEBUG) << \"gpus: \" << num_gpus;\n+  // int timeout_s = 60 * 10;\n+  // gCluster = new tensorflow::grappler::SingleMachine(\n+  //                  timeout_s, num_cpu_cores, num_gpus);\n+\n+  tensorflow::Status status = optimizer.Optimize(gCluster, item, &gdef);\n+\n+  if (status !=tensorflow::Status::OK())\n+    return status;\n+ \n+  // constant folding\n+  item.graph = gdef;\n+  tensorflow::grappler::ConstantFolding fold(nullptr);\n+  status = fold.Optimize(nullptr, item, &gdef);\n+  if (status !=tensorflow::Status::OK())\n+    return status;\n+  \n+  // AJ refactoring shape inference through grappler/GraphProperties.\n+  tensorflow::grappler::GraphProperties static_graph_properties(item);\n+  static_graph_properties.InferStatically(false);\n+  // TF_CHECK_OK(static_graph_prop.InferStatically(false));"
      },
      {
        "body": "const std::set<std::string>&",
        "diff_hunk": "@@ -0,0 +1,308 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_graph.h\"\n+\n+#include <list>\n+#include <set>\n+#include <sstream>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <map>\n+#include <utility>\n+\n+#include \"NvInfer.h\"\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+#include \"tensorflow/contrib/tensorrt/segment/segment.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+#define _TF_LOG_DEBUG ::tensorflow::internal::LogMessage(__FILE__, __LINE__, -1)\n+#include \"tensorflow/core/grappler/optimizers/constant_folding.h\"\n+#include \"tensorflow/core/grappler/optimizers/layout_optimizer.h\" \n+#include \"tensorflow/core/grappler/devices.h\"\n+//#include \"tensorflow/core/grappler/clusters/single_machine.h\"\n+#include \"tensorflow/core/grappler/clusters/virtual_cluster.h\"\n+#include \"tensorflow/core/protobuf/device_properties.pb.h\"\n+#include \"tensorflow/core/grappler/grappler_item.h\"\n+#include \"tensorflow/core/grappler/utils.h\"\n+\n+#include \"tensorflow/core/grappler/costs/graph_properties.h\"\n+\n+//------------------------------------------------------------------------------\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+static std::unordered_set<std::string> output_nodes;\n+bool IsTensorRTCandidate(const tensorflow::NodeDef& node_def) {\n+  static const std::set<std::string> candidate_ops = {\n+      \"Identity\", \"Const\", \"Conv2D\", \"MaxPool\", \"BiasAdd\", \"Relu\",\n+      \"Add\",      \"Mul\",   \"Sub\",    \"Rsqrt\",   \"Pad\"  // \"Placeholder\" ,\"Mean\"\n+                                                       // TODO(ben,jie): ...\n+  };\n+  if (output_nodes.count(node_def.name())) return false;\n+  return candidate_ops.count(node_def.op());\n+}\n+\n+void GetSubGraphIncomingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,\n+                              tensorflow::EdgeSet* incoming_edges) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node const* node = graph.FindNodeId(node_id);\n+    LOG(DEBUG) << node->name() << \" has incoming edges: \";\n+    for (tensorflow::Edge const* edge : node->in_edges()) {\n+      if (!subgraph_node_ids.count(edge->src()->id()) &&\n+          !edge->src()->IsSource()) {\n+        LOG(DEBUG) << edge->src()->name() << \", \";\n+        incoming_edges->insert(edge);\n+      }\n+    }\n+  }\n+}\n+\n+void GetSubGraphOutgoingEdges(tensorflow::Graph const& graph,\n+                              std::set<int> const& subgraph_node_ids,\n+                              tensorflow::EdgeSet* outgoing_edges) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node const* node = graph.FindNodeId(node_id);\n+    LOG(DEBUG) << node->name() << \" has outgoing edges: \";\n+    for (tensorflow::Edge const* edge : node->out_edges()) {\n+      if (!subgraph_node_ids.count(edge->dst()->id()) &&\n+          !edge->dst()->IsSink()) {\n+        outgoing_edges->insert(edge);\n+      }\n+    }\n+  }\n+}\n+\n+std::pair<std::string, int> ParseTensorName(std::string name,\n+                                            int default_idx = 0) {\n+  int idx = default_idx;\n+  size_t sep = name.find_last_of(':');\n+  if (sep != std::string::npos) {\n+    name = name.substr(0, sep);\n+    idx = std::stoi(name.substr(sep + 1));\n+  }\n+  return std::make_pair(name, idx);\n+}\n+\n+std::unordered_map<std::string, std::vector<int>> BuildTensorNameMap(\n+    const std::vector<std::string>& tensor_names) {\n+  std::unordered_map<std::string, std::vector<int>> result;\n+  for (std::string const& tensor_name : tensor_names) {\n+    std::string node_name;\n+    int index;\n+    std::tie(node_name, index) = ParseTensorName(tensor_name);\n+    result[node_name].push_back(index);\n+  }\n+  return result;\n+}\n+\n+tensorflow::Status ConvertSubGraphToTensorRT(\n+    tensorflow::Graph& graph, const std::vector<std::string>& output_names,\n+    const std::set<int>& subgraph_node_ids, size_t max_batch_size,\n+    size_t max_workspace_size,\n+    const tensorflow::grappler::GraphProperties& graph_properties) {\n+  tensorflow::EdgeSet subgraph_incoming_edges;\n+  GetSubGraphIncomingEdges(graph, subgraph_node_ids, &subgraph_incoming_edges);\n+\n+  std::vector<std::pair<int, int>> subgraph_inputs;\n+\n+\n+  // Collect inputs by looking for incoming edges\n+  for (tensorflow::Edge const* edge : subgraph_incoming_edges) {\n+    subgraph_inputs.push_back({edge->src()->id(), edge->src_output()});\n+  }\n+  std::set<std::pair<int, int>> subgraph_outputs_set;\n+  // Collect outputs referenced from output_names\n+  auto output_name_to_index_map = BuildTensorNameMap(output_names);\n+  // for (int node_id : subgraph_node_ids_no_placeholder) {\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node* node = graph.FindNodeId(node_id);\n+    if (output_name_to_index_map.count(node->name())) {\n+      for (int index : output_name_to_index_map.at(node->name())) {\n+        subgraph_outputs_set.insert({node_id, index});\n+      }\n+    }\n+  }\n+  // Collect outputs referenced from outgoing edges\n+  tensorflow::EdgeSet subgraph_outgoing_edges;\n+  // GetSubGraphOutgoingEdges(graph, subgraph_node_ids_no_placeholder,\n+  //  &subgraph_outgoing_edges);\n+  GetSubGraphOutgoingEdges(graph, subgraph_node_ids, &subgraph_outgoing_edges);\n+  for (tensorflow::Edge const* edge : subgraph_outgoing_edges) {\n+    subgraph_outputs_set.insert({edge->src()->id(), edge->src_output()});\n+  }\n+  // Impose an ordering on the outputs\n+  std::vector<std::pair<int, int>> subgraph_outputs(\n+      subgraph_outputs_set.begin(), subgraph_outputs_set.end());\n+  // Build TensorRT node and add it to the graph\n+  tensorflow::NodeDef trt_node_def;\n+  TF_RETURN_IF_ERROR(ConvertSubGraphToTensorRTNodeDef(\n+      graph, subgraph_node_ids, subgraph_inputs, subgraph_outputs,\n+      max_batch_size, max_workspace_size, graph_properties, &trt_node_def));\n+  tensorflow::Status status;\n+  tensorflow::Node* trt_node = graph.AddNode(trt_node_def, &status);\n+\n+  TF_RETURN_IF_ERROR(status);\n+\n+  // Re-map outgoing edges to use the new TRT node instead of the orig subgraph\n+  std::map<std::pair<int, int>, int> subgraph_edge_to_output_map;\n+  for (size_t i = 0; i < subgraph_outputs.size(); ++i) {\n+    subgraph_edge_to_output_map.insert({subgraph_outputs.at(i), i});\n+  }\n+  TF_RETURN_IF_ERROR(status);\n+  for (tensorflow::Edge const* edge : subgraph_outgoing_edges) {\n+    std::pair<int, int> old_src = {edge->src()->id(), edge->src_output()};\n+    int new_src_output = subgraph_edge_to_output_map.at(old_src);\n+    graph.UpdateEdge(trt_node, new_src_output, edge->dst(), edge->dst_input());\n+  }\n+  // Remove the original subgraph\n+  for (int node_id : subgraph_node_ids) {\n+    tensorflow::Node* node = graph.FindNodeId(node_id);\n+    // Don't remove the input placeholders\n+    if (node->type_string() == \"Placeholder\") {\n+      continue;\n+    }\n+    graph.RemoveNode(node);\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status BuildNodeMap(\n+    const tensorflow::Graph& graph,\n+    std::unordered_map<std::string, tensorflow::Node*>* node_map) {\n+  for (auto* node : graph.op_nodes()) {\n+    if (!node_map->insert({node->name(), node}).second) {\n+      return tensorflow::errors::AlreadyExists(\n+          \"Node name is not unique in graph: \" + node->name());\n+    }\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+}  // namespace\n+\n+tensorflow::Status ConvertGraphDefToTensorRT(\n+    const tensorflow::GraphDef& graph_def,\n+    const std::vector<std::string>& output_names, size_t max_batch_size,\n+    size_t max_workspace_size, tensorflow::GraphDef* new_graph_def) {\n+\n+  // optimization pass\n+  tensorflow::grappler::GrapplerItem item;\n+  item.fetch = output_names;\n+  tensorflow::GraphDef gdef;\n+\n+  // layout optimization\n+  item.graph = graph_def;\n+  tensorflow::grappler::LayoutOptimizer optimizer;\n+  tensorflow::grappler::Cluster* gCluster;\n+\n+  // virtual cluster\n+  tensorflow::DeviceProperties device_properties;\n+  device_properties.set_type(\"GPU\");\n+  device_properties.mutable_environment()->insert({\"architecture\", \"6\"});\n+  gCluster =\n+    new tensorflow::grappler::VirtualCluster({{\"/GPU:0\", device_properties}});\n+\n+  // single machine\n+  int num_cpu_cores = tensorflow::grappler::GetNumAvailableLogicalCPUCores();\n+  int num_gpus = tensorflow::grappler::GetNumAvailableGPUs();\n+  LOG(DEBUG) << \"cpu_cores: \" << num_cpu_cores;\n+  LOG(DEBUG) << \"gpus: \" << num_gpus;\n+  // int timeout_s = 60 * 10;\n+  // gCluster = new tensorflow::grappler::SingleMachine(\n+  //                  timeout_s, num_cpu_cores, num_gpus);\n+\n+  tensorflow::Status status = optimizer.Optimize(gCluster, item, &gdef);\n+\n+  if (status !=tensorflow::Status::OK())\n+    return status;\n+ \n+  // constant folding\n+  item.graph = gdef;\n+  tensorflow::grappler::ConstantFolding fold(nullptr);\n+  status = fold.Optimize(nullptr, item, &gdef);\n+  if (status !=tensorflow::Status::OK())\n+    return status;\n+  \n+  // AJ refactoring shape inference through grappler/GraphProperties.\n+  tensorflow::grappler::GraphProperties static_graph_properties(item);\n+  static_graph_properties.InferStatically(false);\n+  // TF_CHECK_OK(static_graph_prop.InferStatically(false));\n+  // ShapeMap shape_map;\n+  // TF_RETURN_IF_ERROR(\n+  //     tensorflow::trt::inferShapes(gdef, output_names, shape_map));\n+  // std::stringstream oss;\n+  // for (auto& n : shape_map) {  // nodes\n+  //   oss << \" Node= \" << n.first << \", \";\n+  //   for (auto o : n.second) {  // outputs\n+  //     oss << o.first.DebugString() << \" T= \" << o.second << \", \";\n+  //   }\n+  //   LOG(DEBUG) << oss.str();\n+  //   oss.str(\"\");\n+  // }\n+\n+  // Build full graph\n+  tensorflow::FunctionLibraryDefinition flib(tensorflow::OpRegistry::Global(),\n+                                             gdef.library());\n+  tensorflow::Graph graph(flib);\n+  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph(\n+      tensorflow::GraphConstructorOptions(), gdef, &graph));\n+\n+  // Segment the graph into subgraphs that can be converted to TensorRT\n+  tensorrt::segment::SegmentOptions segment_options;\n+  // TODO(ben,jie,sami): exclude output nodes (DISCUSS IT)\n+  for (auto node : output_names) output_nodes.insert(node);\n+\n+  // TODO(sami): this should be passed as a knob!!!!\n+  segment_options.minimum_segment_size = 2;\n+  tensorrt::segment::SegmentNodesVector segments;\n+  TF_RETURN_IF_ERROR(tensorrt::segment::SegmentGraph(\n+      gdef, IsTensorRTCandidate, segment_options, &segments));\n+  if (segments.size() > 1) {\n+    // LOG(WARNING) << \"Multiple TensorRT candidate subgraphs were found, \"\n+    //<< \"but only the first can be converted.\";\n+    // segments.erase(++segments.begin(), segments.end());\n+    LOG(INFO) << \"MULTIPLE tensorrt candidate conversion: \" << segments.size();\n+  }\n+  std::unordered_map<std::string, tensorflow::Node*> node_map;\n+  TF_RETURN_IF_ERROR(BuildNodeMap(graph, &node_map));\n+  for (std::set<std::string> const& subgraph_node_names : segments) {"
      },
      {
        "body": "Comment on `max_workspace_size`. Readers might not know what it means or even the units (in bytes or megabytes)? ",
        "diff_hunk": "@@ -0,0 +1,34 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_CONTRIB_TENSORRT_CONVERT_CONVERT_GRAPH_H_\n+#define TENSORFLOW_CONTRIB_TENSORRT_CONVERT_CONVERT_GRAPH_H_\n+\n+#include <string>\n+#include <vector>\n+\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+\n+namespace tensorrt {\n+namespace convert {\n+\n+tensorflow::Status ConvertGraphDefToTensorRT(\n+    const tensorflow::GraphDef& graph_def,\n+    const std::vector<std::string>& output_names, size_t max_batch_size,\n+    size_t max_workspace_size, tensorflow::GraphDef* new_graph_def);"
      }
    ],
    "body": "This PR introduces a new op that wraps around an highly optimized TensorRT engine and provides a seamless integration between TensorRT and TensorFlow.\r\n- Add a TRTEngineOp that encapsulates a TensorRT executable.\r\n- Add CreateInferenceGraph to contract a TensorRT-compilable subgraph to a TRTEngineOp.\r\n- Update BUILD files to include new contrib package\r\n- Add tensorflow.contrib.tensorrt python package to expose API to python\r\n",
    "timestamp": "2025-05-06 01:27:18"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/37682",
    "comments": [
      "It seems like you create a new function call for each of these \"fusion\".\r\nHave you considered a lighter-weight region-based approach?\r\n\r\nI'm thinking about the equivalent of lhlo.fusion operation? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/xla/ir/lhlo_ops.td#L388",
      "> It seems like you create a new function call for each of these \"fusion\".\r\n> Have you considered a lighter-weight region-based approach?\r\n> \r\n> I'm thinking about the equivalent of lhlo.fusion operation? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/xla/ir/lhlo_ops.td#L388\r\n\r\nThanks for your reply!\r\n\r\nI agree that region style is more lightweight for fusion itself. Here I use a function style because this style  is easier to play with other existing passes (e.g. hlo-legalize-to-lhlo) .  I will change to region style after the general structure review of this pass is finished.\r\n",
      "> I looked at the code and left some stylistic comments.\r\n> \r\n> What is the plan for this code moving forward? What do you aim to build and where does it fit in?\r\n\r\nWe plan to add E2E dynamic shape fusion&codegen support based on MLIR.\r\nThis PR mainly covers the fusion pass. As to the codegen part, we are already working on it and later another PR may be submit.\r\n\r\nThanks.",
      "> We plan to add E2E dynamic shape fusion&codegen support based on MLIR.\r\n> This PR mainly covers the fusion pass. As to the codegen part, we are already working on it and later another PR may be submit.\r\n\r\nThis is fantastic!\r\nCan you send RFCs and conduct discussions about the plan ahead of time? We should likely coordinate a bit more to make sure your work align with our. \r\nWe will also try to surface a better roadmap on our side to give visibility to everything that we're doing.\r\nAt the moment IREE is also doing some dynamic codegen and they are very interested in better alignment as well.",
      "@sherhut Thanks for your suggestion. I have done some refine work.",
      "@joker-eph  Here are some documents:\r\n\r\n[RFC](https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/_X48poNcbDI)  and [PPT]( https://drive.google.com/open?id=1ZDzXluB2uVc35r1fBNK5jW6rY8s82pc_)",
      "@joker-eph @sherhut Could you help to have a look?",
      "The corresponding codegen PR for kLoop/kInput fusion is as following:\r\nhttps://github.com/tensorflow/tensorflow/pull/38186/files\r\nhttps://reviews.llvm.org/D77447\r\n\r\nJust refer them here to ensure the reviewers can have a full-picture of what we are working on. ",
      "@wyzero Can you please check @joker-eph's comments and resolve conflicts?. Thanks!",
      "Sorry for the delay. I will fix it soon.",
      "For interface-based fusibility check, it will be implemented in next commit soon. ",
      "@joker-eph Could you help to take a look?",
      "@joker-eph Hi Mehdi, could you take another round of review for this PR? It has been pending for more than one month:) If possible, we would like to close  it ASAP.",
      ">Hi Mehdi, could you take another round of review for this PR? It has been pending for more than one month:) If possible, we would like to close it ASAP.\r\n\r\n@alibaba-common it has been *in review* with a few round that happened during this time, it wasn't \"pending\" as waiting for being reviewed. Most of the time I believe has been spent waiting for update on the author side to address comments. I invite you to take another look at the delays between the pushes/updates to the PR and my reviews.\r\nIt also does not help that some comments/questions are left unanswered. ",
      "FYI the sanity build log is complaining about formatting:\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/compiler/mlir/xla/BUILD:\r\n153d152\r\n<         \"ir/infer_fusibility_op_interface.h.inc\",\r\n154a154\r\n>         \"ir/infer_fusibility_op_interface.h.inc\",\r\n369d368\r\n<     hdrs = [\"transforms/cycle_detector.h\"],\r\n370a370\r\n>     hdrs = [\"transforms/cycle_detector.h\"],\r\n381d380\r\n<         \":hlo\",\r\n382a382\r\n>         \":hlo\",\r\n384c384\r\n<         \"//tensorflow/compiler/jit/graphcycles:graphcycles\",\r\n---\r\n>         \"//tensorflow/compiler/jit/graphcycles\",\r\n593d592\r\n<         \":xla_canonicalize_inc_gen\",\r\n594a594\r\n>         \":xla_canonicalize_inc_gen\",\r\n893d892\r\n<         \":xla_hlo_fusion\",\r\n900a900\r\n>         \":xla_hlo_fusion\",\r\nexit status 1\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```",
      "I suspect you should also have access to the windows build log: https://source.cloud.google.com/results/invocations/dadc78f3-a639-40d5-86bd-301ad8866b1f/log\r\n\r\n```\r\n\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(4285) : error C4716: 'mlir::xla_hlo::XorOp::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(3936) : error C4716: 'mlir::xla_hlo::SubOp::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(1752) : error C4716: 'mlir::xla_hlo::Expm1Op::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(3878) : error C4716: 'mlir::xla_hlo::SqrtOp::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(3744) : error C4716: 'mlir::xla_hlo::SinOp::inferEffectiveWorkloadSize': must return a value\r\n...\r\n```",
      "> FYI the sanity build log is complaining about formatting:\r\n> \r\n> ```\r\n> FAIL: buildifier found errors and/or warnings in above BUILD files.\r\n> buildifier suggested the following changes:\r\n> tensorflow/compiler/mlir/xla/BUILD:\r\n> 153d152\r\n> <         \"ir/infer_fusibility_op_interface.h.inc\",\r\n> 154a154\r\n> >         \"ir/infer_fusibility_op_interface.h.inc\",\r\n> 369d368\r\n> <     hdrs = [\"transforms/cycle_detector.h\"],\r\n> 370a370\r\n> >     hdrs = [\"transforms/cycle_detector.h\"],\r\n> 381d380\r\n> <         \":hlo\",\r\n> 382a382\r\n> >         \":hlo\",\r\n> 384c384\r\n> <         \"//tensorflow/compiler/jit/graphcycles:graphcycles\",\r\n> ---\r\n> >         \"//tensorflow/compiler/jit/graphcycles\",\r\n> 593d592\r\n> <         \":xla_canonicalize_inc_gen\",\r\n> 594a594\r\n> >         \":xla_canonicalize_inc_gen\",\r\n> 893d892\r\n> <         \":xla_hlo_fusion\",\r\n> 900a900\r\n> >         \":xla_hlo_fusion\",\r\n> exit status 1\r\n> Please fix manually or run buildifier <file> to auto-fix.\r\n> ```\r\n\r\nThanks for your help. It seems that there is  no difference for most of lines (except the line for `\"//tensorflow/compiler/jit/graphcycles:graphcycles\"`).  It that true?",
      "> Thanks for your help. It seems that there is no difference for most of lines (except the line for \"//tensorflow/compiler/jit/graphcycles:graphcycles\"). It that true?\r\n\r\nThe other lines are pointing at issues in ordering: these should be alphabetically sorted I think.",
      "(FYI: there is an issue with our import script I had to fix as well, but I need an internal review and I won't get it today I think)",
      "@joker-eph  I have fixed the above problems and rebase to master as well.",
      "It fails with:\r\n\r\n```\r\ntensorflow/compiler/mlir/xla/ir/infer_fusibility_op_interface.h.inc:89:61: error: use of undeclared identifier 'lhs'\r\n        assert(lhs < op.getOperation()->getNumOperands() && lhs >= 0 &&\r\n                                                            ^\r\n```",
      "Our internal linter also forced me to add a header guard in cycle_detector.h, add newlines at end of source files, and replace the `using namespace` directive with more targeted `using`.",
      "Also fixed other issues:\r\n- single-argument constructors must be marked explicit to avoid unintentional implicit conversions\r\n- private field 'planner_' is not used\r\n- function 'mlir::GraphCycles::IsReachable' has a definition with different parameter names\r\n- function 'mlir::GraphCycles::InsertEdge' has a definition with different parameter names\r\n- function 'mlir::GraphCycles::RemoveEdge' has a definition with different parameter names\r\n- function 'mlir::GraphCycles::HasEdge' has a definition with different parameter names\r\n\r\n",
      "> * single-argument constructors must be marked explicit to avoid unintentional implicit conversions\r\n\r\nThanks! I will fix it soon.",
      "@joker-eph Done! Thanks!",
      " @jpienaar  Thanks for your suggestions.  I have done some refine.",
      "@joker-eph Thanks for your suggestions. I will add some tests for `cycle_detector.cc`.  I have one question though. Do we need to use llvm testing framework or just use tensorflow testing framework? And since these codes are in TF repo. now, it seems the latter one is better?",
      "Please follow MLIR best practices.",
      "Okay. Do you mean use FileCheck (like the tests in xla_hlo_fusion.mlir) ?",
      "MLIR/LLVM is using gtest for testing datastructures, and FileCheck for IR test\r\n(Sorry i am terse but I am on my phone)"
    ],
    "review_comments": [
      {
        "body": "Use `isa` instead of `dyn_cast`. Here and below.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||"
      },
      {
        "body": "These are re-exported by mlir, as well.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;"
      },
      {
        "body": "Avoid the search and just use `insert`. It returns whether an insertion took place.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {"
      },
      {
        "body": "Cache the operand for termination predicate. Here and elsewhere.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {"
      },
      {
        "body": "Maybe use insert?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));"
      },
      {
        "body": "Use `getArgument(idx)` instead of explicitly constructing a vector?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);\n+      std::vector<Value> outputs = GetOutputsOfFusionPattern(pattern);\n+      std::vector<Type> input_types;\n+      std::vector<Type> output_types;\n+      for(auto& v : inputs) input_types.push_back(v.getType());\n+      for(auto& v : outputs) output_types.push_back(v.getType());\n+\n+      auto loc = pattern.back()->getLoc();\n+      auto function_type = builder.getFunctionType(input_types, output_types);\n+      std::string name = GetUniqueName(pattern.back()->getName().getStringRef());\n+      auto function = FuncOp::create(loc, name, function_type);\n+      function.setAttr(kHloTensorFuncAttr, builder.getUnitAttr());\n+      module.push_back(function);\n+\n+      OpBuilder func_builder(func.getContext());\n+      func_builder.setInsertionPoint(pattern.back());\n+      auto call_op = func_builder.create<CallOp>(loc, function, inputs);\n+\n+      Block* block = function.addEntryBlock();\n+      for (auto op : pattern) {\n+        op->moveBefore(block, block->end());\n+      }\n+\n+      OpBuilder op_builder(function.getBody());\n+      op_builder.setInsertionPointToEnd(block);\n+      auto return_op = op_builder.create<mlir::ReturnOp>(loc, outputs);\n+\n+      std::vector<Value> func_args(function.args_begin(), function.args_end());"
      },
      {
        "body": "Use `getResult(idx)` here?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);\n+      std::vector<Value> outputs = GetOutputsOfFusionPattern(pattern);\n+      std::vector<Type> input_types;\n+      std::vector<Type> output_types;\n+      for(auto& v : inputs) input_types.push_back(v.getType());\n+      for(auto& v : outputs) output_types.push_back(v.getType());\n+\n+      auto loc = pattern.back()->getLoc();\n+      auto function_type = builder.getFunctionType(input_types, output_types);\n+      std::string name = GetUniqueName(pattern.back()->getName().getStringRef());\n+      auto function = FuncOp::create(loc, name, function_type);\n+      function.setAttr(kHloTensorFuncAttr, builder.getUnitAttr());\n+      module.push_back(function);\n+\n+      OpBuilder func_builder(func.getContext());\n+      func_builder.setInsertionPoint(pattern.back());\n+      auto call_op = func_builder.create<CallOp>(loc, function, inputs);\n+\n+      Block* block = function.addEntryBlock();\n+      for (auto op : pattern) {\n+        op->moveBefore(block, block->end());\n+      }\n+\n+      OpBuilder op_builder(function.getBody());\n+      op_builder.setInsertionPointToEnd(block);\n+      auto return_op = op_builder.create<mlir::ReturnOp>(loc, outputs);\n+\n+      std::vector<Value> func_args(function.args_begin(), function.args_end());\n+      for (auto& it : llvm::enumerate(inputs)) {\n+        Value old_value = it.value();\n+        for (auto& user : llvm::make_early_inc_range(old_value.getUses())) {\n+          auto owner = user.getOwner();\n+          if (op_set.find(owner) != op_set.end()) {\n+            owner->replaceUsesOfWith(old_value, func_args[it.index()]);\n+          }\n+        }\n+      }\n+\n+      for (auto& it : llvm::enumerate(call_op.getResults())) {"
      },
      {
        "body": "Use `SymbolTable::insert` instead to create a unique name.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);\n+      std::vector<Value> outputs = GetOutputsOfFusionPattern(pattern);\n+      std::vector<Type> input_types;\n+      std::vector<Type> output_types;\n+      for(auto& v : inputs) input_types.push_back(v.getType());\n+      for(auto& v : outputs) output_types.push_back(v.getType());\n+\n+      auto loc = pattern.back()->getLoc();\n+      auto function_type = builder.getFunctionType(input_types, output_types);\n+      std::string name = GetUniqueName(pattern.back()->getName().getStringRef());"
      },
      {
        "body": "Use `SmallVector` instead?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);"
      },
      {
        "body": "Use `mlir::replaceAllUsesInRegionWith` from `RegionUtils.h`.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);\n+      std::vector<Value> outputs = GetOutputsOfFusionPattern(pattern);\n+      std::vector<Type> input_types;\n+      std::vector<Type> output_types;\n+      for(auto& v : inputs) input_types.push_back(v.getType());\n+      for(auto& v : outputs) output_types.push_back(v.getType());\n+\n+      auto loc = pattern.back()->getLoc();\n+      auto function_type = builder.getFunctionType(input_types, output_types);\n+      std::string name = GetUniqueName(pattern.back()->getName().getStringRef());\n+      auto function = FuncOp::create(loc, name, function_type);\n+      function.setAttr(kHloTensorFuncAttr, builder.getUnitAttr());\n+      module.push_back(function);\n+\n+      OpBuilder func_builder(func.getContext());\n+      func_builder.setInsertionPoint(pattern.back());\n+      auto call_op = func_builder.create<CallOp>(loc, function, inputs);\n+\n+      Block* block = function.addEntryBlock();\n+      for (auto op : pattern) {\n+        op->moveBefore(block, block->end());\n+      }\n+\n+      OpBuilder op_builder(function.getBody());\n+      op_builder.setInsertionPointToEnd(block);\n+      auto return_op = op_builder.create<mlir::ReturnOp>(loc, outputs);\n+\n+      std::vector<Value> func_args(function.args_begin(), function.args_end());\n+      for (auto& it : llvm::enumerate(inputs)) {\n+        Value old_value = it.value();\n+        for (auto& user : llvm::make_early_inc_range(old_value.getUses())) {"
      },
      {
        "body": "Use `mlir::replaceAllUsesInRegionWith` here, as well?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);\n+      std::vector<Value> outputs = GetOutputsOfFusionPattern(pattern);\n+      std::vector<Type> input_types;\n+      std::vector<Type> output_types;\n+      for(auto& v : inputs) input_types.push_back(v.getType());\n+      for(auto& v : outputs) output_types.push_back(v.getType());\n+\n+      auto loc = pattern.back()->getLoc();\n+      auto function_type = builder.getFunctionType(input_types, output_types);\n+      std::string name = GetUniqueName(pattern.back()->getName().getStringRef());\n+      auto function = FuncOp::create(loc, name, function_type);\n+      function.setAttr(kHloTensorFuncAttr, builder.getUnitAttr());\n+      module.push_back(function);\n+\n+      OpBuilder func_builder(func.getContext());\n+      func_builder.setInsertionPoint(pattern.back());\n+      auto call_op = func_builder.create<CallOp>(loc, function, inputs);\n+\n+      Block* block = function.addEntryBlock();\n+      for (auto op : pattern) {\n+        op->moveBefore(block, block->end());\n+      }\n+\n+      OpBuilder op_builder(function.getBody());\n+      op_builder.setInsertionPointToEnd(block);\n+      auto return_op = op_builder.create<mlir::ReturnOp>(loc, outputs);\n+\n+      std::vector<Value> func_args(function.args_begin(), function.args_end());\n+      for (auto& it : llvm::enumerate(inputs)) {\n+        Value old_value = it.value();\n+        for (auto& user : llvm::make_early_inc_range(old_value.getUses())) {\n+          auto owner = user.getOwner();\n+          if (op_set.find(owner) != op_set.end()) {\n+            owner->replaceUsesOfWith(old_value, func_args[it.index()]);\n+          }\n+        }\n+      }\n+\n+      for (auto& it : llvm::enumerate(call_op.getResults())) {\n+        Value old_value = outputs[it.index()];\n+        for (auto& user : llvm::make_early_inc_range(old_value.getUses())) {"
      },
      {
        "body": "Use `reserve` before `push_back` if number of elements is known.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||\n+                op.getDialect()->getNamespace() == \"xla_lhlo\");\n+    })) {\n+      // no target ops\n+      return false;\n+    }\n+\n+    // not handle already-fused func\n+    if (func.getAttrOfType<UnitAttr>(kHloTensorFuncAttr)) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  void runOnFunction(FuncOp func) {\n+    SmallVector<Operation*, 4> op_list;\n+    for (auto& op : func.getBlocks().front()) {\n+      op_list.push_back(&op);\n+    }\n+    FusionPlanner planner(op_list);\n+    auto plan = planner.Run();\n+    if (!plan) {\n+      emitError(func.getLoc(), \"Fusion planner failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+    if (!ApplyFusionPlan(func, *plan)) {\n+      emitError(func.getLoc(), \"Apply Fusion Plan failed.\");\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+  bool ApplyFusionPlan(FuncOp func, const FusionPlan& plan) {\n+    ModuleOp module = getModule();\n+    Builder builder(module.getContext());\n+\n+    for (auto& pattern: plan) {\n+      std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+      std::vector<Value> inputs = GetInputsOfFusionPattern(pattern);\n+      std::vector<Value> outputs = GetOutputsOfFusionPattern(pattern);\n+      std::vector<Type> input_types;\n+      std::vector<Type> output_types;\n+      for(auto& v : inputs) input_types.push_back(v.getType());"
      },
      {
        "body": "This is expensive. Use `context->getRegisteredDialect<\"xla_hlo\">()` and the compare the `Dialect` objects. Those are unique within the context.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {\n+      return TryToContractEdge(from, to);\n+    });\n+    return changed;\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+\n+  // Shape equality checker\n+  ShapeConstrainAnalysis shape_analysis_;\n+\n+  std::unordered_map<Operation*, int> op_to_node_id_;\n+\n+  GraphCycles cycle_detector_;\n+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;\n+  std::vector<UnionFind<Cluster*>> cluster_for_node_;\n+};\n+\n+struct HloOutlineFusion : public ModulePass<HloOutlineFusion> {\n+  void runOnModule() override {\n+    SmallVector<FuncOp, 4> funcs;\n+    // We may add some fusion func into the module during\n+    // visiting, thus we collect all target funcs first.\n+    for (FuncOp func : getModule().getOps<FuncOp>()) {\n+      if (IsTargetFunc(func)) {\n+        funcs.push_back(func);\n+      }\n+    }\n+    for (FuncOp func : funcs) {\n+      runOnFunction(func);\n+    }\n+  }\n+\n+  bool IsTargetFunc(FuncOp func) {\n+    // We currently only support single block function.\n+    if (func.getBlocks().size() != 1) {\n+      return false;\n+    }\n+\n+    if (llvm::none_of(func.getBlocks().front(), [](Operation& op) {\n+        return (op.getDialect()->getNamespace() == \"xla_hlo\" ||"
      },
      {
        "body": "Why the extra lambda here?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {"
      },
      {
        "body": "This is not implemented, so why not drop it for now?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {"
      },
      {
        "body": "Move this out of the `all_of`",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);"
      },
      {
        "body": "ShapeConstraintAnalysis?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {"
      },
      {
        "body": "It seems wasteful to materialize these in vectors just to be able to iterate over them.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern("
      },
      {
        "body": "Why are these updated? Where is this used?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {"
      },
      {
        "body": "Could you document in a bit more detail what this does?",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready."
      },
      {
        "body": "`TryToContractEdge` is a member function, thus can not be passed to `ForEachEdgeInPostOrder` directly. I have changed to use `std::bind` instead.",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstrainAnalysis {\n+ public:\n+   ShapeConstrainAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);\n+            }\n+          }\n+        }\n+      }\n+    } while (!converged);\n+  }\n+\n+  SmallVector<Operation*, 4> op_list_;\n+  DenseMap<Value, UnionFind<Value>> shape_checker_;\n+};\n+\n+class FusionPlanner {\n+ public:\n+  FusionPlanner(const SmallVector<Operation*, 4>& op_list)\n+   : op_list_(op_list), shape_analysis_(op_list) {\n+    BuildNodeMap();\n+  }\n+\n+  llvm::Optional<FusionPlan> Run() {\n+    // Greedily search kLoop/kInput pattern\n+    RunEdgeContractionLoop();\n+\n+    FusionPlan plan;\n+    DenseMap<int, int> pattern_ids;\n+    for (auto op : op_list_) {\n+      Cluster* cluster = GetClusterForNode(op);\n+      auto node_id = cluster->cycles_graph_node_id();\n+      if (!IsFusible(op_list_[node_id])) {\n+        continue;\n+      }\n+      if (!pattern_ids.count(node_id)) {\n+        int pattern_id = pattern_ids.size();\n+        pattern_ids[node_id] = pattern_id;\n+        plan.emplace_back();\n+      }\n+      plan[pattern_ids[node_id]].push_back(op);\n+    }\n+    return plan;\n+  }\n+\n+  const SmallVector<Operation*, 4>& op_list() const {\n+    return op_list_;\n+  }\n+\n+ private:\n+  class Cluster {\n+   public:\n+    Cluster(int node_id, FusionPlanner* planner)\n+        : node_id_(node_id), planner_(planner) {\n+      auto& op_list = planner->op_list();\n+      pattern_.push_back(op_list[node_id]);\n+      op_set_.insert(pattern_.begin(), pattern_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // Merges `other` into this cluster, and clears `other`.\n+    void Merge(Cluster* other) {\n+      pattern_.insert(pattern_.end(),\n+          other->pattern_.begin(), other->pattern_.end());\n+      op_set_.insert(other->op_set_.begin(), other->op_set_.end());\n+      pattern_results_ = GetOutputsOfFusionPattern(pattern_);\n+    }\n+\n+    // The number of nodes in this cluster.\n+    int cluster_size() const { return pattern_.size(); }\n+\n+    // The ID of the cluster as represented in `cycles_graph_`.\n+    int cycles_graph_node_id() const { return node_id_; }\n+\n+    std::vector<Value> GetResults() {\n+      return pattern_results_;\n+    }\n+\n+    FusionPattern fused_pattern() {\n+      return pattern_;\n+    }\n+\n+   private:\n+    int node_id_;\n+    FusionPlanner* planner_;\n+\n+    FusionPattern pattern_;\n+    std::unordered_set<Operation*> op_set_;\n+    std::vector<Value> pattern_results_;\n+  };\n+\n+ private:\n+  Cluster* MakeCluster(int cycles_graph_node_id) {\n+    cluster_storage_.emplace_back(\n+        new Cluster(cycles_graph_node_id, this));\n+    return cluster_storage_.back().get();\n+  }\n+\n+  void BuildNodeMap() {\n+    for (auto& op : op_list_) {\n+      auto node_id = cycle_detector_.NewNode();\n+      op_to_node_id_[op] = node_id;\n+      cluster_for_node_.emplace_back();\n+      cluster_for_node_[node_id].Get() = MakeCluster(node_id);\n+      for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+        auto operand_op = op->getOperand(idx).getDefiningOp();\n+        if (operand_op == nullptr) {\n+          // skip block argument\n+          continue;\n+        }\n+        auto iter = op_to_node_id_.find(operand_op);\n+        assert(iter != op_to_node_id_.end());\n+        cycle_detector_.InsertEdge(iter->second, node_id);\n+      }\n+    }\n+  }\n+\n+  Cluster* GetClusterForNode(Operation* n) {\n+    auto id = op_to_node_id_.at(n);\n+    return cluster_for_node_[id].Get();\n+  }\n+\n+  Cluster* GetClusterForCyclesGraphNode(int node_id) {\n+    if (node_id >= cluster_for_node_.size() || op_list_[node_id] == nullptr) {\n+      return nullptr;\n+    }\n+    Cluster* cluster = cluster_for_node_[node_id].Get();\n+    if (cluster) {\n+      assert(cluster->cycles_graph_node_id() == node_id);\n+    }\n+    return cluster;\n+  }\n+\n+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the\n+  // larger combined cluster is represented by `cluster_from`'s ID in\n+  // `cycles_graph_`.\n+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {\n+    int from = cluster_from->cycles_graph_node_id();\n+    int to = cluster_to->cycles_graph_node_id();\n+\n+    if (!cycle_detector_.ContractEdge(from, to)) {\n+      llvm::dbgs() << \"Could not contract \" << from << \" -> \" << to\n+                   << \" because contracting the edge would create a cycle.\";\n+      return false;\n+    }\n+\n+    // Merge the clusters.\n+    cluster_from->Merge(cluster_to);\n+\n+    // Merge the UnionFind<Cluster*>.\n+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);\n+\n+    return true;\n+  }\n+\n+  template <typename FnTy>\n+  bool ForEachEdgeInPostOrder(FnTy fn) {\n+    bool changed = false;\n+    for (int32 node : cycle_detector_.AllNodesInPostOrder()) {\n+      Cluster* cluster_from = GetClusterForCyclesGraphNode(node);\n+      if (!cluster_from) {\n+        continue;\n+      }\n+\n+      // Make a copy of the set of successors because we may modify the graph in\n+      // TryToContractEdge.\n+      std::vector<int32> successors_copy =\n+          cycle_detector_.SuccessorsCopy(cluster_from->cycles_graph_node_id());\n+\n+      for (int to : successors_copy) {\n+        Cluster* cluster_to = GetClusterForCyclesGraphNode(to);\n+        if (!cluster_to) {\n+          continue;\n+        }\n+\n+        bool contracted_edge = fn(cluster_from, cluster_to);\n+        changed |= contracted_edge;\n+      }\n+    }\n+\n+    return changed;\n+  }\n+\n+  std::vector<Value> GetResultsOfFusedPattern(Cluster* from, Cluster* to) {\n+    auto fused_pattern = MergeFusionPattern(\n+        from->fused_pattern(), to->fused_pattern());\n+    return GetOutputsOfFusionPattern(fused_pattern);\n+  }\n+\n+  bool TryToContractEdge(Cluster* from, Cluster* to) {\n+    int node_to = to->cycles_graph_node_id();\n+    int node_from = from->cycles_graph_node_id();\n+\n+    // Both node_to and node_from should be fusible\n+    if (!IsFusible(op_list_[node_to]) || !IsFusible(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // ReduceOp is fusible only when it's the output of the fusion pattern.\n+    if (dyn_cast<xla_hlo::ReduceOp>(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // Only Try to fuse along data edge, not index edge\n+    if (IsShapeProducer(op_list_[node_from])) {\n+      return false;\n+    }\n+\n+    // All outputs of a kLoop fusion should be same.\n+    auto results = GetResultsOfFusedPattern(from, to);\n+    auto get_workload_shape = [] (Value v) {\n+      if (dyn_cast<xla_hlo::ReduceOp>(v.getDefiningOp())) {\n+        v = v.getDefiningOp()->getOperand(0);\n+      }\n+      return v;\n+    };\n+    if (!llvm::all_of(results, [&] (Value result) {\n+      Value ref = get_workload_shape(results[0]);\n+      Value val = get_workload_shape(result);\n+      return shape_analysis_.HasSameShape(ref, val);\n+    })) {\n+      return false;\n+    }\n+\n+    return MergeClusters(from, to);\n+  }\n+\n+  bool RunEdgeContractionLoop() {\n+    bool changed = false;\n+    changed = ForEachEdgeInPostOrder([&](Cluster* from, Cluster* to) -> bool {"
      },
      {
        "body": "find() should be cheaper than insert() if the element already exists? ",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}\n+\n+SmallVector<Value, 4> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.insert(value).second) {"
      },
      {
        "body": "Why use DenseSet rather than normal set? ",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}\n+\n+SmallVector<Value, 4> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> inputs;\n+  DenseSet<Value> input_set;"
      },
      {
        "body": "std::move()  ?",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}\n+\n+SmallVector<Value, 4> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.insert(value).second) {\n+        inputs.push_back(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+SmallVector<Value, 4> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(), end = result.use_end(); iter != end; ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.insert(result).second) {\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;"
      },
      {
        "body": "I think here insert() should have the same time complexity as push_back().",
        "diff_hunk": "@@ -0,0 +1,555 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass implements the basic kLoop/kInput fusion pattern of XLA\n+// in fully dynamic shape scenario.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent.\n+//\n+// To this end, we also add an simple shape constrain analysis. This could\n+// be built on the shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using llvm::dyn_cast;\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AbsOp>(op) ||\n+          dyn_cast<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (dyn_cast<xla_hlo::AddOp>(op) ||\n+          dyn_cast<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          dyn_cast<xla_hlo::ReduceOp>(op)  ||\n+          dyn_cast<ConstantOp>(op) ||\n+          dyn_cast<xla_hlo::ConstOp>(op));\n+}\n+\n+bool IsShapeProducer(Operation*) {\n+  return false;\n+}\n+\n+std::vector<Value> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.find(value) == input_set.end()) {\n+        inputs.push_back(value);\n+        input_set.insert(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+std::vector<Value> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  std::vector<Value> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(); iter != result.use_end(); ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.find(result) == output_set.end()) {\n+          output_set.insert(result);\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+std::vector<Value> GetInputusAndOutputsOfOp(Operation* op) {\n+  std::vector<Value> ios;\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));"
      },
      {
        "body": "Is this Merge() action safe or not?\r\nFor simple pattern, it is expected to be safe.\r\nFor patterns which are more complicated, such as there is a reduction operation in the middle(although I think for kInput/kLoop this will not happen), the rule assumed here will be broken. ",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}\n+\n+SmallVector<Value, 4> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumOperands(); ++i) {\n+      auto operand_op = op->getOperand(i).getDefiningOp();\n+      if (op_set.find(operand_op) != op_set.end()) {\n+        // skip if defining op is in the pattern\n+        continue;\n+      }\n+      auto value = op->getOperand(i);\n+      if (input_set.insert(value).second) {\n+        inputs.push_back(value);\n+      }\n+    }\n+  }\n+  return std::move(inputs);\n+}\n+\n+SmallVector<Value, 4> GetOutputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> outputs;\n+  DenseSet<Value> output_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+  for (auto& op : pattern) {\n+    for (int i = 0; i < op->getNumResults(); ++i) {\n+      Value result = op->getResult(i);\n+      bool has_external_user = false;\n+      for (auto iter = result.use_begin(), end = result.use_end(); iter != end; ++iter) {\n+        if (op_set.find(iter->getOwner()) == op_set.end()) {\n+          has_external_user = true;\n+          break;\n+        }\n+      }\n+      if (has_external_user) {\n+        if (output_set.insert(result).second) {\n+          outputs.push_back(result);\n+        }\n+      }\n+    }\n+  }\n+  return outputs;\n+}\n+\n+SmallVector<Value, 4> GetInputusAndOutputsOfOp(Operation* op) {\n+  SmallVector<Value, 4> ios;\n+  ios.reserve(op->getNumOperands() + op->getNumResults());\n+  for (int idx = 0; idx < op->getNumOperands() ; ++idx) {\n+    ios.push_back(op->getOperand(idx));\n+  }\n+  for (int idx = 0; idx < op->getNumResults(); ++idx) {\n+    ios.push_back(op->getResult(idx));\n+  }\n+  return ios;\n+}\n+\n+FusionPattern MergeFusionPattern(\n+    const FusionPattern& lhs, const FusionPattern& rhs) {\n+  FusionPattern pattern(lhs);\n+  pattern.insert(pattern.end(), rhs.begin(), rhs.end());\n+  return pattern;\n+}\n+\n+class ShapeConstraintAnalysis {\n+ public:\n+   ShapeConstraintAnalysis(const SmallVector<Operation*, 4>& op_list)\n+       : op_list_(op_list) {\n+     InitState();\n+     PropagateEquality();\n+   }\n+\n+   bool HasSameShape(Value lhs, Value rhs) {\n+     return (shape_checker_.count(lhs) &&\n+         shape_checker_.count(rhs) &&\n+         shape_checker_[lhs].Get() == shape_checker_[rhs].Get());\n+   }\n+\n+ private:\n+  void InitState() {\n+    for (auto op : op_list_) {\n+      for (auto v : GetInputusAndOutputsOfOp(op)) {\n+        shape_checker_[v].Get() = v;\n+      }\n+    }\n+  }\n+\n+  void PropagateEquality() {\n+    bool converged = true;\n+    do {\n+      converged = true;\n+      for (auto op : op_list_) {\n+        if (IsElementWise(op)) {\n+          auto ios = GetInputusAndOutputsOfOp(op);\n+          for (auto v : ios) {\n+            Value ref = ios[0];\n+            if (shape_checker_[ref].Get() != shape_checker_[v].Get()) {\n+              converged = false;\n+              shape_checker_[ref].Merge(&shape_checker_[v]);"
      },
      {
        "body": "In general for MLIR core I expect to favor LLVM data structure for consistency. More info in http://llvm.org/docs/ProgrammersManual.html#picking-the-right-data-structure-for-a-task",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}\n+\n+SmallVector<Value, 4> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> inputs;\n+  DenseSet<Value> input_set;"
      },
      {
        "body": "Ideally we shouldn't bring any dependency from outside ` tensorflow/compiler/mlir/`, can we avoid these? If needed we can reimplement these functionalities.\r\n\r\nThis is important because we don't see the long term location for this work to necessarily stay under TensorFlow and I'd like it to be easy to upstream to MLIR core as needed.",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;"
      },
      {
        "body": "In general we don't want to hard-code any operation this way, this makes the compiler less extensible.\r\nI expect instead passes to be written to operate through on [MLIR interfaces](https://mlir.llvm.org/docs/Interfaces/) and dialects to define them.",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}"
      },
      {
        "body": "Nit: use `auto` only when the type is obvious from the context or when it improves significantly the readability. Here `for(Operation*op : pattern) {` seems more readable to me.",
        "diff_hunk": "@@ -0,0 +1,553 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file implements logic for fusing dhlo ops.\n+\n+#include \"absl/memory/memory.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // TF:llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // TF:llvm-project\n+#include \"mlir/Transforms/RegionUtils.h\"  // TF:llvm-project\n+#include \"mlir/Pass/Pass.h\"  // TF:local_config_mlir\n+\n+#include \"tensorflow/compiler/jit/union_find.h\"\n+#include \"tensorflow/compiler/jit/graphcycles/graphcycles.h\"\n+#include \"tensorflow/compiler/mlir/xla/ir/hlo_ops.h\"\n+\n+#include <unordered_set>\n+#include <unordered_map>\n+#include <vector>\n+\n+// This pass has similar functionality of the fusion pass in XLA stack.\n+// However, unlike XLA, it targets the fully dynamic shape scenario.\n+// Currently, it implements the kLoop and kInput fusion templates.\n+// During conversion, it tries to greedily find kLoop/kInput fusion\n+// patterns and outlines them to functions.\n+//\n+// Similar to XLA, this pass supports fusion pattern having multiple outputs\n+// if all the shape of outputs are consistent. Following are some examples.\n+//\n+//        kLoop                          kInput\n+// +----+  +----+  +----+    +----+    +----+    +----+\n+// |elem|  |elem|  |elem|    |elem<----+elem+---->elem+----+\n+// +-+--+  +-+--+  +-+--+    +-+--+    +----+    +-+--+    |\n+//   |       |       |         |                   |       |\n+//   |               |         |                   |       |\n+// +-v--+    |     +-v--+   +--v---+            +--v---+   |\n+// |elem+<---+----<+elem|   |reduce|            |reduce|   |\n+// +-+--+          +-+--+   +--+---+            +--+---+   |\n+//   |               |         |                   |       |\n+//   |               |         |                   |       |\n+//   v               v         v                   v       v\n+//\n+// To this end, we also add an simple shape constraint analysis phase.\n+// For kLoop fusion template, it requires all the outputs of the fused\n+// pattern have the same shape. However, we don't know the actual value\n+// of the shape at the compile time in the dynamic shape world.\n+// Fortunately, we could still infer the relationship among different ops\n+// according to their shape constrain traits. Currently, We only consider\n+// shape equality propagation for elementwise ops (assuming that implicit\n+// shape broadcast is forbidden). The above process could be built on the\n+// shape dialect once it is ready.\n+\n+namespace mlir {\n+namespace xla_hlo {\n+namespace {\n+\n+using tensorflow::GraphCycles;\n+using tensorflow::UnionFind;\n+using tensorflow::int32;\n+\n+const StringRef kHloTensorFuncAttr = \"xla_hlo.tensor_func\";\n+\n+using FusionPattern = std::vector<Operation*>;\n+using FusionPlan = std::vector<FusionPattern>;\n+\n+bool IsElementWiseUnary(Operation* op) {\n+  return (isa<xla_hlo::AbsOp>(op) ||\n+          isa<xla_hlo::ExpOp>(op));\n+}\n+\n+bool IsElementWiseBinary(Operation* op) {\n+  return (isa<xla_hlo::AddOp>(op) ||\n+          isa<xla_hlo::SubOp>(op));\n+}\n+\n+bool IsElementWise(Operation* op) {\n+  return IsElementWiseUnary(op) || IsElementWiseBinary(op);\n+}\n+\n+bool IsFusible(Operation* op) {\n+  return (IsElementWise(op) ||\n+          isa<xla_hlo::ReduceOp>(op)  ||\n+          isa<ConstantOp>(op) ||\n+          isa<xla_hlo::ConstOp>(op));\n+}\n+\n+SmallVector<Value, 4> GetInputsOfFusionPattern(const FusionPattern& pattern) {\n+  SmallVector<Value, 4> inputs;\n+  DenseSet<Value> input_set;\n+  std::unordered_set<Operation*> op_set(pattern.begin(), pattern.end());\n+\n+  for (auto& op : pattern) {"
      }
    ],
    "body": "This pass implements the basic kLoop/kInput fusion in fully dynamic shape scenario. Similar to XLA, this pass also supports fusion pattern having multiple outputs if all the shape of outputs are consistent. To this end, we also add a simple shape constrain analysis. This could be built on the shape dialect once it is ready.\r\n\r\n![image](https://user-images.githubusercontent.com/15364516/76931205-1ed0e580-6923-11ea-8732-fe9d93c3bcaf.png)\r\n\r\n",
    "timestamp": "2025-05-06 01:27:22"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/25203",
    "comments": [
      "@hgadig \r\nReopening this PR: https://github.com/tensorflow/tensorflow/pull/25038 which had unusual git rebasing problem. \r\n",
      "@penpornk can you review this PR and provide updates or merge please!",
      "@penpornk Updated the code and comments to address the review comments! I refrained from clicking all the resolve comment button, which i guess you will be doing. Let me know if any further changes are needed! @rthadur - FYI",
      "@rthadur addressed all review comments! ",
      "Thank you for your fast response! I'll review this PR tomorrow.\r\n\r\n> I refrained from clicking all the resolve comment button, which i guess you will be doing.\r\n\r\nThank you! Being able to click the resolve button is convenient to me. :) (But please also feel free to mark them as resolved if it's easier for you to keep track as well!)",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->",
      "Looks like the other author that @googlebot has a CLA problem with is me. Manually setting CLA to yes then.",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->",
      "@penpornk haven't completed it! will ping you late this evening or tomorrow and confirm edits. :) ",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->",
      "@nammbash Got it. Please take your time. :) And don't worry about the CLA. I have a feeling googlebot will do this a lot. I'll just change it again after we are done with the PR.",
      "@penpornk I have made the changes as requested! let me know. (As a side note, there is another PR that needs to go in after this on convolution Op support per channel. (https://github.com/tensorflow/tensorflow/pull/25506)\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#discussion_r\r\nSure! Note that the transpose operation per some internal reviews is was very fast with MKL. Not sure with eigen though. TODO well warranted.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#discussion_r253669164\r\nGood suggestion! Learnt something new thank you!\r\nThis was the delay as opposed to submitting it last Friday as the behavior was different in reality vs what was suggested. Took the time to verify logic and test suggestion.\r\nConclusion: It is reduction(max : out_min_max) and worked only with the max as a function and not with maximum as a logic! (So was in the middle of the testing things committed too soon). Changes upstreamed now!! \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#discussion_r253677900\r\nI see!",
      "@penpornk changes done!\r\n",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->",
      "@nammbash gentle ping to  check test failures.",
      "@rthadur Interesting! \r\n1. some of these are not related to CPU. (Like the GPU CC) . Correct me if I am wrong.\r\n2. the test //tensorflow/tools/api/tests:api_compatibility_test passes in c++. Is there a python version of the same test? Let me check into that",
      "@rthadur @penpornk \r\nInteresting findings on the build server failures. per your issue-comment: https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461208892\r\n\r\n**I am ignoring the following failures as it does not pertain to my PR:**\r\n```\r\nGPU CC\r\nGPU python 3\r\nMacOS\r\nWindows Bazel GPU\r\n```\r\n**The pertaining ones then is this**\r\n\r\n```Ubuntu python2```\r\n\r\n**google build server error as displayed by web link points to the test:**\r\n```\r\ntensorflow/tools/api/tests:api_compatibility_test\r\n\r\nand hints problem to be in the pbtxt apis of RequantizePerchannel. RequantizeRnagePerchannel and requantize.\r\n```\r\n**Findings when I run it locally**\r\n1. Bazeltest  succeeds if I use \"bazel test\" directly. \r\n```\r\nbazel --output_base=../test_folder test tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\n2. if I build it separately and then run it as the log online seems to indicate\r\n```\r\nbazel --output_base=../test_folder build tensorflow/tools/api/tests:api_compatibility_test\r\nthen\r\na. bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True (success)\r\nb. bazel-bin/tensorflow/tools/api/tests/api_compatibility_test  (Failed)\r\n\r\nyet the failure seems to not have any mention of perchannel or requantize ops.\r\n```\r\n**Analysis Conclusion:(see findings below)**\r\n```\r\n1.  I believe that google build server is having some issues, \r\nor \r\n2. maybe with this specific run \r\nor \r\n3. bazel bug??! Internal bazel build is different than what I have locally?? locally \r\nI am using bazel version: 0.19.2 running on Ubuntu xfce: 4.12\r\n```\r\n",
      "@nammbash You are right that other failures are unrelated. But the API ones are indeed because of this PR. \r\n\r\nI forgot about updating the API golden files. Sorry! Basically, when you add any new API pbtxt, you need to add an entry to the golden files too. I believe running the test with `--update_golden` modifies the necessary goldenfiles (e.g., tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt, ensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt, etc) for you.\r\n\r\nFor example, see how the golden files are modified in https://github.com/tensorflow/tensorflow/commit/de87e628e6d89382783ea948c21fa66b182d69d3. (You shouldn't modify it yourself. Let the test modify them and add the changes to the PR.)\r\n",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->",
      "@penpornk modifications done and committed. Please review and merge. https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461318566\r\n@rthadur  FYI\r\n\r\nAction:\r\nbuild test separately and then run\r\n```\r\nbazel --output_base=../test_folder build tensorflow/tools/api/tests:api_compatibility_test\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test  (Failed)\r\n\r\nbut\r\n\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True \r\nFails and updated the files\r\n\r\nbazel --output_base=../test_folder build tensorflow/tools/api/tests:api_compatibility_test\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test  (Success)\r\n\r\n",
      "@nammbash Hmm. This is weird. The golden files aren't related to your PR. (But the test passed without your PR.) I'll ask someone who actually knows more about them. Will get back to you soon.",
      "@penpornk \r\n:) Exactly my point from my comments yesterday: https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461262408.\r\n\r\nthank you for the internal reviews! Will await updates. ",
      "@nammbash So @annarev told me we actually don't need to modify the golden files at all since the visibility is hidden. The mismatch on your part might be the estimator version mismatch. But then I'm not sure why our Kokoro test failed on your PR and not on the current repo. Could you please try syncing your local copy? (Also, please drop the last commit changing the golden files.) Sorry for the inconvenience! ",
      "@nammbash I'm looking at the [test results from last night](https://source.cloud.google.com/results/invocations/dcc891bd-2e73-4b95-9a72-32de274bd127/log). The differences were actually the entries for your ops (`RequantizationPerChannel` and `RequantizationRangePerChannel`). That's why I thought they needed to be added to the golden files. Now there two questions: why did you see different results, and why did the test try to add your 'hidden' ops to the golden files. \r\n\r\n```\r\nE0207 07:22:46.303906 140336513210112 api_compatibility_test.py:216] TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\nE0207 07:22:46.304153 140336513210112 api_compatibility_test.py:217] 1 differences found between API and golden.\r\nIssue 1\t: None :\r\n---\r\n+++\r\n@@ -2645,10 +2645,18 @@\r\n     argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n   member_method {\r\n+    name: \"RequantizationRangePerChannel\"\r\n+    argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'clip_value_max\\'], varargs=None, keywords=None, defaults=None\"\r\n+  }\r\n+  member_method {\r\n     name: \"Requantize\"\r\n     argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'requested_output_min\\', \\'requested_output_max\\', \\'out_type\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n   member_method {\r\n+    name: \"RequantizePerChannel\"\r\n+    argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'requested_output_min\\', \\'requested_output_max\\', \\'out_type\\'], varargs=None, keywords=None, defaults=None\"\r\n+  }\r\n+  member_method {\r\n     name: \"Reshape\"\r\n     argspec: \"args=[\\'tensor\\', \\'shape\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n```",
      "@penpornk I was confused last afternoon on this question too. hence the question is there a build server/ specific run error.\r\n\r\nInterestingly  bazel test passes but bazel build and then running test fails! Why? and even if it fails it was not related to perchannel.\r\n\r\nbazel --output_base=../test_folder test tensorflow/tools/api/tests:api_compatibility_test passes.\r\n\r\nAnyways I am trying to undo based on https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461569094 and see where it takes us.\r\n",
      "@penpornk \r\nStatus Quo: https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461573084\r\n\r\nQ1: why did you see different results\r\nA1: No more after the merge with master from yesterday. I see the same error.\r\n\r\nQ2: why did the test try to add your 'hidden' ops to the golden files\r\nThis is confusing. test fails and investigating. Not sure why hidden is doing this. Yet still the changes made by the golden file is for the estimators and not my op. but the test passes after these changes? Is the problem even related! Weird!\r\n\r\nmaybe @gunan is able to provide some extra insights?!",
      "@nammbash Sorry for the delay! I was in meetings. \r\n@annarev thinks we are actually including hidden ops in the golden file now. She said you need to make sure those `raw_ops.*` golden files exist in your branch (you should have them after syncing), then pass `--only_test_core_api=True` to the test. \r\nLet us know if that works!",
      "To add to @penpornk reply, there is this RFC for exporting all ops to the raw_ops namespace:\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\r\nThis seems to include hidden ops as well. @alextp  would know more.\r\n\r\nAlso, to be more specific, --only_test_core_api=True needs to be passed in to api_compatibility_test when updating goldens. This flag is passed automatically when running test with \"bazel test\", but not when running it for golden update.",
      "Anna, should we update the error message in the test to instruct people to\npass it when updating goldens?\n\nOn Thu, Feb 7, 2019 at 3:26 PM annarev <notifications@github.com> wrote:\n\n> To add to @penpornk <https://github.com/penpornk> reply, there is this\n> RFC for exporting all ops to the raw_ops namespace:\n>\n> https://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\n> This seems to include hidden ops as well. @alextp\n> <https://github.com/alextp> would know more.\n>\n> Also, to be more specific, --only_test_core_api=True needs to be passed in\n> to api_compatibility_test when updating goldens. This flag is passed\n> automatically when running test with \"bazel test\", but not when running it\n> for golden update.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461632527>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcnOkmT6QfQhTfmMiMp6Wfpbytviks5vLLYwgaJpZM4aTxMo>\n> .\n>\n\n\n-- \n - Alex\n",
      "And yes, the raw_ops namespace includes hidden ops. Feel free to bypass API\nreview for it, though.\n\nOn Thu, Feb 7, 2019 at 4:14 PM Alexandre Passos <apassos@google.com> wrote:\n\n> Anna, should we update the error message in the test to instruct people to\n> pass it when updating goldens?\n>\n> On Thu, Feb 7, 2019 at 3:26 PM annarev <notifications@github.com> wrote:\n>\n>> To add to @penpornk <https://github.com/penpornk> reply, there is this\n>> RFC for exporting all ops to the raw_ops namespace:\n>>\n>> https://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\n>> This seems to include hidden ops as well. @alextp\n>> <https://github.com/alextp> would know more.\n>>\n>> Also, to be more specific, --only_test_core_api=True needs to be passed\n>> in to api_compatibility_test when updating goldens. This flag is passed\n>> automatically when running test with \"bazel test\", but not when running it\n>> for golden update.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461632527>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAATxcnOkmT6QfQhTfmMiMp6Wfpbytviks5vLLYwgaJpZM4aTxMo>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n"
    ],
    "review_comments": [
      {
        "body": "The formatting looks wrong. `graph_op_name` should be indented and `visibility` should be in a new line. Please also add `summary` and `description` fields. Please also add input and output info if applicable. See [api_def_FilterDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_FilterDataset.pbtxt) for example.\r\n\r\nThe same goes for `api_def_RequantizePerChannel.pbtxt`.",
        "diff_hunk": "@@ -0,0 +1,4 @@\n+op {\n+graph_op_name:\n+  \"RequantizationRangePerChannel\" visibility : HIDDEN\n+}"
      },
      {
        "body": "I think this can remain in one line since there is only one file in the list.",
        "diff_hunk": "@@ -6104,7 +6104,9 @@ tf_kernel_library(\n tf_cc_test(\n     name = \"requantization_range_op_test\",\n     size = \"small\",\n-    srcs = [\"requantization_range_op_test.cc\"],\n+    srcs = [\n+        \"requantization_range_op_test.cc\","
      },
      {
        "body": "```suggestion\r\n    deps = if_mkl([\r\n```\r\nAnd please remove the line with the single bracket below.",
        "diff_hunk": "@@ -6981,6 +6983,67 @@ tf_mkl_kernel_library(\n     deps = NN_DEPS + mkl_deps() + [\":cwise_op\"],\n )\n \n+tf_mkl_kernel_library(\n+    name = \"mkl_requantize_ops\",\n+    srcs = [\n+        \"mkl_requantization_range_per_channel_op.cc\",\n+        \"mkl_requantize_per_channel_op.cc\",\n+    ],\n+    hdrs = [\n+        \"meta_support.h\",\n+        \"no_op.h\",\n+        \"reference_gemm.h\",\n+    ],\n+    deps = if_mkl("
      },
      {
        "body": "The dependencies are sorted alphabetically. Please move `:transpose_functor` to the line after `:quantization_utils`.",
        "diff_hunk": "@@ -6981,6 +6983,67 @@ tf_mkl_kernel_library(\n     deps = NN_DEPS + mkl_deps() + [\":cwise_op\"],\n )\n \n+tf_mkl_kernel_library(\n+    name = \"mkl_requantize_ops\",\n+    srcs = [\n+        \"mkl_requantization_range_per_channel_op.cc\",\n+        \"mkl_requantize_per_channel_op.cc\",\n+    ],\n+    hdrs = [\n+        \"meta_support.h\",\n+        \"no_op.h\",\n+        \"reference_gemm.h\",\n+    ],\n+    deps = if_mkl(\n+        [\n+            \":concat_lib_hdrs\",\n+            \":conv_ops\",\n+            \":cwise_op\",\n+            \":eigen_helpers\",\n+            \":image_resizer_state\",\n+            \":ops_util\",\n+            \":pooling_ops\",\n+            \":quantization_utils\",\n+            \"//tensorflow/core:array_ops_op_lib\",\n+            \"//tensorflow/core:core_cpu\",\n+            \"//tensorflow/core:framework\",\n+            \"//tensorflow/core:lib\",\n+            \"//tensorflow/core:math_ops_op_lib\",\n+            \"//tensorflow/core:nn_ops_op_lib\",\n+            \"//third_party/eigen3\",\n+            \"@gemmlowp\",\n+            \":transpose_functor\","
      },
      {
        "body": "This should be immediately after the line with `//third_party/eigen3`.",
        "diff_hunk": "@@ -6981,6 +6983,67 @@ tf_mkl_kernel_library(\n     deps = NN_DEPS + mkl_deps() + [\":cwise_op\"],\n )\n \n+tf_mkl_kernel_library(\n+    name = \"mkl_requantize_ops\",\n+    srcs = [\n+        \"mkl_requantization_range_per_channel_op.cc\",\n+        \"mkl_requantize_per_channel_op.cc\",\n+    ],\n+    hdrs = [\n+        \"meta_support.h\",\n+        \"no_op.h\",\n+        \"reference_gemm.h\",\n+    ],\n+    deps = if_mkl(\n+        [\n+            \":concat_lib_hdrs\",\n+            \":conv_ops\",\n+            \":cwise_op\",\n+            \":eigen_helpers\",\n+            \":image_resizer_state\",\n+            \":ops_util\",\n+            \":pooling_ops\",\n+            \":quantization_utils\",\n+            \"//tensorflow/core:array_ops_op_lib\",\n+            \"//tensorflow/core:core_cpu\",\n+            \"//tensorflow/core:framework\",\n+            \"//tensorflow/core:lib\",\n+            \"//tensorflow/core:math_ops_op_lib\",\n+            \"//tensorflow/core:nn_ops_op_lib\",\n+            \"//third_party/eigen3\",\n+            \"@gemmlowp\",\n+            \":transpose_functor\",\n+            \"@mkl_dnn\",\n+            \"//third_party/mkl:intel_binary_blob\","
      },
      {
        "body": "```suggestion\r\n  if (lowest < -highest) ++lowest;\r\n```\r\nPlease also add a comment in the code saying why you adjusted the range (as opposed to not adjusting in the beginning).",
        "diff_hunk": "@@ -16,16 +16,18 @@ limitations under the License.\n #ifndef TENSORFLOW_CORE_KERNELS_MKL_QUANTIZED_CONV_OPS_H_\n #define TENSORFLOW_CORE_KERNELS_MKL_QUANTIZED_CONV_OPS_H_\n \n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n \n #ifdef INTEL_MKL\n \n namespace tensorflow {\n template <class T>\n float MklFloatForOneQuantizedLevel(float range_min, float range_max) {\n-  const int64 highest = static_cast<int64>(Eigen::NumTraits<T>::highest());\n-  const int64 lowest = static_cast<int64>(Eigen::NumTraits<T>::lowest());\n+  int64 highest = static_cast<int64>(Eigen::NumTraits<T>::highest());\n+  int64 lowest = static_cast<int64>(Eigen::NumTraits<T>::lowest());\n+  if (lowest < -highest) lowest += 1;"
      },
      {
        "body": "```suggestion\r\n  DCHECK(min_b_vector.NumElements() == (*min_c_vector)->NumElements());\r\n```\r\nWe are replacing `CHECK` with `DCHECK`.",
        "diff_hunk": "@@ -48,6 +50,35 @@ void MklQuantizationRangeForMultiplication(float min_a, float max_a,\n   *min_c = c_float_for_one_quant_level * c_lowest;\n   *max_c = c_float_for_one_quant_level * c_highest;\n }\n+\n+template <class T1, class T2, class T3>\n+void MklQuantizationRangeForMultiplication(float min_a, float max_a,\n+                                           const Tensor& min_b_vector,\n+                                           const Tensor& max_b_vector,\n+                                           Tensor** min_c_vector,\n+                                           Tensor** max_c_vector) {\n+  CHECK(min_b_vector.NumElements() == (*min_c_vector)->NumElements());"
      },
      {
        "body": "```suggestion\r\n  DCHECK(max_b_vector.NumElements() == (*max_c_vector)->NumElements());\r\n```",
        "diff_hunk": "@@ -48,6 +50,35 @@ void MklQuantizationRangeForMultiplication(float min_a, float max_a,\n   *min_c = c_float_for_one_quant_level * c_lowest;\n   *max_c = c_float_for_one_quant_level * c_highest;\n }\n+\n+template <class T1, class T2, class T3>\n+void MklQuantizationRangeForMultiplication(float min_a, float max_a,\n+                                           const Tensor& min_b_vector,\n+                                           const Tensor& max_b_vector,\n+                                           Tensor** min_c_vector,\n+                                           Tensor** max_c_vector) {\n+  CHECK(min_b_vector.NumElements() == (*min_c_vector)->NumElements());\n+  CHECK(max_b_vector.NumElements() == (*max_c_vector)->NumElements());"
      },
      {
        "body": "```suggestion\r\n  for (size_t n = 0; n < n_channel; ++n) {\r\n```",
        "diff_hunk": "@@ -48,6 +50,35 @@ void MklQuantizationRangeForMultiplication(float min_a, float max_a,\n   *min_c = c_float_for_one_quant_level * c_lowest;\n   *max_c = c_float_for_one_quant_level * c_highest;\n }\n+\n+template <class T1, class T2, class T3>\n+void MklQuantizationRangeForMultiplication(float min_a, float max_a,\n+                                           const Tensor& min_b_vector,\n+                                           const Tensor& max_b_vector,\n+                                           Tensor** min_c_vector,\n+                                           Tensor** max_c_vector) {\n+  CHECK(min_b_vector.NumElements() == (*min_c_vector)->NumElements());\n+  CHECK(max_b_vector.NumElements() == (*max_c_vector)->NumElements());\n+  size_t n_channel = min_b_vector.NumElements();\n+  const int64 c_highest = static_cast<int64>(Eigen::NumTraits<T3>::highest());\n+  const int64 c_lowest = static_cast<int64>(Eigen::NumTraits<T3>::lowest());\n+  const float* min_b = min_b_vector.flat<float>().data();\n+  const float* max_b = max_b_vector.flat<float>().data();\n+  float* min_c = (*min_c_vector)->flat<float>().data();\n+  float* max_c = (*max_c_vector)->flat<float>().data();\n+#pragma omp parallel for\n+  for (size_t n = 0; n < n_channel; n++) {"
      },
      {
        "body": "```suggestion\r\n/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n```",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved."
      },
      {
        "body": "```suggestion\r\n                errors::InvalidArgument(\"input_min has incorrect size, expected \",\r\n```",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \","
      },
      {
        "body": "```suggestion\r\n                errors::InvalidArgument(\"input_max has incorrect size, expected \",\r\n```",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \","
      },
      {
        "body": "```suggestion\r\n      ranges[i] = scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\r\n```",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \",\n+                                        depth, \" was \", input_max.dim_size(0)));\n+\n+    const float* input_min_data = input_min.flat<float>().data();\n+    const float* input_max_data = input_max.flat<float>().data();\n+    std::vector<float> ranges(depth);\n+    bool is_non_negative = true;\n+    Eigen::array<int, 2> shuffling({1, 0});\n+    auto input_matrix = input.flat_inner_dims<qint32>();\n+    auto transposed_input = input_matrix.shuffle(shuffling);\n+\n+#pragma omp parallel for\n+    for (size_t i = 0; i < depth; i++) {\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n+          transposed_input.chip<0>(i).minimum();\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n+          transposed_input.chip<0>(i).maximum();\n+      int32_t min_per_channel = min();\n+      int32_t max_per_channel = max();\n+      int32_t abs_max =\n+          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n+      float scale =\n+          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n+      ranges[i] = (scale * (float)abs_max / (float)(1L << 31));"
      },
      {
        "body": "Why do we need to transpose the input?",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \",\n+                                        depth, \" was \", input_max.dim_size(0)));\n+\n+    const float* input_min_data = input_min.flat<float>().data();\n+    const float* input_max_data = input_max.flat<float>().data();\n+    std::vector<float> ranges(depth);\n+    bool is_non_negative = true;\n+    Eigen::array<int, 2> shuffling({1, 0});\n+    auto input_matrix = input.flat_inner_dims<qint32>();\n+    auto transposed_input = input_matrix.shuffle(shuffling);"
      },
      {
        "body": "```suggestion\r\n    for (size_t i = 0; i < depth; ++i) {\r\n```",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \",\n+                                        depth, \" was \", input_max.dim_size(0)));\n+\n+    const float* input_min_data = input_min.flat<float>().data();\n+    const float* input_max_data = input_max.flat<float>().data();\n+    std::vector<float> ranges(depth);\n+    bool is_non_negative = true;\n+    Eigen::array<int, 2> shuffling({1, 0});\n+    auto input_matrix = input.flat_inner_dims<qint32>();\n+    auto transposed_input = input_matrix.shuffle(shuffling);\n+\n+#pragma omp parallel for\n+    for (size_t i = 0; i < depth; i++) {"
      },
      {
        "body": "This can be done in the previous loop.",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \",\n+                                        depth, \" was \", input_max.dim_size(0)));\n+\n+    const float* input_min_data = input_min.flat<float>().data();\n+    const float* input_max_data = input_max.flat<float>().data();\n+    std::vector<float> ranges(depth);\n+    bool is_non_negative = true;\n+    Eigen::array<int, 2> shuffling({1, 0});\n+    auto input_matrix = input.flat_inner_dims<qint32>();\n+    auto transposed_input = input_matrix.shuffle(shuffling);\n+\n+#pragma omp parallel for\n+    for (size_t i = 0; i < depth; i++) {\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n+          transposed_input.chip<0>(i).minimum();\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n+          transposed_input.chip<0>(i).maximum();\n+      int32_t min_per_channel = min();\n+      int32_t max_per_channel = max();\n+      int32_t abs_max =\n+          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n+      float scale =\n+          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n+      ranges[i] = (scale * (float)abs_max / (float)(1L << 31));\n+      if (min_per_channel < 0) is_non_negative = false;\n+    }\n+\n+    float out_min_max = std::numeric_limits<float>::min();\n+    for (size_t i = 0; i < depth; i++) {\n+      if (out_min_max < ranges[i]) out_min_max = ranges[i];"
      },
      {
        "body": "```suggestion\r\n  const int kInputMinIndex = 1;\r\n```\r\nThe same goes for `kInputMax`, `kOutputMin`, and `kOutputMax`.",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \",\n+                                        depth, \" was \", input_max.dim_size(0)));\n+\n+    const float* input_min_data = input_min.flat<float>().data();\n+    const float* input_max_data = input_max.flat<float>().data();\n+    std::vector<float> ranges(depth);\n+    bool is_non_negative = true;\n+    Eigen::array<int, 2> shuffling({1, 0});\n+    auto input_matrix = input.flat_inner_dims<qint32>();\n+    auto transposed_input = input_matrix.shuffle(shuffling);\n+\n+#pragma omp parallel for\n+    for (size_t i = 0; i < depth; i++) {\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n+          transposed_input.chip<0>(i).minimum();\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n+          transposed_input.chip<0>(i).maximum();\n+      int32_t min_per_channel = min();\n+      int32_t max_per_channel = max();\n+      int32_t abs_max =\n+          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n+      float scale =\n+          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n+      ranges[i] = (scale * (float)abs_max / (float)(1L << 31));\n+      if (min_per_channel < 0) is_non_negative = false;\n+    }\n+\n+    float out_min_max = std::numeric_limits<float>::min();\n+    for (size_t i = 0; i < depth; i++) {\n+      if (out_min_max < ranges[i]) out_min_max = ranges[i];\n+    }\n+    // Fixing max to clip_value_max_ (example 6.0 to support relu6)\n+    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n+\n+    Tensor* output_min = nullptr;\n+    Tensor* output_max = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMin, {}, &output_min));\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMax, {}, &output_max));\n+    output_min->flat<float>()(0) = is_non_negative ? 0.0f : out_min_max * -1.0f;\n+    output_max->flat<float>()(0) = out_min_max;\n+  }\n+\n+ private:\n+  float clip_value_max_ = std::numeric_limits<float>::infinity();\n+  const int kInputTensorIndex = 0;\n+  const int kInputMin = 1;"
      },
      {
        "body": "```suggestion\r\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\r\n```",
        "diff_hunk": "@@ -0,0 +1,110 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+#ifdef INTEL_MKL\n+#define EIGEN_USE_THREADS\n+\n+#include <math.h>\n+#include <limits>\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/meta_support.h\"\n+#include \"tensorflow/core/kernels/no_op.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+class MklRequantizationRangePerChannelOp : public OpKernel {\n+ public:\n+  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input = ctx->input(kInputTensorIndex);\n+    const Tensor& input_min = ctx->input(kInputMin);\n+    const Tensor& input_max = ctx->input(kInputMax);\n+\n+    size_t depth = input_max.NumElements();\n+    OP_REQUIRES(ctx, input_min.dim_size(0) == depth,\n+                errors::InvalidArgument(\"min has incorrect size, expected \",\n+                                        depth, \" was \", input_min.dim_size(0)));\n+    OP_REQUIRES(ctx, input_max.dim_size(0) == depth,\n+                errors::InvalidArgument(\"max has incorrect size, expected \",\n+                                        depth, \" was \", input_max.dim_size(0)));\n+\n+    const float* input_min_data = input_min.flat<float>().data();\n+    const float* input_max_data = input_max.flat<float>().data();\n+    std::vector<float> ranges(depth);\n+    bool is_non_negative = true;\n+    Eigen::array<int, 2> shuffling({1, 0});\n+    auto input_matrix = input.flat_inner_dims<qint32>();\n+    auto transposed_input = input_matrix.shuffle(shuffling);\n+\n+#pragma omp parallel for\n+    for (size_t i = 0; i < depth; i++) {\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n+          transposed_input.chip<0>(i).minimum();\n+      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n+          transposed_input.chip<0>(i).maximum();\n+      int32_t min_per_channel = min();\n+      int32_t max_per_channel = max();\n+      int32_t abs_max =\n+          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n+      float scale =\n+          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n+      ranges[i] = (scale * (float)abs_max / (float)(1L << 31));\n+      if (min_per_channel < 0) is_non_negative = false;\n+    }\n+\n+    float out_min_max = std::numeric_limits<float>::min();\n+    for (size_t i = 0; i < depth; i++) {\n+      if (out_min_max < ranges[i]) out_min_max = ranges[i];\n+    }\n+    // Fixing max to clip_value_max_ (example 6.0 to support relu6)\n+    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n+\n+    Tensor* output_min = nullptr;\n+    Tensor* output_max = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMin, {}, &output_min));\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMax, {}, &output_max));\n+    output_min->flat<float>()(0) = is_non_negative ? 0.0f : out_min_max * -1.0f;"
      },
      {
        "body": "C headers should be in a group above custom headers, i.e.,\r\n```c++\r\n#include <cmath>\r\n\r\n#include \"tensorflow/core/framework/allocator.h\"\r\n...\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>"
      },
      {
        "body": "```suggestion\r\n  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht*wt*2\r\n```\r\nPlease also fixes the three lines below.",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2"
      },
      {
        "body": "```suggestion\r\n  // Step 1: Input range assumptions\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs"
      },
      {
        "body": "```suggestion\r\n  // Step 2: Quantization details (per channel)\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)"
      },
      {
        "body": "```suggestion\r\n  // Range of Quantized T  in uint8[0   , 255] maps to orig T  in FP32[0   , 5.0]\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8.\n+  // While the input Filter W (NHWC) is qunatized to signed int8.\n+  // hence T max value is mapped to ((2^8-1) = 255) while W to ((2^7)-1 = 127))\n+\n+  // Range of Quantized T  in int8[0   , 255] maps to orig T  in FP32[0   , 5.0]"
      },
      {
        "body": "```suggestion\r\n  // T and W are quantized using a Quantize Op.\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op."
      },
      {
        "body": "```suggestion\r\n  // The input Tensor T (NHWC) is quantized to unsigned int8.\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8."
      },
      {
        "body": "```suggestion\r\n  // The input Filter W (NHWC) is quantized to signed int8.\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8.\n+  // While the input Filter W (NHWC) is qunatized to signed int8."
      },
      {
        "body": "```suggestion\r\n  // Hence, T's max value is mapped to ((2^8-1) = 255), while W's to ((2^7)-1 = 127)).\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8.\n+  // While the input Filter W (NHWC) is qunatized to signed int8.\n+  // hence T max value is mapped to ((2^8-1) = 255) while W to ((2^7)-1 = 127))"
      },
      {
        "body": "```suggestion\r\n  // The output Tensor T is in int32 whose range is [-2^31, 2^31)\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8.\n+  // While the input Filter W (NHWC) is qunatized to signed int8.\n+  // hence T max value is mapped to ((2^8-1) = 255) while W to ((2^7)-1 = 127))\n+\n+  // Range of Quantized T  in int8[0   , 255] maps to orig T  in FP32[0   , 5.0]\n+  // Range of Quantized W1 in int8[-127, 127] maps to orig W1 in FP32[-2.0, 2.0]\n+  // Range of Quantized W2 in int8[-127, 127] maps to orig W2 in FP32[-3.0, 3.0]\n+\n+  // Hence the resolution of Quantized T will be 5.0/255\n+  // Hence the resolution of Quantized W1 will be 2.0/127\n+  // Hence the resolution of Quantized W2 will be 3.0/127\n+\n+  // Step 3: Assumption of quantizedconv on quantized input&weights(per channel)\n+  // ---------------------------------------------------------------------------\n+  // The input T and weights W1 (or W2) will be convolved (and multipled)\n+  // The output Tensor T is in int32 whose range is [-2^31, 2^31]"
      },
      {
        "body": "Shouldn't `2^31` be `2^31-1`? `int32` is from `-2^31` to `2^31-1`, and you are narrowing the negative range to be the same as the positive range, `[-(2^31-1), 2^31 - 1]`.",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8.\n+  // While the input Filter W (NHWC) is qunatized to signed int8.\n+  // hence T max value is mapped to ((2^8-1) = 255) while W to ((2^7)-1 = 127))\n+\n+  // Range of Quantized T  in int8[0   , 255] maps to orig T  in FP32[0   , 5.0]\n+  // Range of Quantized W1 in int8[-127, 127] maps to orig W1 in FP32[-2.0, 2.0]\n+  // Range of Quantized W2 in int8[-127, 127] maps to orig W2 in FP32[-3.0, 3.0]\n+\n+  // Hence the resolution of Quantized T will be 5.0/255\n+  // Hence the resolution of Quantized W1 will be 2.0/127\n+  // Hence the resolution of Quantized W2 will be 3.0/127\n+\n+  // Step 3: Assumption of quantizedconv on quantized input&weights(per channel)\n+  // ---------------------------------------------------------------------------\n+  // The input T and weights W1 (or W2) will be convolved (and multipled)\n+  // The output Tensor T is in int32 whose range is [-2^31, 2^31]\n+  // The Range of the Convolved T*W1 is 2^31 * 5.0/255 * 2.0/127 = 663110.59"
      },
      {
        "body": "```suggestion\r\n  // make good use in int8 quantization from int32 to int8.\r\n```",
        "diff_hunk": "@@ -0,0 +1,297 @@\n+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/allocator.h\"\n+#include \"tensorflow/core/framework/fake_input.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/kernels/ops_testutil.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/platform/test_benchmark.h\"\n+\n+#include <cmath>\n+\n+namespace tensorflow {\n+\n+class MklRequantizatedOpsTest : public OpsTestBase {};\n+\n+class MklRequantizatedOpsTestHelper : public OpsTestBase {\n+ public:\n+  void Setup(Tensor &input_tensor_qint32, float &range_weights_ch1,\n+             float &range_weights_ch2);\n+  void TestBody(){};\n+};\n+\n+void MklRequantizatedOpsTestHelper::Setup(Tensor &input_tensor_qint32,\n+                                          float &range_weights_ch1,\n+                                          float &range_weights_ch2) {\n+  // Step 1: Assumption of inputs\n+  // ----------------------------\n+  // Assume input Tensor T (NHWC) in FP32 has range [0, 5.0]   size nt*ht*wt*ct\n+  // Assume input Filter W (NHWC) with 2 output channels of    size nw*ht**wt*2\n+  // logically,   Filter W has 2 channels W1 and W2 each of    size nw*ht**wt*1\n+  // Assume input Filter W1(NHWC) in FP32 has range [-2.0, 2.0]size nw*ht**wt*1\n+  // Assume input Filter W2(NHWC) in FP32 has range [-3.0, 3.0]size nw*ht**wt*1\n+\n+  // Step 2: Assumption of Quantizing inputs and weights (per channel)\n+  // ------------------------------------------------------------------\n+  // When these 2 Tensors, T and W are quantized using a Quantize Op.\n+  // When  the input Tensor T (NHWC) is quantized to unsigned int8.\n+  // While the input Filter W (NHWC) is qunatized to signed int8.\n+  // hence T max value is mapped to ((2^8-1) = 255) while W to ((2^7)-1 = 127))\n+\n+  // Range of Quantized T  in int8[0   , 255] maps to orig T  in FP32[0   , 5.0]\n+  // Range of Quantized W1 in int8[-127, 127] maps to orig W1 in FP32[-2.0, 2.0]\n+  // Range of Quantized W2 in int8[-127, 127] maps to orig W2 in FP32[-3.0, 3.0]\n+\n+  // Hence the resolution of Quantized T will be 5.0/255\n+  // Hence the resolution of Quantized W1 will be 2.0/127\n+  // Hence the resolution of Quantized W2 will be 3.0/127\n+\n+  // Step 3: Assumption of quantizedconv on quantized input&weights(per channel)\n+  // ---------------------------------------------------------------------------\n+  // The input T and weights W1 (or W2) will be convolved (and multipled)\n+  // The output Tensor T is in int32 whose range is [-2^31, 2^31]\n+  // The Range of the Convolved T*W1 is 2^31 * 5.0/255 * 2.0/127 = 663110.59\n+  // So Range of Convolved T*W1 in int32[-2^31, 22^31] that maps to\n+  // orig T Range in FP32[0,5.0] * [-2.0, 2.0] is [-663110.59, 663110.59]\n+\n+  // The Range of the Convolved T*W2 is 2^31 * 5.0/255 * 3.0/127 = 994665.88\n+  // So Range of Convolved T*W2 in int32[-2^31, 22^31] that maps to\n+  // orig T Range in FP32[0,5.0] * [-3.0, 3.0]  is [-994665.88, 994665.88]\n+\n+  // Step 4: Assumption output above is fed to Requantization_range_perchannel\n+  // --------------------------------------------------------------------------\n+  // Here we recalculate the new Range for Convolved T*W so that we\n+  // make good use in int8 qunatization from int32 to int8."
      }
    ],
    "body": "## Requantization_Perchannel_op\r\n\r\n1. Adding two new hidden operators to support requantization to be performed perchannel and provided the mkl implementations for the same.\r\n\r\n        1. requantize_per_channel_op\r\n        2. requantization_range_per_channel_op\r\n\r\n2. Provided Unit test to test the implementation logic of both these above mentioned ops.",
    "timestamp": "2025-05-06 01:27:24"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/9376",
    "comments": [
      "Can one of the admins verify this patch?",
      "I jumped the gun and made the pull request before getting feedback on issue #9369. Thus, I'll CC @martinwicke here.",
      "Can you rebase against master? There are some changes in your branch that are not in master.",
      "Before we go into details, I am wondering whether shuffle is the best name. I associate shuffling with some sort of random permutation being applied, which clearly isn't the case here.\r\n\r\nThe example in the docstring didn't really make it clear to me, it just looks like reshape (and so does the test), can you explain what this actually does?",
      "@martinwicke I think your concern regarding the name of the operation is valid. I had difficulty coming up with a generic enough operation name and \"shuffle\" was the best I could come up with. Maybe \"periodic_arrange\" or \"periodic_intersperse\" may be better, but I'm open to suggestions.\r\n\r\nRegarding what the operation actually does, it performs a generic version of the sub-pixel convolution found [here](https://arxiv.org/pdf/1609.05158.pdf) (Figure 1). My version allows for analogous operations for tensors of arbitrary rank and and relaxes a few of the other restrictions. For instance, the operation can handle rearranging a tensor of shape [2,3,6] into a tensor of shape [6,6,1]. I do need to add more examples and tests. I'll devise more tests and examples.\r\n\r\nIn the mean time, I currently only perform `assert`s rather than actually throwing errors. Can you recommend a format for raising exceptions, etc. in TF?",
      "Errors should be reported using the OP_REQUIRES macros. You can look at other kernels for examples. They will cleanly return with an appropriate Status.",
      "ping for @jhetherly ! \r\n\r\nTrying to get our active PR count down, so a response would be appreciated!",
      "I'm in the process of correctly converting the `assert`s to using the `OP_REQUIRES ` macro. I'll try to have something ready for further review by tomorrow.",
      "Just to update: I've had a bit of trouble with hardware failure which has slowed my progress. I will still try to have something by the end of the day.",
      "I've committed an updated name (`periodic_intersperse`), proper error handling, and more mathematical detail regarding what the transformation does located in the documentation string (tensorflow/contrib/periodic_intersperse/core/ops/array_ops.cc). The math is written in Latex, so I was curious if TF includes MathJax in it's documentation pages. If not, is it possible to specify this in the documentation string?",
      "Can one of the admins verify this patch?",
      "@suharshs would you have time to review this?",
      "I pushed the changes that @suharshs recommended.",
      "We do support MathJax. There's a bunch of math in the tutorials, and the code docs go through the same pipeline.",
      "Hey @suharshs, I just uploaded the files after running clang-format",
      "Thanks, I am gonna ask Rasmus to take a closer look (or find someone more knowledgeable than me) at the op implementation, which i am not too familiar with.",
      "I think \"periodic_intersperse\" is a very confusing name. This feel more like a (nearest neighbor) resampling or slicing operation. Is there an equivalent numpy method from which we can get inspiration for a better name?",
      "How does periodic_resample sound, @rmlarsen?",
      "@rmlarsen, if you think my proposed name of the op is suitable (periodic_resample) I can upload the change rather quickly",
      "@jhetherly periodic_resample sounds good.",
      "@jhetherly @martinwicke @shlens Before we proceed, I'd like to question if this is a general enough op to eventually belong in core TensorFlow?",
      "@rmlarsen,\r\nI'm currently battling an issue similar to #10436 in getting the current master to compile. Regardless, once I resolve this issue I'll implement the desired changes and comment on each issue in turn. Regarding your comment on making this a core op, this is how I envisioned it eventually being used as it's quite generic.",
      "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->",
      "@jhetherly Just to be clear, did you address all reviewer comments in your last push?",
      "@drpngx no, not yet. I'll likely have addressed all of them by the end of today",
      "drpngx@ @jhetherly I will be on vacation until July 26, so I am unassigning myself.",
      "Thanks @rmlarsen !",
      "@drpngx and @rmlarsen: I've implemented the requested changes including more Python tests.",
      "Jenkins, test this please.",
      "BTW, the CLA switched to no. Did you add another author?"
    ],
    "review_comments": [
      {
        "body": "Remove these commented out sections in this file?",
        "diff_hunk": "@@ -0,0 +1,125 @@\n+# Description:\n+# An example for custom op and kernel defined as a TensorFlow plugin.\n+\n+package(\n+    default_visibility = [\"//visibility:public\"],\n+)\n+\n+licenses([\"notice\"])  # Apache 2.0\n+\n+exports_files([\"LICENSE\"])\n+\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_gen_op_libs\")\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_py_test\")\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_gen_op_wrapper_py\")\n+\n+filegroup(\n+    name = \"custom_op_sources\",\n+    srcs = glob(\n+        [\n+            \"core/ops/*.cc\",\n+            \"core/kernels/*.cc\",\n+        ],\n+        exclude = [\n+            \"core/ops/*_test.cc\",\n+            \"core/kernels/*_test.cc\",\n+        ],\n+    ),\n+)\n+\n+filegroup(\n+    name = \"custom_op_headers\",\n+    srcs = glob(\n+        [\n+            \"core/kernels/*.h\",\n+            \"core/ops/*.h\",\n+        ],\n+    ),\n+)\n+\n+cc_library(\n+    name = \"all_ops\",\n+    srcs = [\":custom_op_sources\"],\n+    hdrs = [\":custom_op_headers\"],\n+    deps = [\n+        \"//tensorflow/core:framework_headers_lib\",\n+    ],\n+    alwayslink = 1,\n+)\n+\n+tf_custom_op_library(\n+    name = \"python/ops/_periodic_intersperse_op.so\",\n+    srcs = [\":custom_op_sources\", \":custom_op_headers\"],\n+)\n+\n+py_library(\n+    name = \"init_py\",\n+    srcs = [\n+        \"__init__.py\",\n+        \"python/__init__.py\",\n+    ],\n+    srcs_version = \"PY2AND3\",\n+    deps = [\n+        \":periodic_intersperse_op_py\",\n+    ],\n+)\n+\n+#tf_gen_op_libs("
      },
      {
        "body": "Please add `}  // namespace tensorflow` to the closing of this namespace.",
        "diff_hunk": "@@ -0,0 +1,27 @@\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/contrib/periodic_intersperse/core/kernels/periodic_intersperse_op.h\"\n+\n+namespace tensorflow {\n+\n+REGISTER_KERNEL_BUILDER(Name(\"PeriodicIntersperse\")\n+                            .HostMemory(\"desired_shape\")\n+                            .Device(DEVICE_CPU),\n+                        PeriodicIntersperseOp);\n+\n+#define REGISTER_GPU_KERNEL(type)                               \\\n+  REGISTER_KERNEL_BUILDER(Name(\"PeriodicIntersperse\")           \\\n+                              .Device(DEVICE_GPU)               \\\n+                              .HostMemory(\"desired_shape\")      \\\n+                              .TypeConstraint<type>(\"T\"),       \\\n+                          PeriodicIntersperseOp);\n+TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU_KERNEL);\n+#undef REGISTER_GPU_KERNEL\n+\n+#ifdef TENSORFLOW_USE_SYCL\n+REGISTER_KERNEL_BUILDER(Name(\"PeriodicIntersperse\")\n+                            .Device(DEVICE_SYCL)\n+                            .HostMemory(\"output\"),\n+                        PeriodicIntersperseOp);\n+#endif  // TENSORFLOW_USE_SYCL\n+\n+}"
      },
      {
        "body": "Please avoid `using namespace` to avoid accidental name collisions. Instead use the explicit name or `namespace tf = tensorflow` to write less.",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;"
      },
      {
        "body": "nit: Could you remove NOTE from these comments :) (otherwise thanks for the useful commenting)",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  // rasterize the input index\n+  {\n+    Index_t last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      Index_t index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;\n+          if (qi == q) continue;\n+          index += G[qi]*(output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        // index *= Y[r];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor*index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+\n+template<class T, class Vec_t>\n+void main_logic (OpKernelContext *context,\n+                 const Vec_t &desired_shape,\n+                 const Tensor &input_tensor)\n+{\n+  // NOTE input is a strided array (last index is fastest, C-ordered)"
      },
      {
        "body": "minor, could you import this after `import tensorflow as tf` to make it clearer what the dependencies of this test are?",
        "diff_hunk": "@@ -0,0 +1,27 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+\n+class PeriodicIntersperseTest(tf.test.TestCase):\n+    def testPeriodicIntersperse(self):\n+        from tensorflow.contrib.periodic_intersperse import periodic_intersperse as pi"
      },
      {
        "body": "Please add the tensorflow copyright header to the top of all added files.\r\n\r\n```\r\n// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n//\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n//     http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n// =============================================================================\r\n```",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_"
      },
      {
        "body": "are these for loops the same as `for (auto r = rank - 1; r > 0; --r)`?\r\n\r\nIf so could we put the end condition into the for loop to make it easier to read? Same below.",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;"
      },
      {
        "body": "desired_shape",
        "diff_hunk": "@@ -0,0 +1,71 @@\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+using namespace tensorflow;\n+\n+REGISTER_OP(\"PeriodicIntersperse\")\n+    .Attr(\"T: numbertype\")\n+    .Attr(\"S: {int32, int64}\")\n+    .Input(\"values: T\")\n+    .Input(\"desired_shape: S\")\n+    .Output(\"output: T\")\n+    .Doc(R\"doc(\n+Periodicly intersperses elements of a tensor to conform to `desired shape`."
      },
      {
        "body": "Preiodically -> Periodically",
        "diff_hunk": "@@ -0,0 +1,71 @@\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+using namespace tensorflow;\n+\n+REGISTER_OP(\"PeriodicIntersperse\")\n+    .Attr(\"T: numbertype\")\n+    .Attr(\"S: {int32, int64}\")\n+    .Input(\"values: T\")\n+    .Input(\"desired_shape: S\")\n+    .Output(\"output: T\")\n+    .Doc(R\"doc(\n+Periodicly intersperses elements of a tensor to conform to `desired shape`.\n+\n+This function implements a slightly more generic version of the subpixel\n+convolutions found in this [paper](https://arxiv.org/abs/1609.05158).\n+\n+The formula for computing the elements in the `output` tensor is as follows:\n+  `T` = `values` tensor of rank `R`\n+  `S` = `desired_shape` tensor (vector of length `R`)\n+  `P` = `output` tensor of rank `R`\n+  \\((T_1,\\ldots,T_R)\\) = shape(`T`)\n+  \\([S_1,\\ldots,S_q,\\ldots,S_R]\\) = elements of vector `S`\n+\n+  A single element in `S` is left unspecified (denoted \\(S_q=-1\\)).\n+  Let \\(f_i\\) denote the (possibly non-integer) factor that relates the original\n+  dimension to the desired dimensions, \\(S_i=f_i T_i\\), for \\(i\\neq q\\) where\n+  \\(f_i>0\\).\n+  Define the following:\n+    \\(g_i=\\lceil f_i\\rceil\\)\n+    \\(t=\\prod_i T_i\\)\n+    \\(s=\\prod_{i\\neq q} S_i\\)\n+  \\(S_q\\) can then be defined as by \\(S_q=\\lfloor t/s\\rfloor\\).\n+  The elements of the resulting tensor are defined as\n+  \\(P_{s_1,\\ldots,s_R}=T_{h_1,\\ldots,h_q,\\ldots,h_R}\\).\n+  The \\(h_i\\) (\\(i\\neq q\\)) are defined by \\(h_i=\\lfloor s_i/g_i\\rfloor\\).\n+  \\(h_q=S_q\\sum_{j\\neq q}^{q-1}G_j \\mathrm{mod}(s_j,g_j) + s_q\\), where\n+  \\(G_j=\\prod_{i}^{j-1}g_i\\) (\\(G_0=1\\)).\n+\n+One drawback of this method is that whenever the output dimensions are slightly\n+less than integer multiples of the input dimensions, many of the tensor elements\n+are repeated in an inefficient way. This is resolved be specifying that all\n+desired dimensions are integer multiples of the input tensor.\n+\n+For example:\n+\n+```prettyprint\n+`input` is [[ 0  1  2  3]\n+            [ 4  5  6  7]\n+            [ 8  9 10 11]]\n+\n+tf.periodic_intersperse(input, [6, -1]) ==> [[ 0  1]\n+                                             [ 2  3]\n+                                             [ 4  5]\n+                                             [ 6  7]\n+                                             [ 8  9]\n+                                             [10 11]]\n+```\n+\n+values: The tensor of rank `R` to periodic_intersperse\n+desired_shape: A 1-D tensor representing the desired shape of the output tensor.\n+  Exactly one element of this tensor must have the value `-1` which represents\n+  that this dimension of `values` can be adjusted downward in order to\n+  accomodate increases in other dimensions. The specified sizes of the\n+  non-adjustable dimensions must by at least as large as in the `values` tensor.\n+output: Preiodically interspersed tensor that has dimensions specified as in"
      },
      {
        "body": "be -> by",
        "diff_hunk": "@@ -0,0 +1,71 @@\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+using namespace tensorflow;\n+\n+REGISTER_OP(\"PeriodicIntersperse\")\n+    .Attr(\"T: numbertype\")\n+    .Attr(\"S: {int32, int64}\")\n+    .Input(\"values: T\")\n+    .Input(\"desired_shape: S\")\n+    .Output(\"output: T\")\n+    .Doc(R\"doc(\n+Periodicly intersperses elements of a tensor to conform to `desired shape`.\n+\n+This function implements a slightly more generic version of the subpixel\n+convolutions found in this [paper](https://arxiv.org/abs/1609.05158).\n+\n+The formula for computing the elements in the `output` tensor is as follows:\n+  `T` = `values` tensor of rank `R`\n+  `S` = `desired_shape` tensor (vector of length `R`)\n+  `P` = `output` tensor of rank `R`\n+  \\((T_1,\\ldots,T_R)\\) = shape(`T`)\n+  \\([S_1,\\ldots,S_q,\\ldots,S_R]\\) = elements of vector `S`\n+\n+  A single element in `S` is left unspecified (denoted \\(S_q=-1\\)).\n+  Let \\(f_i\\) denote the (possibly non-integer) factor that relates the original\n+  dimension to the desired dimensions, \\(S_i=f_i T_i\\), for \\(i\\neq q\\) where\n+  \\(f_i>0\\).\n+  Define the following:\n+    \\(g_i=\\lceil f_i\\rceil\\)\n+    \\(t=\\prod_i T_i\\)\n+    \\(s=\\prod_{i\\neq q} S_i\\)\n+  \\(S_q\\) can then be defined as by \\(S_q=\\lfloor t/s\\rfloor\\).\n+  The elements of the resulting tensor are defined as\n+  \\(P_{s_1,\\ldots,s_R}=T_{h_1,\\ldots,h_q,\\ldots,h_R}\\).\n+  The \\(h_i\\) (\\(i\\neq q\\)) are defined by \\(h_i=\\lfloor s_i/g_i\\rfloor\\).\n+  \\(h_q=S_q\\sum_{j\\neq q}^{q-1}G_j \\mathrm{mod}(s_j,g_j) + s_q\\), where\n+  \\(G_j=\\prod_{i}^{j-1}g_i\\) (\\(G_0=1\\)).\n+\n+One drawback of this method is that whenever the output dimensions are slightly\n+less than integer multiples of the input dimensions, many of the tensor elements\n+are repeated in an inefficient way. This is resolved be specifying that all"
      },
      {
        "body": "create_output_Tensor -> create_output_tensor\r\n\r\nPlease see https://google.github.io/styleguide/cppguide.html#General_Naming_Rules for other naming style guide information.",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  // rasterize the input index\n+  {\n+    Index_t last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      Index_t index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;\n+          if (qi == q) continue;\n+          index += G[qi]*(output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        // index *= Y[r];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor*index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+\n+template<class T, class Vec_t>\n+void main_logic (OpKernelContext *context,\n+                 const Vec_t &desired_shape,\n+                 const Tensor &input_tensor)\n+{\n+  // NOTE input is a strided array (last index is fastest, C-ordered)\n+  auto input = input_tensor.flat<T>();\n+  const int rank = input_tensor.dims();\n+  const auto original_size = input.size();\n+  // NOTE original and target dimensions\n+  vector<int64> X(rank), Y(rank);\n+  int64 total_size = 1, new_sliced_size = 1;\n+  // NOTE factor by which X increases/decreases w.r.t. Y\n+  vector<float> f(rank);\n+  // NOTE helper arrays related to f\n+  vector<int64> g(rank), G(rank);\n+  // NOTE index of adjustable dimension\n+  int q;\n+  TensorShape output_shape;\n+\n+  // NOTE requires that the rank of the input tensor and length of the desired shape are equal\n+  OP_REQUIRES(context, rank == desired_shape.size(),\n+              errors::InvalidArgument(\"periodic_intersperse expects the rank of the input tensor, \",\n+              rank, \", to be the same as the length of the desired shape, \", desired_shape.size(), \".\"));\n+\n+  // NOTE from here on, the logic parallels notebook\n+  {\n+    bool found = false;\n+    for (int i = 0; i < rank; ++i) {\n+      if (desired_shape(i) < 1) {\n+        // NOTE only one index can be adjustable\n+        OP_REQUIRES(context, !found,\n+                    errors::InvalidArgument(\"periodic_intersperse expects only \"\n+                    \"one index to be marked as adjustable.\"));\n+        q = i;\n+        found = true;\n+      }\n+      else {\n+        Y[i] = desired_shape(i);\n+        new_sliced_size *= Y[i];\n+      }\n+    }\n+    // NOTE at least one index needs to be adjustable\n+    OP_REQUIRES(context, found,\n+                errors::InvalidArgument(\"periodic_intersperse expects at least \"\n+                \"one index to be marked as adjustable.\"));\n+\n+    int count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+      X[count] = dim_info.size;\n+      total_size *= X[count];\n+      ++count;\n+    }\n+\n+    Y[q] = int64(std::floor(float(total_size)/float(new_sliced_size)));\n+\n+    count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+        f[count] = float(Y[count])/float(X[count]);\n+        g[count] = int64(std::ceil(f[count]));\n+        if (count == 0) G[count] = 1;\n+        else G[count] = G[count - 1]*g[count - 1];\n+      ++count;\n+    }\n+  }\n+\n+  // NOTE ensure that the new dimension is greater than zero\n+  OP_REQUIRES(context, Y[q] > 0,\n+              errors::InvalidArgument(\"periodic_intersperse found that the \"\n+              \"adjustable dimension, \", q, \", isn't greater than zero, \", Y[q],\n+              \".\"));\n+  for (int i = 0; i < rank; ++i) {\n+    output_shape.AddDim(Y[i]);\n+  }\n+  const auto new_size = new_sliced_size*Y[q];\n+\n+  // Create an output tensor and attach it to the current context\n+  Tensor* output_tensor = NULL;\n+  OP_REQUIRES_OK(context, context->allocate_output(0, output_shape,\n+    &output_tensor));\n+  auto output = output_tensor->flat<T>();\n+\n+  {\n+    // NOTE memory is allocated for these variables outside the inner loop for\n+    //      efficiency (yes, I know I could create a separate class scope for\n+    //      this purpose instead)\n+    typename decay<decltype(new_size)>::type result = 0;\n+    vector<decltype(result)> output_indices(Y.size());\n+    const auto rank = Y.size();\n+\n+    // Fill output tensor with shuffled input tensor values\n+    for (typename decay<decltype(new_size)>::type i = 0; i < new_size; ++i) {\n+      output(i) = input(compute_input_index(Y, i, X, q, g, G,\n+                                            result, output_indices, rank));\n+    }\n+  }\n+}\n+\n+\n+template<class T>\n+void create_output_Tensor (OpKernelContext *context,"
      },
      {
        "body": "What is notebook?",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  // rasterize the input index\n+  {\n+    Index_t last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      Index_t index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;\n+          if (qi == q) continue;\n+          index += G[qi]*(output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        // index *= Y[r];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor*index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+\n+template<class T, class Vec_t>\n+void main_logic (OpKernelContext *context,\n+                 const Vec_t &desired_shape,\n+                 const Tensor &input_tensor)\n+{\n+  // NOTE input is a strided array (last index is fastest, C-ordered)\n+  auto input = input_tensor.flat<T>();\n+  const int rank = input_tensor.dims();\n+  const auto original_size = input.size();\n+  // NOTE original and target dimensions\n+  vector<int64> X(rank), Y(rank);\n+  int64 total_size = 1, new_sliced_size = 1;\n+  // NOTE factor by which X increases/decreases w.r.t. Y\n+  vector<float> f(rank);\n+  // NOTE helper arrays related to f\n+  vector<int64> g(rank), G(rank);\n+  // NOTE index of adjustable dimension\n+  int q;\n+  TensorShape output_shape;\n+\n+  // NOTE requires that the rank of the input tensor and length of the desired shape are equal\n+  OP_REQUIRES(context, rank == desired_shape.size(),\n+              errors::InvalidArgument(\"periodic_intersperse expects the rank of the input tensor, \",\n+              rank, \", to be the same as the length of the desired shape, \", desired_shape.size(), \".\"));\n+\n+  // NOTE from here on, the logic parallels notebook"
      },
      {
        "body": "Please change the conditionals to either\r\n\r\n```if (foo) bar;```\r\n\r\nor \r\n\r\n```\r\nif (foo) {\r\n  bar;\r\n}\r\n```\r\nhttps://google.github.io/styleguide/cppguide.html#Conditionals\r\nAdditionally please use else ifs in this clause. ",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  // rasterize the input index\n+  {\n+    Index_t last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      Index_t index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;\n+          if (qi == q) continue;\n+          index += G[qi]*(output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        // index *= Y[r];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor*index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+\n+template<class T, class Vec_t>\n+void main_logic (OpKernelContext *context,\n+                 const Vec_t &desired_shape,\n+                 const Tensor &input_tensor)\n+{\n+  // NOTE input is a strided array (last index is fastest, C-ordered)\n+  auto input = input_tensor.flat<T>();\n+  const int rank = input_tensor.dims();\n+  const auto original_size = input.size();\n+  // NOTE original and target dimensions\n+  vector<int64> X(rank), Y(rank);\n+  int64 total_size = 1, new_sliced_size = 1;\n+  // NOTE factor by which X increases/decreases w.r.t. Y\n+  vector<float> f(rank);\n+  // NOTE helper arrays related to f\n+  vector<int64> g(rank), G(rank);\n+  // NOTE index of adjustable dimension\n+  int q;\n+  TensorShape output_shape;\n+\n+  // NOTE requires that the rank of the input tensor and length of the desired shape are equal\n+  OP_REQUIRES(context, rank == desired_shape.size(),\n+              errors::InvalidArgument(\"periodic_intersperse expects the rank of the input tensor, \",\n+              rank, \", to be the same as the length of the desired shape, \", desired_shape.size(), \".\"));\n+\n+  // NOTE from here on, the logic parallels notebook\n+  {\n+    bool found = false;\n+    for (int i = 0; i < rank; ++i) {\n+      if (desired_shape(i) < 1) {\n+        // NOTE only one index can be adjustable\n+        OP_REQUIRES(context, !found,\n+                    errors::InvalidArgument(\"periodic_intersperse expects only \"\n+                    \"one index to be marked as adjustable.\"));\n+        q = i;\n+        found = true;\n+      }\n+      else {\n+        Y[i] = desired_shape(i);\n+        new_sliced_size *= Y[i];\n+      }\n+    }\n+    // NOTE at least one index needs to be adjustable\n+    OP_REQUIRES(context, found,\n+                errors::InvalidArgument(\"periodic_intersperse expects at least \"\n+                \"one index to be marked as adjustable.\"));\n+\n+    int count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+      X[count] = dim_info.size;\n+      total_size *= X[count];\n+      ++count;\n+    }\n+\n+    Y[q] = int64(std::floor(float(total_size)/float(new_sliced_size)));\n+\n+    count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+        f[count] = float(Y[count])/float(X[count]);\n+        g[count] = int64(std::ceil(f[count]));\n+        if (count == 0) G[count] = 1;\n+        else G[count] = G[count - 1]*g[count - 1];\n+      ++count;\n+    }\n+  }\n+\n+  // NOTE ensure that the new dimension is greater than zero\n+  OP_REQUIRES(context, Y[q] > 0,\n+              errors::InvalidArgument(\"periodic_intersperse found that the \"\n+              \"adjustable dimension, \", q, \", isn't greater than zero, \", Y[q],\n+              \".\"));\n+  for (int i = 0; i < rank; ++i) {\n+    output_shape.AddDim(Y[i]);\n+  }\n+  const auto new_size = new_sliced_size*Y[q];\n+\n+  // Create an output tensor and attach it to the current context\n+  Tensor* output_tensor = NULL;\n+  OP_REQUIRES_OK(context, context->allocate_output(0, output_shape,\n+    &output_tensor));\n+  auto output = output_tensor->flat<T>();\n+\n+  {\n+    // NOTE memory is allocated for these variables outside the inner loop for\n+    //      efficiency (yes, I know I could create a separate class scope for\n+    //      this purpose instead)\n+    typename decay<decltype(new_size)>::type result = 0;\n+    vector<decltype(result)> output_indices(Y.size());\n+    const auto rank = Y.size();\n+\n+    // Fill output tensor with shuffled input tensor values\n+    for (typename decay<decltype(new_size)>::type i = 0; i < new_size; ++i) {\n+      output(i) = input(compute_input_index(Y, i, X, q, g, G,\n+                                            result, output_indices, rank));\n+    }\n+  }\n+}\n+\n+\n+template<class T>\n+void create_output_Tensor (OpKernelContext *context,\n+                           const Tensor &input_tensor,\n+                           const DataType &input_tensor_type,\n+                           const Tensor &desired_shape_tensor)\n+{\n+  auto desired_shape = desired_shape_tensor.flat<T>();\n+\n+  // obligatory type switch\n+  if (input_tensor_type == DataTypeToEnum<float>::value)\n+    main_logic<float>(context, desired_shape, input_tensor);"
      },
      {
        "body": "Same here please.",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  // rasterize the input index\n+  {\n+    Index_t last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      Index_t index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;\n+          if (qi == q) continue;\n+          index += G[qi]*(output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        // index *= Y[r];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor*index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+\n+template<class T, class Vec_t>\n+void main_logic (OpKernelContext *context,\n+                 const Vec_t &desired_shape,\n+                 const Tensor &input_tensor)\n+{\n+  // NOTE input is a strided array (last index is fastest, C-ordered)\n+  auto input = input_tensor.flat<T>();\n+  const int rank = input_tensor.dims();\n+  const auto original_size = input.size();\n+  // NOTE original and target dimensions\n+  vector<int64> X(rank), Y(rank);\n+  int64 total_size = 1, new_sliced_size = 1;\n+  // NOTE factor by which X increases/decreases w.r.t. Y\n+  vector<float> f(rank);\n+  // NOTE helper arrays related to f\n+  vector<int64> g(rank), G(rank);\n+  // NOTE index of adjustable dimension\n+  int q;\n+  TensorShape output_shape;\n+\n+  // NOTE requires that the rank of the input tensor and length of the desired shape are equal\n+  OP_REQUIRES(context, rank == desired_shape.size(),\n+              errors::InvalidArgument(\"periodic_intersperse expects the rank of the input tensor, \",\n+              rank, \", to be the same as the length of the desired shape, \", desired_shape.size(), \".\"));\n+\n+  // NOTE from here on, the logic parallels notebook\n+  {\n+    bool found = false;\n+    for (int i = 0; i < rank; ++i) {\n+      if (desired_shape(i) < 1) {\n+        // NOTE only one index can be adjustable\n+        OP_REQUIRES(context, !found,\n+                    errors::InvalidArgument(\"periodic_intersperse expects only \"\n+                    \"one index to be marked as adjustable.\"));\n+        q = i;\n+        found = true;\n+      }\n+      else {\n+        Y[i] = desired_shape(i);\n+        new_sliced_size *= Y[i];\n+      }\n+    }\n+    // NOTE at least one index needs to be adjustable\n+    OP_REQUIRES(context, found,\n+                errors::InvalidArgument(\"periodic_intersperse expects at least \"\n+                \"one index to be marked as adjustable.\"));\n+\n+    int count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+      X[count] = dim_info.size;\n+      total_size *= X[count];\n+      ++count;\n+    }\n+\n+    Y[q] = int64(std::floor(float(total_size)/float(new_sliced_size)));\n+\n+    count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+        f[count] = float(Y[count])/float(X[count]);\n+        g[count] = int64(std::ceil(f[count]));\n+        if (count == 0) G[count] = 1;\n+        else G[count] = G[count - 1]*g[count - 1];\n+      ++count;\n+    }\n+  }\n+\n+  // NOTE ensure that the new dimension is greater than zero\n+  OP_REQUIRES(context, Y[q] > 0,\n+              errors::InvalidArgument(\"periodic_intersperse found that the \"\n+              \"adjustable dimension, \", q, \", isn't greater than zero, \", Y[q],\n+              \".\"));\n+  for (int i = 0; i < rank; ++i) {\n+    output_shape.AddDim(Y[i]);\n+  }\n+  const auto new_size = new_sliced_size*Y[q];\n+\n+  // Create an output tensor and attach it to the current context\n+  Tensor* output_tensor = NULL;\n+  OP_REQUIRES_OK(context, context->allocate_output(0, output_shape,\n+    &output_tensor));\n+  auto output = output_tensor->flat<T>();\n+\n+  {\n+    // NOTE memory is allocated for these variables outside the inner loop for\n+    //      efficiency (yes, I know I could create a separate class scope for\n+    //      this purpose instead)\n+    typename decay<decltype(new_size)>::type result = 0;\n+    vector<decltype(result)> output_indices(Y.size());\n+    const auto rank = Y.size();\n+\n+    // Fill output tensor with shuffled input tensor values\n+    for (typename decay<decltype(new_size)>::type i = 0; i < new_size; ++i) {\n+      output(i) = input(compute_input_index(Y, i, X, q, g, G,\n+                                            result, output_indices, rank));\n+    }\n+  }\n+}\n+\n+\n+template<class T>\n+void create_output_Tensor (OpKernelContext *context,\n+                           const Tensor &input_tensor,\n+                           const DataType &input_tensor_type,\n+                           const Tensor &desired_shape_tensor)\n+{\n+  auto desired_shape = desired_shape_tensor.flat<T>();\n+\n+  // obligatory type switch\n+  if (input_tensor_type == DataTypeToEnum<float>::value)\n+    main_logic<float>(context, desired_shape, input_tensor);\n+  if (input_tensor_type == DataTypeToEnum<double>::value)\n+    main_logic<double>(context, desired_shape, input_tensor);\n+  if (input_tensor_type == DataTypeToEnum<int32>::value)\n+    main_logic<int32>(context, desired_shape, input_tensor);\n+  if (input_tensor_type == DataTypeToEnum<int64>::value)\n+    main_logic<int64>(context, desired_shape, input_tensor);\n+}\n+\n+\n+class PeriodicIntersperseOp : public OpKernel {\n+public:\n+  explicit PeriodicIntersperseOp (OpKernelConstruction* context) : OpKernel(context) {}\n+\n+  void Compute (OpKernelContext* context) override {\n+\n+    // Grab the input tensor and desired shape\n+    const Tensor &input_tensor = context->input(0);\n+    const DataType input_tensor_type = context->input_dtype(0);\n+    const Tensor &desired_shape_tensor = context->input(1);\n+    const DataType desired_shape_tensor_type = context->input_dtype(1);\n+\n+    // NOTE requires that the desired shape is a vector\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(desired_shape_tensor.shape()),\n+        errors::InvalidArgument(\n+          \"periodic_intersperse expects a 1D vector for the desired shape.\"));\n+\n+    // obligatory type switch\n+    if (desired_shape_tensor_type == DataTypeToEnum<int32>::value)"
      },
      {
        "body": "Remove this comment?",
        "diff_hunk": "@@ -0,0 +1,125 @@\n+# Description:"
      },
      {
        "body": "Template type names should be CamelCase: https://google.github.io/styleguide/cppguide.html#Type_Names",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>"
      },
      {
        "body": "Please use nullptr instead of NULL",
        "diff_hunk": "@@ -0,0 +1,216 @@\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+using namespace tensorflow;\n+using std::vector;\n+using std::decay;\n+\n+\n+template<class IndexVec_t, class Index_t>\n+Index_t compute_input_index(IndexVec_t &Y, const Index_t &i,\n+                            const IndexVec_t &X, const int &q,\n+                            const vector<int64> &g, const vector<int64> &G,\n+                            Index_t &result, vector<Index_t> &output_indices,\n+                            const typename decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    for (auto r = rank - 1;; --r) {\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  // rasterize the input index\n+  {\n+    Index_t last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      Index_t index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;\n+          if (qi == q) continue;\n+          index += G[qi]*(output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        // index *= Y[r];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor*index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+\n+template<class T, class Vec_t>\n+void main_logic (OpKernelContext *context,\n+                 const Vec_t &desired_shape,\n+                 const Tensor &input_tensor)\n+{\n+  // NOTE input is a strided array (last index is fastest, C-ordered)\n+  auto input = input_tensor.flat<T>();\n+  const int rank = input_tensor.dims();\n+  const auto original_size = input.size();\n+  // NOTE original and target dimensions\n+  vector<int64> X(rank), Y(rank);\n+  int64 total_size = 1, new_sliced_size = 1;\n+  // NOTE factor by which X increases/decreases w.r.t. Y\n+  vector<float> f(rank);\n+  // NOTE helper arrays related to f\n+  vector<int64> g(rank), G(rank);\n+  // NOTE index of adjustable dimension\n+  int q;\n+  TensorShape output_shape;\n+\n+  // NOTE requires that the rank of the input tensor and length of the desired shape are equal\n+  OP_REQUIRES(context, rank == desired_shape.size(),\n+              errors::InvalidArgument(\"periodic_intersperse expects the rank of the input tensor, \",\n+              rank, \", to be the same as the length of the desired shape, \", desired_shape.size(), \".\"));\n+\n+  // NOTE from here on, the logic parallels notebook\n+  {\n+    bool found = false;\n+    for (int i = 0; i < rank; ++i) {\n+      if (desired_shape(i) < 1) {\n+        // NOTE only one index can be adjustable\n+        OP_REQUIRES(context, !found,\n+                    errors::InvalidArgument(\"periodic_intersperse expects only \"\n+                    \"one index to be marked as adjustable.\"));\n+        q = i;\n+        found = true;\n+      }\n+      else {\n+        Y[i] = desired_shape(i);\n+        new_sliced_size *= Y[i];\n+      }\n+    }\n+    // NOTE at least one index needs to be adjustable\n+    OP_REQUIRES(context, found,\n+                errors::InvalidArgument(\"periodic_intersperse expects at least \"\n+                \"one index to be marked as adjustable.\"));\n+\n+    int count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+      X[count] = dim_info.size;\n+      total_size *= X[count];\n+      ++count;\n+    }\n+\n+    Y[q] = int64(std::floor(float(total_size)/float(new_sliced_size)));\n+\n+    count = 0;\n+    for (const auto dim_info : input_tensor.shape()) {\n+        f[count] = float(Y[count])/float(X[count]);\n+        g[count] = int64(std::ceil(f[count]));\n+        if (count == 0) G[count] = 1;\n+        else G[count] = G[count - 1]*g[count - 1];\n+      ++count;\n+    }\n+  }\n+\n+  // NOTE ensure that the new dimension is greater than zero\n+  OP_REQUIRES(context, Y[q] > 0,\n+              errors::InvalidArgument(\"periodic_intersperse found that the \"\n+              \"adjustable dimension, \", q, \", isn't greater than zero, \", Y[q],\n+              \".\"));\n+  for (int i = 0; i < rank; ++i) {\n+    output_shape.AddDim(Y[i]);\n+  }\n+  const auto new_size = new_sliced_size*Y[q];\n+\n+  // Create an output tensor and attach it to the current context\n+  Tensor* output_tensor = NULL;"
      },
      {
        "body": "Periodicly -> Periodically",
        "diff_hunk": "@@ -11,7 +27,7 @@ REGISTER_OP(\"PeriodicIntersperse\")\n     .Input(\"desired_shape: S\")\n     .Output(\"output: T\")\n     .Doc(R\"doc(\n-Periodicly intersperses elements of a tensor to conform to `desired shape`.\n+Periodicly intersperses elements of a tensor to conform to `desired_shape`."
      },
      {
        "body": "Please remove these commented out lines of code here and below.",
        "diff_hunk": "@@ -0,0 +1,236 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+\n+template<class IndexVecT, class IndexT>\n+IndexT compute_input_index(IndexVecT &Y, const IndexT &i,\n+                           const IndexVecT &X, const int &q,\n+                           const std::vector<tensorflow::int64> &g,\n+                           const std::vector<tensorflow::int64> &G,\n+                           IndexT &result,\n+                           std::vector<IndexT> &output_indices,\n+                           const typename std::decay<decltype(Y.size())>::type &rank)\n+{\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    auto r = rank;\n+    do {\n+      --r;\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+    } while (r > 0);\n+  }\n+\n+  // rasterize the input index\n+  {\n+    IndexT last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      IndexT index = 0;\n+      if (r != q)\n+        index = output_indices[r]/g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          // if (qi == r) continue;"
      },
      {
        "body": "I still don't know about whether we support MathJax. Looking at the math ops in this [file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc), it looks like the format for math descriptions is a bit different. Could you take a look at those and change these comments style to match?",
        "diff_hunk": "@@ -0,0 +1,87 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+using namespace tensorflow;\n+\n+REGISTER_OP(\"PeriodicIntersperse\")\n+    .Attr(\"T: numbertype\")\n+    .Attr(\"S: {int32, int64}\")\n+    .Input(\"values: T\")\n+    .Input(\"desired_shape: S\")\n+    .Output(\"output: T\")\n+    .Doc(R\"doc(\n+Periodicly intersperses elements of a tensor to conform to `desired_shape`.\n+\n+This function implements a slightly more generic version of the subpixel\n+convolutions found in this [paper](https://arxiv.org/abs/1609.05158).\n+\n+The formula for computing the elements in the `output` tensor is as follows:\n+  `T` = `values` tensor of rank `R`\n+  `S` = `desired_shape` tensor (vector of length `R`)\n+  `P` = `output` tensor of rank `R`\n+  \\((T_1,\\ldots,T_R)\\) = shape(`T`)\n+  \\([S_1,\\ldots,S_q,\\ldots,S_R]\\) = elements of vector `S`\n+\n+  A single element in `S` is left unspecified (denoted \\(S_q=-1\\)).\n+  Let \\(f_i\\) denote the (possibly non-integer) factor that relates the original\n+  dimension to the desired dimensions, \\(S_i=f_i T_i\\), for \\(i\\neq q\\) where\n+  \\(f_i>0\\).\n+  Define the following:\n+    \\(g_i=\\lceil f_i\\rceil\\)"
      },
      {
        "body": "Why no .TypeConstraint for this one?\r\nAnd why is output specified as HostMemory instead of desired_shape?",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/contrib/periodic_intersperse/core/kernels/periodic_intersperse_op.h\"\n+\n+namespace tensorflow {\n+\n+REGISTER_KERNEL_BUILDER(Name(\"PeriodicIntersperse\")\n+                            .HostMemory(\"desired_shape\")\n+                            .Device(DEVICE_CPU),\n+                        PeriodicIntersperseOp);\n+\n+#define REGISTER_GPU_KERNEL(type)                          \\\n+  REGISTER_KERNEL_BUILDER(Name(\"PeriodicIntersperse\")      \\\n+                              .Device(DEVICE_GPU)          \\\n+                              .HostMemory(\"desired_shape\") \\\n+                              .TypeConstraint<type>(\"T\"),  \\\n+                          PeriodicIntersperseOp);\n+TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU_KERNEL);\n+#undef REGISTER_GPU_KERNEL\n+\n+#ifdef TENSORFLOW_USE_SYCL\n+REGISTER_KERNEL_BUILDER(Name(\"PeriodicIntersperse\")\n+                            .Device(DEVICE_SYCL)\n+                            .HostMemory(\"output\"),"
      },
      {
        "body": "Please eschew single letter variable names. See https://google.github.io/styleguide/cppguide.html#Naming",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,\n+    const std::vector<tensorflow::int64>& g,\n+    const std::vector<tensorflow::int64>& G, IndexT& result,\n+    std::vector<IndexT>& output_indices,\n+    const typename std::decay<decltype(Y.size())>::type& rank) {\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    auto r = rank;\n+    do {\n+      --r;\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+    } while (r > 0);\n+  }\n+\n+  // rasterize the input index\n+  {\n+    IndexT last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      IndexT index = 0;\n+      if (r != q)\n+        index = output_indices[r] / g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          if (qi == q) continue;\n+          index += G[qi] * (output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor * index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+template <class T, class VecT>\n+void main_logic(tensorflow::OpKernelContext* context, const VecT& desired_shape,\n+                const tensorflow::Tensor& input_tensor) {\n+  // input is a strided array (last index is fastest, C-ordered)\n+  auto input = input_tensor.flat<T>();\n+  const int rank = input_tensor.dims();\n+  const auto original_size = input.size();\n+  // original and target dimensions\n+  std::vector<tensorflow::int64> X(rank), Y(rank);\n+  tensorflow::int64 total_size = 1, new_sliced_size = 1;\n+  // factor by which X increases/decreases w.r.t. Y\n+  std::vector<float> f(rank);"
      },
      {
        "body": "main_logic is not descriptive. Please rename this to reflect what the function does.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,\n+    const std::vector<tensorflow::int64>& g,\n+    const std::vector<tensorflow::int64>& G, IndexT& result,\n+    std::vector<IndexT>& output_indices,\n+    const typename std::decay<decltype(Y.size())>::type& rank) {\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    auto r = rank;\n+    do {\n+      --r;\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+    } while (r > 0);\n+  }\n+\n+  // rasterize the input index\n+  {\n+    IndexT last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {\n+      IndexT index = 0;\n+      if (r != q)\n+        index = output_indices[r] / g[r];\n+      else {\n+        for (int qi = 0; qi < rank; ++qi) {\n+          if (qi == q) continue;\n+          index += G[qi] * (output_indices[qi] % g[qi]);\n+        }\n+        index *= Y[q];\n+        index += output_indices[r];\n+      }\n+      result += last_index_factor * index;\n+      last_index_factor *= X[r];\n+      if (r == 0) break;\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+template <class T, class VecT>\n+void main_logic(tensorflow::OpKernelContext* context, const VecT& desired_shape,"
      },
      {
        "body": "I think this should be in anonymous namespace.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>"
      },
      {
        "body": "Don't use non-const references. Use pointers for output arguments instead.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,\n+    const std::vector<tensorflow::int64>& g,\n+    const std::vector<tensorflow::int64>& G, IndexT& result,"
      },
      {
        "body": "If IndexVecT is a function of IndexT, perhaps you should only use a single template parameter.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>"
      },
      {
        "body": "Please eschew single letter variable names. See https://google.github.io/styleguide/cppguide.html#Naming\r\n\r\nSame everywhere.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,"
      },
      {
        "body": "No need for the extra scope here.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,\n+    const std::vector<tensorflow::int64>& g,\n+    const std::vector<tensorflow::int64>& G, IndexT& result,\n+    std::vector<IndexT>& output_indices,\n+    const typename std::decay<decltype(Y.size())>::type& rank) {\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {"
      },
      {
        "body": "no need for extra scope",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,\n+    const std::vector<tensorflow::int64>& g,\n+    const std::vector<tensorflow::int64>& G, IndexT& result,\n+    std::vector<IndexT>& output_indices,\n+    const typename std::decay<decltype(Y.size())>::type& rank) {\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    auto r = rank;\n+    do {\n+      --r;\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+    } while (r > 0);\n+  }\n+\n+  // rasterize the input index\n+  {"
      },
      {
        "body": "Why not \r\n\r\nfor (auto r = rank - 1; r >= 0; --r) {\r\n...\r\n}\r\n\r\ninstead of putting a break at the end of the loop.",
        "diff_hunk": "@@ -0,0 +1,231 @@\n+// =============================================================================\n+// Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+// =============================================================================\n+\n+#ifndef TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+#define TENSORFLOW_KERNELS_PERIODICINTERSPERSE_OP_H_\n+\n+#include <cmath>\n+#include <type_traits>\n+#include <vector>\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+\n+template <class IndexVecT, class IndexT>\n+IndexT compute_input_index(\n+    IndexVecT& Y, const IndexT& i, const IndexVecT& X, const int& q,\n+    const std::vector<tensorflow::int64>& g,\n+    const std::vector<tensorflow::int64>& G, IndexT& result,\n+    std::vector<IndexT>& output_indices,\n+    const typename std::decay<decltype(Y.size())>::type& rank) {\n+  result = 0;\n+  output_indices.clear();\n+\n+  // un-rasterize the output index\n+  {\n+    auto last_reduced_i = i;\n+    auto r = rank;\n+    do {\n+      --r;\n+      output_indices[r] = last_reduced_i % Y[r];\n+      last_reduced_i = (last_reduced_i - output_indices[r]) / Y[r];\n+    } while (r > 0);\n+  }\n+\n+  // rasterize the input index\n+  {\n+    IndexT last_index_factor = 1;\n+    for (auto r = rank - 1;; --r) {"
      }
    ],
    "body": "This implements sub-pixel shuffling in the module `contrib`. https://github.com/tensorflow/tensorflow/issues/9369",
    "timestamp": "2025-05-06 01:27:27"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/27454",
    "comments": [
      "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->",
      "CLA Agree",
      "CLA Agree",
      "@byronyi Please sign CLA in order to proceed with this PR. Thank you.",
      "@gbaned Look closer; I did signed CLA and the commit author did, too. You need to manually confirm the CLA status though, as we have coauthors for this commit who commented above.",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->",
      "CLA Agree",
      "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->",
      "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->",
      "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->",
      "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->",
      "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->",
      "@byronyi please resolve conflicts",
      "Let me know if this change is ready to take a look again.",
      "> @byronyi please resolve conflicts\r\n\r\nconflicts fixed.",
      "grpc+seastar protocol can not be identified.\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf.py\", line 55, in <module>\r\n    server = tf.train.Server(cluster, protocol=\"grpc+seastar\", job_name=job_name, task_index=task_index, config=config)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 149, in __init__\r\n    self._server = c_api.TF_NewServer(self._server_def.SerializeToString())\r\ntensorflow.python.framework.errors_impl.NotFoundError: No server factory registered for the given ServerDef: cluster {\r\n  job {\r\n    name: \"ps\"\r\n    tasks {\r\n      key: 0\r\n      value: \"node1:31152\"\r\n    }\r\n    tasks {\r\n      key: 1\r\n      value: \"node1:26427\"\r\n    }\r\n  }\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"node2:8122\"\r\n    }\r\n    tasks {\r\n      key: 1\r\n      value: \"node2:27213\"\r\n    }\r\n  }\r\n}\r\njob_name: \"worker\"\r\ndefault_session_config {\r\n  intra_op_parallelism_threads: 40\r\n  inter_op_parallelism_threads: 40\r\n}\r\nprotocol: \"grpc+seastar\"\r\n\r\nThe available server factories are: [ GRPC_SERVER ]\r\n```",
      "> grpc+seastar protocol can not be identified.\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"tf.py\", line 55, in <module>\r\n>     server = tf.train.Server(cluster, protocol=\"grpc+seastar\", job_name=job_name, task_index=task_index, config=config)\r\n>   File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 149, in __init__\r\n>     self._server = c_api.TF_NewServer(self._server_def.SerializeToString())\r\n> tensorflow.python.framework.errors_impl.NotFoundError: No server factory registered for the given ServerDef: cluster {\r\n>   job {\r\n>     name: \"ps\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"node1:31152\"\r\n>     }\r\n>     tasks {\r\n>       key: 1\r\n>       value: \"node1:26427\"\r\n>     }\r\n>   }\r\n>   job {\r\n>     name: \"worker\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"node2:8122\"\r\n>     }\r\n>     tasks {\r\n>       key: 1\r\n>       value: \"node2:27213\"\r\n>     }\r\n>   }\r\n> }\r\n> job_name: \"worker\"\r\n> default_session_config {\r\n>   intra_op_parallelism_threads: 40\r\n>   inter_op_parallelism_threads: 40\r\n> }\r\n> protocol: \"grpc+seastar\"\r\n> \r\n> The available server factories are: [ GRPC_SERVER ]\r\n> ```\r\n\r\nPlease manually add  \"build:seastar --define with_seastar_support=true\" in .tf_configure.bazelrc then build command \"bazel build --config=opt --config=seastar //tensorflow/tools/pip_package:build_pip_package\", then you could succefully build up tensorflow with seastar. \r\nThere's left conflicts are that seastar and tensorflow both use hwloc which conflict with each other.",
      "@gunan added `@hwloc` third-party package and @liutongxuan you could seek help from him.",
      "Load endpoint map file failed.\r\n```\r\n    std::ifstream fin(kEndpointMapFile, std::ios::in);\r\n    if (!fin.good()) {\r\n      LOG(FATAL) << \"Load endpoint map file failed.\";\r\n    }\r\n```\r\nShould we add .endpoint_map file manually ?",
      "> Our code hangs using this patch. Need we modify some configuration ? @liutongxua\r\n\r\n> Load endpoint map file failed.\r\n> \r\n> ```\r\n>     std::ifstream fin(kEndpointMapFile, std::ios::in);\r\n>     if (!fin.good()) {\r\n>       LOG(FATAL) << \"Load endpoint map file failed.\";\r\n>     }\r\n> ```\r\n> Should we add .endpoint_map file manually ?\r\n\r\nI'll refactor the code by adding a new service API to pass Seastar's ports later. Follow the similar solution as rdma's ports. \r\n\".endpoint_map\" format like follow:\r\n42353, 42354 is grpc's ports, 46068, 47079 is related seastar's ports.\r\n```\r\n127.0.0.1:42353=127.0.0.1:46068\r\n127.0.0.1:42354=127.0.0.1:47079\r\n```",
      "> On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> \r\n> Then another error we got :\r\n> \r\n> ```\r\n> 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> terminate called after throwing an instance of 'std::runtime_error'\r\n>   what():  insufficient processing units\r\n> Aborted (core dumped)\r\n> ```\r\n> In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> \r\n> ```\r\n> #include <hwloc.h>\r\n> int main(int argc,char **argv)\r\n> {\r\n>     int nPhysicalProcessorCount = 0;\r\n>     hwloc_topology_t topology;\r\n>     hwloc_topology_init(&topology);\r\n>     hwloc_topology_load(topology);\r\n>     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n>     printf(\"%d\", available_procs); // 28\r\n> }\r\n> ```\r\n> Our machine contains 28 cores.\r\n> \r\n> After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> \r\n> What's more, why core_number_ is equals to server_number?\r\n> `core_number_ = GetCoreNumber(server_number);`\r\n> \r\n> @liutongxuan\r\n\r\nSeastar's hwloc is conflict with TensorFlow's.  I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n\r\nPlease pull latest code. I tested several cases passed. If any questions let me know.\r\n`core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n",
      "> @gunan added `@hwloc` third-party package and @liutongxuan you could seek help from him.\r\nthx, @gunan @byronyi ",
      "> Let me know if this change is ready to take a look again.\r\n\r\nCurrently code could run now. I'm not sure the design of endpoint_map in seastar_server_lib.cc is ok or not for you guys?  User have to configure a .endpoint_map to record seastar's ports. @byronyi @poxvoculi @annarev",
      "> > Let me know if this change is ready to take a look again.\r\n> \r\n> Currently code could run now. I'm not sure the design of endpoint_map in seastar_server_lib.cc is ok or not for you guys? User have to configure a .endpoint_map to record seastar's ports. @byronyi @poxvoculi @annarev\r\n\r\nThere seems to be a file conflict in `tensorflow/python/training/server_lib.py`. ",
      "> > > Let me know if this change is ready to take a look again.\r\n> > \r\n> > \r\n> > Currently code could run now. I'm not sure the design of endpoint_map in seastar_server_lib.cc is ok or not for you guys? User have to configure a .endpoint_map to record seastar's ports. @byronyi @poxvoculi @annarev\r\n> \r\n> There seems to be a file conflict in `tensorflow/python/training/server_lib.py`.\r\n\r\nfixed, thx @byronyi ",
      "@liutongxuan Thanks! Have a wonderful Sunday night :)",
      "Can you make sure \"/root/.cache/bazel/_bazel_root/***/external/seastar/core/channel.hh\",   \r\nIf the is_init is not defined like follows, please cleanup the bazel cache then build again.\r\n`const std::atomic_bool& is_init();\r\n`\r\nAnother concern is the port which you use, any other process use the same port?",
      "is_init is defined like this 'bool is_init();', we will try to clean and build again. thx!\r\nWhat's more, should we modify the channel.hh in seastar ? \r\nAfter rebuilding, is_init is still defined as 'bool is_init()' .\r\n\r\n\r\n@liutongxuan "
    ],
    "review_comments": [
      {
        "body": "I cannot find this file in open source TF.",
        "diff_hunk": "@@ -0,0 +1,289 @@\n+#include \"tensorflow/contrib/seastar/seastar_channel_cache.h\"\n+\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/lib/gtl/map_util.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/util/device_name_utils.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#include \"tensorflow/core/platform/thread_annotations.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/distributed_runtime/rpc/dynamic_rpc_address_resolver.h\""
      },
      {
        "body": "The following files cannot be found.",
        "diff_hunk": "@@ -0,0 +1,170 @@\n+#include \"tensorflow/contrib/seastar/seastar_client.h\"\n+#include \"core/sleep.hh\""
      },
      {
        "body": "Cannot find these headers either.",
        "diff_hunk": "@@ -0,0 +1,59 @@\n+#ifndef TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_CLIENT_H_\n+#define TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_CLIENT_H_\n+\n+#include \"core/future-util.hh\""
      },
      {
        "body": "Is this a magic debug value in the header? ``DEADBEEF`` or ``CAFEBABE`` could be a better choice.",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);"
      },
      {
        "body": "Better use a constant for 32.",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;"
      },
      {
        "body": "Return ``Status::OK()``?",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  // internal error, that we dont need to parse the response,\n+                  // reponse is nullptr\n+                  if (s.code() != error::INTERNAL) {\n+                    response->ParseFromArray(tag->resp_body_buf_.data_,\n+                                            tag->resp_body_buf_.len_);\n+                  }\n+                  if (!s.ok()) {\n+                    if (tag->method_ == SeastarWorkerServiceMethod::kLogging ||\n+                        tag->method_ == SeastarWorkerServiceMethod::kTracing) {\n+                      // Logging & Tracing in worker.cc is UNIMPLEMENTED, ignore the error\n+                    } else {\n+                      // Debugging info\n+                      LOG(INFO) << \"RPC's status is not ok. status code=\" << s.code()\n+                                << \", err msg=\" << s.error_message().c_str();\n+                    }\n+                  }\n+                  done(s);\n+                  delete tag;\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  // LOG(INFO) << \"InitSeastarClientTag for no fuse tensor recv\";\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_,\n+                            tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDead(sm.is_dead_);\n+      response->SetDataType(sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for no fuse, can memcpy and on cpu\"\n+          //          << \",request:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \",request:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();"
      },
      {
        "body": "Same above.",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  // internal error, that we dont need to parse the response,\n+                  // reponse is nullptr\n+                  if (s.code() != error::INTERNAL) {\n+                    response->ParseFromArray(tag->resp_body_buf_.data_,\n+                                            tag->resp_body_buf_.len_);\n+                  }\n+                  if (!s.ok()) {\n+                    if (tag->method_ == SeastarWorkerServiceMethod::kLogging ||\n+                        tag->method_ == SeastarWorkerServiceMethod::kTracing) {\n+                      // Logging & Tracing in worker.cc is UNIMPLEMENTED, ignore the error\n+                    } else {\n+                      // Debugging info\n+                      LOG(INFO) << \"RPC's status is not ok. status code=\" << s.code()\n+                                << \", err msg=\" << s.error_message().c_str();\n+                    }\n+                  }\n+                  done(s);\n+                  delete tag;\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  // LOG(INFO) << \"InitSeastarClientTag for no fuse tensor recv\";\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_,\n+                            tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDead(sm.is_dead_);\n+      response->SetDataType(sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for no fuse, can memcpy and on cpu\"\n+          //          << \",request:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \",request:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+                  \n+                  bool can_memcpy = DataTypeCanUseMemcpy(response->GetDataType());\n+                  if (can_memcpy) {\n+                    if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+                        (!response->GetOnHost())) {\n+#if GOOGLE_CUDA\n+                      Tensor* gpu_copy = new Tensor(response->GetAlloc(), response->GetTensor().dtype(), response->GetTensor().shape());\n+                      GPUUtil::CopyCPUTensorToGPU(&response->GetTensor(),\n+                                                  response->GetDevice()->tensorflow_gpu_device_info()->default_context,\n+                                                  response->GetDevice(),\n+                                                  gpu_copy,\n+                                                  [gpu_copy, response, done, tag](const Status& s) {\n+                                                    CHECK(s.ok()) << \"copy tensor to gpu sync\";\n+                                                    response->SetTensor(*gpu_copy);\n+                                                    done(s);\n+                                                    delete gpu_copy;\n+                                                    delete tag;\n+                                                  });\n+#else\n+                      done(errors::Internal(\"No GPU device in process\"));\n+                      delete tag;\n+#endif\n+                    } else {\n+                      // LOG(INFO) << \"wrapper_done for no fuse, nothon to do, in the case that tensor on cpu and can memcpy\";\n+                      done(s);\n+                      delete tag;\n+                    }\n+                  } else {\n+                    // could not memcoy\n+                    // LOG(INFO) << \"wrapper_done, could not memcpy, recv bytes: \"\n+                    // << tag->resp_tensor_bufs_[0].len_\n+                    // << \", DataType: \" << response->GetDataType();\n+                    ParseProtoUnlimited(&response->GetTensorProto(),\n+                                        tag->resp_tensor_bufs_[0].data_,\n+                                        tag->resp_tensor_bufs_[0].len_);\n+                    Tensor val;\n+                    Status status = response->GetDevice()->MakeTensorFromProto(\n+                        response->GetTensorProto(),\n+                        response->GetAllocAttributes(),\n+                        &val);\n+                    //LOG(INFO) << \"parse msg status: \" << status.error_message();\n+                    CHECK(status.ok()) << \"make cpu tensor from proto.\";\n+                    response->SetTensor(val);\n+                    done(status);\n+                    delete tag;\n+                  }\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarFuseTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDeadByIndex(idx, sm.is_dead_);\n+      response->SetDataTypeByIndex(idx, sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for fuse, can memcpy and on cpu\"\n+          //          << \"idx is: \" << idx\n+          //          << \", request is:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \", idx is: \" << idx\n+        //          << \", request is:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();"
      },
      {
        "body": "This is confusing. Considering changing it to ``std::atomic<size_t>``.",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  // internal error, that we dont need to parse the response,\n+                  // reponse is nullptr\n+                  if (s.code() != error::INTERNAL) {\n+                    response->ParseFromArray(tag->resp_body_buf_.data_,\n+                                            tag->resp_body_buf_.len_);\n+                  }\n+                  if (!s.ok()) {\n+                    if (tag->method_ == SeastarWorkerServiceMethod::kLogging ||\n+                        tag->method_ == SeastarWorkerServiceMethod::kTracing) {\n+                      // Logging & Tracing in worker.cc is UNIMPLEMENTED, ignore the error\n+                    } else {\n+                      // Debugging info\n+                      LOG(INFO) << \"RPC's status is not ok. status code=\" << s.code()\n+                                << \", err msg=\" << s.error_message().c_str();\n+                    }\n+                  }\n+                  done(s);\n+                  delete tag;\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  // LOG(INFO) << \"InitSeastarClientTag for no fuse tensor recv\";\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_,\n+                            tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDead(sm.is_dead_);\n+      response->SetDataType(sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for no fuse, can memcpy and on cpu\"\n+          //          << \",request:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \",request:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+                  \n+                  bool can_memcpy = DataTypeCanUseMemcpy(response->GetDataType());\n+                  if (can_memcpy) {\n+                    if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+                        (!response->GetOnHost())) {\n+#if GOOGLE_CUDA\n+                      Tensor* gpu_copy = new Tensor(response->GetAlloc(), response->GetTensor().dtype(), response->GetTensor().shape());\n+                      GPUUtil::CopyCPUTensorToGPU(&response->GetTensor(),\n+                                                  response->GetDevice()->tensorflow_gpu_device_info()->default_context,\n+                                                  response->GetDevice(),\n+                                                  gpu_copy,\n+                                                  [gpu_copy, response, done, tag](const Status& s) {\n+                                                    CHECK(s.ok()) << \"copy tensor to gpu sync\";\n+                                                    response->SetTensor(*gpu_copy);\n+                                                    done(s);\n+                                                    delete gpu_copy;\n+                                                    delete tag;\n+                                                  });\n+#else\n+                      done(errors::Internal(\"No GPU device in process\"));\n+                      delete tag;\n+#endif\n+                    } else {\n+                      // LOG(INFO) << \"wrapper_done for no fuse, nothon to do, in the case that tensor on cpu and can memcpy\";\n+                      done(s);\n+                      delete tag;\n+                    }\n+                  } else {\n+                    // could not memcoy\n+                    // LOG(INFO) << \"wrapper_done, could not memcpy, recv bytes: \"\n+                    // << tag->resp_tensor_bufs_[0].len_\n+                    // << \", DataType: \" << response->GetDataType();\n+                    ParseProtoUnlimited(&response->GetTensorProto(),\n+                                        tag->resp_tensor_bufs_[0].data_,\n+                                        tag->resp_tensor_bufs_[0].len_);\n+                    Tensor val;\n+                    Status status = response->GetDevice()->MakeTensorFromProto(\n+                        response->GetTensorProto(),\n+                        response->GetAllocAttributes(),\n+                        &val);\n+                    //LOG(INFO) << \"parse msg status: \" << status.error_message();\n+                    CHECK(status.ok()) << \"make cpu tensor from proto.\";\n+                    response->SetTensor(val);\n+                    done(status);\n+                    delete tag;\n+                  }\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarFuseTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDeadByIndex(idx, sm.is_dead_);\n+      response->SetDataTypeByIndex(idx, sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for fuse, can memcpy and on cpu\"\n+          //          << \"idx is: \" << idx\n+          //          << \", request is:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \", idx is: \" << idx\n+        //          << \", request is:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+\n+                  int fuse_count = tag->fuse_count_;\n+                  int *fuse_counter = new int(fuse_count);"
      },
      {
        "body": "This is not portable for non-GCC compilers. Change it to ``std::atomic`` would be better.",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  // internal error, that we dont need to parse the response,\n+                  // reponse is nullptr\n+                  if (s.code() != error::INTERNAL) {\n+                    response->ParseFromArray(tag->resp_body_buf_.data_,\n+                                            tag->resp_body_buf_.len_);\n+                  }\n+                  if (!s.ok()) {\n+                    if (tag->method_ == SeastarWorkerServiceMethod::kLogging ||\n+                        tag->method_ == SeastarWorkerServiceMethod::kTracing) {\n+                      // Logging & Tracing in worker.cc is UNIMPLEMENTED, ignore the error\n+                    } else {\n+                      // Debugging info\n+                      LOG(INFO) << \"RPC's status is not ok. status code=\" << s.code()\n+                                << \", err msg=\" << s.error_message().c_str();\n+                    }\n+                  }\n+                  done(s);\n+                  delete tag;\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  // LOG(INFO) << \"InitSeastarClientTag for no fuse tensor recv\";\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_,\n+                            tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDead(sm.is_dead_);\n+      response->SetDataType(sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for no fuse, can memcpy and on cpu\"\n+          //          << \",request:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \",request:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+                  \n+                  bool can_memcpy = DataTypeCanUseMemcpy(response->GetDataType());\n+                  if (can_memcpy) {\n+                    if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+                        (!response->GetOnHost())) {\n+#if GOOGLE_CUDA\n+                      Tensor* gpu_copy = new Tensor(response->GetAlloc(), response->GetTensor().dtype(), response->GetTensor().shape());\n+                      GPUUtil::CopyCPUTensorToGPU(&response->GetTensor(),\n+                                                  response->GetDevice()->tensorflow_gpu_device_info()->default_context,\n+                                                  response->GetDevice(),\n+                                                  gpu_copy,\n+                                                  [gpu_copy, response, done, tag](const Status& s) {\n+                                                    CHECK(s.ok()) << \"copy tensor to gpu sync\";\n+                                                    response->SetTensor(*gpu_copy);\n+                                                    done(s);\n+                                                    delete gpu_copy;\n+                                                    delete tag;\n+                                                  });\n+#else\n+                      done(errors::Internal(\"No GPU device in process\"));\n+                      delete tag;\n+#endif\n+                    } else {\n+                      // LOG(INFO) << \"wrapper_done for no fuse, nothon to do, in the case that tensor on cpu and can memcpy\";\n+                      done(s);\n+                      delete tag;\n+                    }\n+                  } else {\n+                    // could not memcoy\n+                    // LOG(INFO) << \"wrapper_done, could not memcpy, recv bytes: \"\n+                    // << tag->resp_tensor_bufs_[0].len_\n+                    // << \", DataType: \" << response->GetDataType();\n+                    ParseProtoUnlimited(&response->GetTensorProto(),\n+                                        tag->resp_tensor_bufs_[0].data_,\n+                                        tag->resp_tensor_bufs_[0].len_);\n+                    Tensor val;\n+                    Status status = response->GetDevice()->MakeTensorFromProto(\n+                        response->GetTensorProto(),\n+                        response->GetAllocAttributes(),\n+                        &val);\n+                    //LOG(INFO) << \"parse msg status: \" << status.error_message();\n+                    CHECK(status.ok()) << \"make cpu tensor from proto.\";\n+                    response->SetTensor(val);\n+                    done(status);\n+                    delete tag;\n+                  }\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarFuseTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDeadByIndex(idx, sm.is_dead_);\n+      response->SetDataTypeByIndex(idx, sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for fuse, can memcpy and on cpu\"\n+          //          << \"idx is: \" << idx\n+          //          << \", request is:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \", idx is: \" << idx\n+        //          << \", request is:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+\n+                  int fuse_count = tag->fuse_count_;\n+                  int *fuse_counter = new int(fuse_count);\n+\n+                  for (int idx = 0; idx < fuse_count; ++idx) {\n+                    bool can_memcpy = DataTypeCanUseMemcpy(response->GetDataTypeByIndex(idx));\n+                    // LOG(INFO) << \"wrapper_done for fuse recv, fuse count is: \" << tag->fuse_count_\n+                    //          <<\", idx is: \" << idx << \", data type is: \" << response->GetDataTypeByIndex(idx);\n+                    if (can_memcpy) {\n+                      if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+                          (!response->GetOnHost())) {\n+#if GOOGLE_CUDA\n+                        Tensor* gpu_copy = new Tensor(response->GetAlloc(),\n+                                                      response->GetTensorByIndex(idx).dtype(),\n+                                                      response->GetTensorByIndex(idx).shape());\n+                        GPUUtil::CopyCPUTensorToGPU(&response->GetTensorByIndex(idx),\n+                                                    response->GetDevice()->tensorflow_gpu_device_info()->default_context,\n+                                                    response->GetDevice(),\n+                                                    gpu_copy,\n+                                                    [gpu_copy, response, done, tag, fuse_counter, idx](const Status& s) {\n+                                                      CHECK(s.ok()) << \"copy tensor to gpu sync\";\n+                                                      response->SetTensorByIndex(idx, *gpu_copy);\n+                                                      delete gpu_copy;\n+                                                      if (__sync_sub_and_fetch(fuse_counter, 1) == 0) {\n+                                                        delete fuse_counter;\n+                                                        done(s);\n+                                                        delete tag;\n+                                                      }\n+                                                    });\n+#else\n+                        done(errors::Internal(\"No GPU device in process\"));\n+                        // delete tag;\n+                        // It may be not safe to delete tag here, just abort here.\n+                        abort();\n+#endif\n+                      } else {\n+                        // LOG(INFO) << \"wrapper_done for fuse recv, nothon to do, in the case that tensor on cpu and can memcpy\";\n+                        if (__sync_sub_and_fetch(fuse_counter, 1) == 0) {"
      },
      {
        "body": "This comment is unnecessary for everyone else not working in Alibaba. Remove it or add a brief explanation here. ",
        "diff_hunk": "@@ -0,0 +1,551 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif // GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_env.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace {\n+// Default connection timeout is 10s.\n+static const size_t kMaxConnectionTimeoutInMS = 10000;\n+static const size_t kUSleepInMS = 100;\n+static const size_t kUSleepInUs = 1000 * kUSleepInMS;\n+\n+void ProcessCallOptions(SeastarClientTag* tag) {\n+  if (tag->call_opts_ != nullptr) {\n+    if (!tag->call_opts_->UseWaitForReady()) {\n+      tag->fail_fast_ = true;\n+    }\n+    \n+    if (tag->call_opts_->GetTimeout() > 0) {\n+      tag->timeout_in_ms_ = tag->call_opts_->GetTimeout();\n+    }\n+  }\n+}\n+\n+std::string GenErrorMsg(bool is_init, bool is_broken, const std::string& addr) {\n+  std::string msg = \"Seastar channel: unknown error. connection is : \" + addr;\n+\n+  if (!is_init) {\n+    msg = \"Seastar channel: connection is timeout. connection is : \" + addr;\n+  }\n+\n+  if (is_broken) {\n+    msg = \"Seastar channel: connection is broken. connection is : \" + addr;\n+  }\n+\n+  return msg;\n+}\n+\n+} // namespace\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  protobuf::Message* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  // internal error, that we dont need to parse the response,\n+                  // reponse is nullptr\n+                  if (s.code() != error::INTERNAL) {\n+                    response->ParseFromArray(tag->resp_body_buf_.data_,\n+                                            tag->resp_body_buf_.len_);\n+                  }\n+                  if (!s.ok()) {\n+                    if (tag->method_ == SeastarWorkerServiceMethod::kLogging ||\n+                        tag->method_ == SeastarWorkerServiceMethod::kTracing) {\n+                      // Logging & Tracing in worker.cc is UNIMPLEMENTED, ignore the error\n+                    } else {\n+                      // Debugging info\n+                      LOG(INFO) << \"RPC's status is not ok. status code=\" << s.code()\n+                                << \", err msg=\" << s.error_message().c_str();\n+                    }\n+                  }\n+                  done(s);\n+                  delete tag;\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  // LOG(INFO) << \"InitSeastarClientTag for no fuse tensor recv\";\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_,\n+                            tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDead(sm.is_dead_);\n+      response->SetDataType(sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for no fuse, can memcpy and on cpu\"\n+          //          << \",request:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensor(val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \",request:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+                  \n+                  bool can_memcpy = DataTypeCanUseMemcpy(response->GetDataType());\n+                  if (can_memcpy) {\n+                    if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+                        (!response->GetOnHost())) {\n+#if GOOGLE_CUDA\n+                      Tensor* gpu_copy = new Tensor(response->GetAlloc(), response->GetTensor().dtype(), response->GetTensor().shape());\n+                      GPUUtil::CopyCPUTensorToGPU(&response->GetTensor(),\n+                                                  response->GetDevice()->tensorflow_gpu_device_info()->default_context,\n+                                                  response->GetDevice(),\n+                                                  gpu_copy,\n+                                                  [gpu_copy, response, done, tag](const Status& s) {\n+                                                    CHECK(s.ok()) << \"copy tensor to gpu sync\";\n+                                                    response->SetTensor(*gpu_copy);\n+                                                    done(s);\n+                                                    delete gpu_copy;\n+                                                    delete tag;\n+                                                  });\n+#else\n+                      done(errors::Internal(\"No GPU device in process\"));\n+                      delete tag;\n+#endif\n+                    } else {\n+                      // LOG(INFO) << \"wrapper_done for no fuse, nothon to do, in the case that tensor on cpu and can memcpy\";\n+                      done(s);\n+                      delete tag;\n+                    }\n+                  } else {\n+                    // could not memcoy\n+                    // LOG(INFO) << \"wrapper_done, could not memcpy, recv bytes: \"\n+                    // << tag->resp_tensor_bufs_[0].len_\n+                    // << \", DataType: \" << response->GetDataType();\n+                    ParseProtoUnlimited(&response->GetTensorProto(),\n+                                        tag->resp_tensor_bufs_[0].data_,\n+                                        tag->resp_tensor_bufs_[0].len_);\n+                    Tensor val;\n+                    Status status = response->GetDevice()->MakeTensorFromProto(\n+                        response->GetTensorProto(),\n+                        response->GetAllocAttributes(),\n+                        &val);\n+                    //LOG(INFO) << \"parse msg status: \" << status.error_message();\n+                    CHECK(status.ok()) << \"make cpu tensor from proto.\";\n+                    response->SetTensor(val);\n+                    done(status);\n+                    delete tag;\n+                  }\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+void InitSeastarClientTag(protobuf::Message* request,\n+\t\t\t  SeastarFuseTensorResponse* response,\n+\t\t\t  StatusCallback done,\n+\t\t\t  SeastarClientTag* tag,\n+        CallOptions* call_opts) {\n+  tag->req_body_buf_.len_ = request->ByteSize();\n+  tag->req_body_buf_.data_ = new char[tag->req_body_buf_.len_]();\n+  request->SerializeToArray(tag->req_body_buf_.data_, tag->req_body_buf_.len_);\n+\n+  tag->req_header_buf_.len_ = 32;\n+  tag->req_header_buf_.data_ = new char[32]();\n+  \n+  memcpy(tag->req_header_buf_.data_, \"AAAAAAAA\", 8);\n+  memcpy(tag->req_header_buf_.data_ + 8, &tag, 8);\n+  memcpy(tag->req_header_buf_.data_ + 16, &tag->method_, 4);\n+  // Ignore the status segment in request\n+  // memcpy(tag->req_header_buf_.data_ + 20, &tag->status_, 2);\n+  memcpy(tag->req_header_buf_.data_ + 24, &tag->req_body_buf_.len_, 8);\n+\n+  // LOG(INFO) << \"tensor request: \" << request->DebugString();\n+\n+ ParseMessageCallback wrapper_parse_message\n+    = [request, response, tag] (int idx) {\n+      SeastarMessage sm;\n+      SeastarMessage::DeserializeMessage(&sm, tag->resp_message_bufs_[idx].data_);\n+\n+      response->SetIsDeadByIndex(idx, sm.is_dead_);\n+      response->SetDataTypeByIndex(idx, sm.data_type_);\n+      bool can_memcpy = DataTypeCanUseMemcpy(sm.data_type_);\n+\n+      if (can_memcpy) {\n+        if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+            (!response->GetOnHost())) {\n+ #if GOOGLE_CUDA\n+          // LOG(INFO) << \"parse msg, can memcpy and on GPU\";\n+          // dst tensor on gpu\n+          Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+          Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\n+\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&cpu_copy));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, cpu_copy);\n+#else\n+          return errors::Internal(\"No GPU device in process\");\n+#endif\n+\n+        } else { \n+          // LOG(INFO) << \"parse msg for fuse, can memcpy and on cpu\"\n+          //          << \"idx is: \" << idx\n+          //          << \", request is:\" << request->DebugString();\n+          Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\n+          tag->resp_tensor_bufs_[idx].data_ = reinterpret_cast<char*>(DMAHelper::base(&val));\n+          tag->resp_tensor_bufs_[idx].len_ =  sm.tensor_bytes_;\n+          tag->resp_tensor_bufs_[idx].owned_ = false;\n+          \n+          response->SetTensorByIndex(idx, val);\n+        }\n+      } else {\n+        // LOG(INFO) << \"parse msg, could not memcpy, tensor bytes: \" << sm.tensor_bytes_\n+        //          << \", idx is: \" << idx\n+        //          << \", request is:\" << request->DebugString();\n+        tag->resp_tensor_bufs_[idx].len_ = sm.tensor_bytes_;\n+        tag->resp_tensor_bufs_[idx].data_ = new char[tag->resp_tensor_bufs_[idx].len_]();\n+      }\n+\n+      return Status();\n+  };\n+  tag->parse_message_ = std::move(wrapper_parse_message);\n+\n+  StatusCallback wrapper_done\n+    = std::bind([response, tag](StatusCallback done,\n+                                const Status& s) {\n+                  if (!s.ok()) {\n+                    LOG(ERROR) << \"wrapper_done, status not ok. status code=\" << s.code()\n+                               << \", err msg=\" << s.error_message().c_str();\n+                    done(s);\n+                    delete tag;\n+                    return;\n+                  }\n+\n+                  int fuse_count = tag->fuse_count_;\n+                  int *fuse_counter = new int(fuse_count);\n+\n+                  for (int idx = 0; idx < fuse_count; ++idx) {\n+                    bool can_memcpy = DataTypeCanUseMemcpy(response->GetDataTypeByIndex(idx));\n+                    // LOG(INFO) << \"wrapper_done for fuse recv, fuse count is: \" << tag->fuse_count_\n+                    //          <<\", idx is: \" << idx << \", data type is: \" << response->GetDataTypeByIndex(idx);\n+                    if (can_memcpy) {\n+                      if (response->GetDevice()->tensorflow_gpu_device_info() &&\n+                          (!response->GetOnHost())) {\n+#if GOOGLE_CUDA\n+                        Tensor* gpu_copy = new Tensor(response->GetAlloc(),\n+                                                      response->GetTensorByIndex(idx).dtype(),\n+                                                      response->GetTensorByIndex(idx).shape());\n+                        GPUUtil::CopyCPUTensorToGPU(&response->GetTensorByIndex(idx),\n+                                                    response->GetDevice()->tensorflow_gpu_device_info()->default_context,\n+                                                    response->GetDevice(),\n+                                                    gpu_copy,\n+                                                    [gpu_copy, response, done, tag, fuse_counter, idx](const Status& s) {\n+                                                      CHECK(s.ok()) << \"copy tensor to gpu sync\";\n+                                                      response->SetTensorByIndex(idx, *gpu_copy);\n+                                                      delete gpu_copy;\n+                                                      if (__sync_sub_and_fetch(fuse_counter, 1) == 0) {\n+                                                        delete fuse_counter;\n+                                                        done(s);\n+                                                        delete tag;\n+                                                      }\n+                                                    });\n+#else\n+                        done(errors::Internal(\"No GPU device in process\"));\n+                        // delete tag;\n+                        // It may be not safe to delete tag here, just abort here.\n+                        abort();\n+#endif\n+                      } else {\n+                        // LOG(INFO) << \"wrapper_done for fuse recv, nothon to do, in the case that tensor on cpu and can memcpy\";\n+                        if (__sync_sub_and_fetch(fuse_counter, 1) == 0) {\n+                          delete fuse_counter;\n+                          done(s);\n+                          delete tag;\n+                        }\n+                      }\n+                    } else { // could not memcoy\n+                      // LOG(INFO) << \"wrapper_done for fuse recv, could not memcpy, recv bytes: \"\n+                      //        << tag->resp_tensor_bufs_[idx].len_\n+                      //        << \", DataType: \" << response->GetDataTypeByIndex(idx);\n+                      ParseProtoUnlimited(&response->GetTensorProtoByIndex(idx),\n+                                          tag->resp_tensor_bufs_[idx].data_,\n+                                          tag->resp_tensor_bufs_[idx].len_);\n+                      Tensor val;\n+                      Status status = response->GetDevice()->MakeTensorFromProto(\n+                          response->GetTensorProtoByIndex(idx),\n+                          response->GetAllocAttributes(), &val);\n+                      CHECK(status.ok()) << \"make cpu tensor from proto.\";\n+                      response->SetTensorByIndex(idx, val);\n+                      if (__sync_sub_and_fetch(fuse_counter, 1) == 0) {\n+                        delete fuse_counter;\n+                        done(status);\n+                        delete tag;\n+                      }\n+                    }\n+                  } // end for cycle of the fuse count\n+                },\n+                std::move(done),\n+                std::placeholders::_1);\n+\n+  tag->done_ = std::move(wrapper_done);\n+  tag->call_opts_ = call_opts;\n+  ProcessCallOptions(tag);\n+}\n+\n+SeastarClientTag::SeastarClientTag(tensorflow::SeastarWorkerServiceMethod method,\n+                                   WorkerEnv* env,\n+                                   int fuse_count)\n+  : method_(method),\n+    env_(env),\n+    resp_err_msg_len_(0),\n+    fuse_count_(fuse_count),\n+    resp_message_bufs_(fuse_count),\n+    resp_tensor_bufs_(fuse_count),\n+    fail_fast_(false),\n+    timeout_in_ms_(0) {\n+\n+  for (int idx = 0; idx < fuse_count_; ++idx) {\n+    resp_message_bufs_[idx].len_ = SeastarMessage::kMessageTotalBytes;\n+    resp_message_bufs_[idx].data_ = new char[resp_message_bufs_[idx].len_];\n+  }  \n+}\n+\n+SeastarClientTag::~SeastarClientTag() {\n+  delete [] req_header_buf_.data_;\n+  delete [] req_body_buf_.data_;\n+  delete [] resp_body_buf_.data_;\n+\n+  for (int i = 0; i < fuse_count_; ++i) {\n+    delete [] resp_message_bufs_[i].data_;\n+\n+    if (resp_tensor_bufs_[i].owned_) {\n+      delete [] resp_tensor_bufs_[i].data_;\n+    }\n+  }\n+}\n+\n+void SeastarClientTag::RetryStartReq(int retry_count,\n+                                     seastar::channel* seastar_channel) {\n+  usleep(kUSleepInUs);\n+  bool is_init = seastar_channel->is_init();\n+  bool is_broken = seastar_channel->is_channel_broken();\n+\n+  if (is_init && !is_broken) {\n+    // Good case.\n+    LOG(WARNING) << \"Seastar conn success after retry.\";\n+    seastar_channel->put(ToUserPacket());\n+\n+  } else if (--retry_count > 0) {\n+    // Bad case and need retry.\n+    LOG(WARNING) << \"Seastar conn timeout for: \" << seastar_channel->get_addr()\n+                 << \", left retry count: \" << retry_count;\n+    Schedule([this, retry_count, seastar_channel]() {\n+        RetryStartReq(retry_count, seastar_channel);\n+      });\n+\n+  } else {\n+    // Bad case and retry count is exhausted.\n+    LOG(ERROR) << \"Seastar conn timeout for: \" << seastar_channel->get_addr()\n+               << \", retry count is exhausted.\";\n+    RecvRespDone(tensorflow::Status(error::INTERNAL,\n+                                    GenErrorMsg(is_init, is_broken,\n+                                                seastar_channel->get_addr())));\n+  }\n+}\n+\n+void SeastarClientTag::StartReq(seastar::channel* seastar_channel) {\n+  if (seastar_channel->is_channel_broken()) {\n+    seastar_channel->reconnect();\n+  }\n+\n+  bool is_init = seastar_channel->is_init();\n+  bool is_broken = seastar_channel->is_channel_broken();\n+  \n+  if (is_init && !is_broken) {\n+    // Good case.\n+    seastar_channel->put(ToUserPacket());\n+\n+  } else if (!fail_fast_) {\n+    // Bad case and need retry.\n+    int max_retry = kMaxConnectionTimeoutInMS / kUSleepInMS;\n+    if (timeout_in_ms_ != 0) {\n+      max_retry = timeout_in_ms_ / kUSleepInMS;\n+    }\n+\n+    // NOTE(rangeng.llb): Maybe this is in seastar thread, retry by schedule\n+    // again.\n+    // Refer to: https://workitem.aone.alibaba-inc.com/issue/16534619 for more details."
      },
      {
        "body": "s/NumAllCPUs/NumTotalCPUs/",
        "diff_hunk": "@@ -0,0 +1,134 @@\n+#include \"tensorflow/contrib/seastar/seastar_cpuset.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/cpu_info.h\"\n+\n+#include <sys/file.h>\n+#include <dirent.h>\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+#include <string>\n+\n+namespace tensorflow {\n+namespace {\n+const char* ROOT_PATH = \"/tmp_tf\";\n+const char* DEFAULT_ROOT_PATH = \"/tmp\";\n+const char* CPUSET_FILE_PATH = \"/cpuset\";\n+const size_t CORES_PER_FILE = 1;\n+const size_t INIT_CPU_ID = 16;\n+}\n+\n+class FileLocker {\n+public:\n+  FileLocker(const std::string& rd) : _root_dir(rd) {}\n+  virtual ~FileLocker() {}\n+\n+  bool Lock(const std::string& file_name) {\n+    return LockerOpImpl(file_name, LOCK_EX | LOCK_NB);\n+  }\n+\n+  void Unlock(const std::string& file_name) {\n+    LockerOpImpl(file_name, LOCK_UN | LOCK_NB);\n+  }\n+\n+private:\n+  bool LockerOpImpl(const std::string& file_name, int lock_type) {\n+    std::string file_path;\n+    file_path += _root_dir + std::string(\"/\") + file_name;\n+    int fd = open(file_path.c_str(), O_RDWR | O_CREAT, 0777);\n+    if (fd < 0) {\n+      LOG(ERROR) << \"can't open file:\" << file_path;\n+      return false;\n+    }\n+\n+    int stat = flock(fd, lock_type);\n+    return (stat == 0);\n+  }\n+\n+private:\n+  const std::string _root_dir;\n+};\n+\n+std::string CpusetAllocator::GetCpuset(size_t core_number) {\n+  // critical section: semphore to lock this function\n+  if (!ExistDir()) {\n+    CreateDir();\n+  }\n+  CreateFiles();\n+  auto locked_files = LockFiles(core_number);\n+  return ToCpuset(locked_files);\n+}\n+\n+bool CpusetAllocator::ExistDir() {\n+  if (opendir(ROOT_PATH) != nullptr) {\n+    _root_dir = ROOT_PATH;\n+  } else if (opendir(DEFAULT_ROOT_PATH) != nullptr) {\n+    _root_dir = DEFAULT_ROOT_PATH;\n+  } else {\n+    LOG(ERROR) << \"create cpuset dir failure,\" \n+               << \"both /tmp & /tmp_tf not exist in the machine, \"\n+               << \"please try other protocol\";\n+    return false;\n+  }\n+  _root_dir += CPUSET_FILE_PATH;\n+\n+  return opendir(_root_dir.c_str()) != nullptr;\n+}\n+\n+void CpusetAllocator::CreateDir() {\n+  int flag=mkdir(_root_dir.c_str(), 0777);\n+  if (flag != 0) {\n+    LOG(ERROR) << \"create cpuset dir failure\";\n+  }\n+}\n+\n+void CpusetAllocator::CreateFiles() {\n+  // todo: port::NumAllCPUs(), all phsical core should be available in docker, or this would bug here\n+  // fuxi set value is better.\n+  for (auto i = INIT_CPU_ID; i < port::NumAllCPUs(); ++i) {"
      },
      {
        "body": "You need to move next line above this line, i.e. release worker before calling ``call->done``. See 95147fbbf7b62b421c3a4da7f770249abc3f1287 for details.",
        "diff_hunk": "@@ -0,0 +1,493 @@\n+#include \"tensorflow/contrib/seastar/seastar_rendezvous_mgr.h\"\n+#include \"tensorflow/core/common_runtime/device.h\"\n+#include \"tensorflow/core/common_runtime/device_mgr.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#include \"tensorflow/core/common_runtime/process_util.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/contrib/seastar/seastar_worker_cache.h\"\n+#include \"tensorflow/contrib/seastar/seastar_worker_interface.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_interface.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+class SeastarRemoteRendezvous : public BaseRemoteRendezvous {\n+ public:\n+  SeastarRemoteRendezvous(const WorkerEnv* env, int64 step_id)\n+      : BaseRemoteRendezvous(env, step_id) {}\n+\n+ protected:\n+  void RecvFromRemoteAsync(const Rendezvous::ParsedKey& parsed,\n+                           const Rendezvous::Args& args,\n+                           DoneCallback done,\n+                           CallOptions* opts) override;\n+\n+  void FuseRecvFromRemoteAsync(\n+      const std::vector<Rendezvous::ParsedKey>& parsed_keys,\n+      const Rendezvous::Args& args,\n+      FuseDoneCallback done) override;\n+\n+ private:\n+  ~SeastarRemoteRendezvous() override {}\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(SeastarRemoteRendezvous);\n+};\n+\n+// Used only to retrieve tensors from remote processes.\n+class SeastarRecvTensorCall : public BaseRecvTensorCall {\n+ public:\n+  SeastarRecvTensorCall() : wi_(nullptr), dst_device_(nullptr) {}\n+\n+  void Init(WorkerInterface* wi, int64 step_id, StringPiece key,\n+            AllocatorAttributes alloc_attrs, Device* dst_device,\n+            const Rendezvous::Args& recv_args, Rendezvous::DoneCallback done) {\n+    wi_ = wi;\n+    seastar_wi_ = dynamic_cast<SeastarWorkerInterface*>(wi_);\n+    alloc_attrs_ = alloc_attrs;\n+    dst_device_ = dst_device;\n+    recv_args_ = recv_args;\n+    done_ = std::move(done);\n+    req_.set_step_id(step_id);\n+    req_.set_rendezvous_key(key.data(), key.size());\n+  }\n+\n+  void Reset(WorkerCacheInterface* wc) {\n+    wc->ReleaseWorker(src_worker_, wi_);\n+    wi_ = nullptr;\n+    seastar_wi_ = nullptr;\n+    alloc_attrs_ = AllocatorAttributes();\n+    dst_device_ = nullptr;\n+    // We don't clear opts_ and assume that Init will set up the state for\n+    // opts_ appropriately.\n+    req_.Clear();\n+    resp_.Clear();\n+    {\n+      mutex_lock l(mu_);\n+      status_ = Status::OK();\n+    }\n+    done_ = nullptr;\n+  }\n+\n+  ~SeastarRecvTensorCall() override {\n+    // Since only the SeastarRecvTensorFreeList will delete an\n+    // SeastarRecvTensorCall, and it always sets this->wi_ to null when\n+    // a call object is released to it, we can assert that this->wi_ is\n+    // always null at the point of deletion.\n+    CHECK_EQ(static_cast<WorkerInterface*>(nullptr), wi_)\n+        << \"Leaking WorkerInterface in SeastarRecvTensorCall destructor.\";\n+  }\n+\n+  void Start(std::function<void()> recv_done) override {\n+    StartRTCall(std::move(recv_done));\n+  }\n+\n+  void StartAbort(const Status& s) override {\n+    {\n+      mutex_lock l(mu_);\n+      status_.Update(s);\n+    }\n+    opts_.StartCancel();\n+  }\n+\n+  Status status() const override {\n+    mutex_lock l(mu_);\n+    return status_;\n+  }\n+\n+  const Tensor& tensor() const { return resp_.GetTensor(); }\n+  bool is_dead() const { return resp_.GetIsDead(); }\n+  const Rendezvous::Args& recv_args() const { return recv_args_; }\n+  const Rendezvous::DoneCallback& done() const { return done_; }\n+\n+ private:\n+  friend class SeastarRemoteRendezvous;\n+\n+  // Start the main RecvTensor call, checking for an async abort.\n+  void StartRTCall(std::function<void()> recv_done) {\n+    resp_.InitAlloc(dst_device_, alloc_attrs_);\n+    using namespace std::placeholders;\n+    StatusCallback cb = std::bind(\n+        [this](std::function<void()> recv_done,\n+               // Begin unbound arguments.\n+               const Status& s) {\n+          if (!s.ok()) {\n+            mutex_lock l(mu_);\n+            status_.Update(s);\n+          }\n+          recv_done();\n+        },\n+        std::move(recv_done), _1);\n+    seastar_wi_->RecvTensorAsync(&opts_, &req_, &resp_, std::move(cb));\n+  }\n+\n+private:\n+  string src_worker_;\n+  string src_rel_device_;\n+  WorkerInterface* wi_;\n+  SeastarWorkerInterface* seastar_wi_;\n+  AllocatorAttributes alloc_attrs_;\n+  Device* dst_device_;\n+  CallOptions opts_;\n+  RecvTensorRequest req_;\n+  SeastarTensorResponse resp_;\n+  Rendezvous::Args recv_args_;\n+  Rendezvous::DoneCallback done_;\n+\n+  mutable mutex mu_;\n+  Status status_ GUARDED_BY(mu_);\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(SeastarRecvTensorCall);\n+};\n+\n+class SeastarRecvTensorFreeList {\n+ public:\n+  virtual ~SeastarRecvTensorFreeList() {\n+    for (size_t i = 0; i < objects_.size(); i++) {\n+      delete objects_[i];\n+    }\n+  }\n+\n+  SeastarRecvTensorCall* New() {\n+    {\n+      mutex_lock l(mu_);\n+      if (!objects_.empty()) {\n+        SeastarRecvTensorCall* result = objects_.back();\n+        objects_.pop_back();\n+        return result;\n+      }\n+    }\n+    return new SeastarRecvTensorCall;\n+  }\n+\n+  void Release(SeastarRecvTensorCall* obj, WorkerCacheInterface* wc) {\n+    obj->Reset(wc);\n+    {\n+      mutex_lock l(mu_);\n+      if (objects_.size() < kMaxObjects) {\n+        objects_.push_back(obj);\n+        return;\n+      }\n+    }\n+    delete obj;\n+  }\n+\n+ private:\n+  static const int kMaxObjects = 1000;\n+\n+  mutex mu_;\n+  std::vector<SeastarRecvTensorCall*> objects_ GUARDED_BY(mu_);\n+};\n+\n+static SeastarRecvTensorFreeList* get_call_freelist() {\n+  static SeastarRecvTensorFreeList* call_freelist =\n+    new SeastarRecvTensorFreeList();\n+  return call_freelist;\n+}\n+\n+void SeastarRemoteRendezvous::RecvFromRemoteAsync(\n+    const Rendezvous::ParsedKey& parsed, const Rendezvous::Args& recv_args,\n+    DoneCallback done, CallOptions* opts) {\n+  CHECK(is_initialized());\n+  Status s;\n+\n+  // Prepare a RecvTensor call that can handle being aborted.\n+  SeastarRecvTensorCall* call = get_call_freelist()->New();\n+\n+  // key.src_device identifies a remote device.\n+  if (!DeviceNameUtils::SplitDeviceName(parsed.src_device, &call->src_worker_,\n+                                        &call->src_rel_device_)) {\n+    s = errors::Internal(parsed.src_device,\n+                         \" is invalid remote source device.\");\n+  }\n+  WorkerSession* sess = session();\n+  WorkerInterface* rwi = sess->worker_cache->CreateWorker(call->src_worker_);\n+  if (s.ok() && rwi == nullptr) {\n+    s = errors::Internal(\"No worker known as \", call->src_worker_);\n+  }\n+\n+  Device* dst_device;\n+  if (s.ok()) {\n+    s = sess->device_mgr->LookupDevice(parsed.dst_device, &dst_device);\n+  }\n+  if (!s.ok()) {\n+    if (rwi != nullptr) {\n+      sess->worker_cache->ReleaseWorker(call->src_worker_, rwi);\n+    }\n+    get_call_freelist()->Release(call, sess->worker_cache.get());\n+    done(s, Args(), recv_args, Tensor{}, false);\n+    LOG(ERROR) << \"RecvFromRemoteAsync failed, detail \" << s.error_message();\n+    return;\n+  }\n+  call->Init(rwi, step_id_, parsed.FullKey(), recv_args.alloc_attrs, dst_device,\n+             recv_args, std::move(done));\n+\n+  // Record \"call\" in active_ so that it can be aborted cleanly.\n+  s = RegisterCall(call);\n+  if (!s.ok()) {\n+    LOG(WARNING) << \"Rendezvous has been aborted, ignore the rpc call.\"\n+                 << \", rendezvous key: \" << parsed.FullKey().ToString();\n+    session()->worker_cache->ReleaseWorker(call->src_worker_, call->wi_);\n+    call->done()(s, Args(), Args(), Tensor(), false);\n+    call->wi_ = nullptr;\n+    get_call_freelist()->Release(call, session()->worker_cache.get());\n+    return;\n+  }\n+\n+  // Start \"call\".\n+  Ref();\n+  call->Start([this, call]() {\n+    // Removes \"call\" from active_. Prevent StartAbort().\n+    DeregisterCall(call);\n+    // If StartAbort was called prior to DeregisterCall, then the\n+    // current status should be bad.\n+    Status s = call->status();\n+    call->done()(s, Args(), call->recv_args(), call->tensor(), call->is_dead());"
      },
      {
        "body": "Same above.",
        "diff_hunk": "@@ -0,0 +1,493 @@\n+#include \"tensorflow/contrib/seastar/seastar_rendezvous_mgr.h\"\n+#include \"tensorflow/core/common_runtime/device.h\"\n+#include \"tensorflow/core/common_runtime/device_mgr.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#include \"tensorflow/core/common_runtime/process_util.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/contrib/seastar/seastar_worker_cache.h\"\n+#include \"tensorflow/contrib/seastar/seastar_worker_interface.h\"\n+#include \"tensorflow/core/distributed_runtime/worker_interface.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+class SeastarRemoteRendezvous : public BaseRemoteRendezvous {\n+ public:\n+  SeastarRemoteRendezvous(const WorkerEnv* env, int64 step_id)\n+      : BaseRemoteRendezvous(env, step_id) {}\n+\n+ protected:\n+  void RecvFromRemoteAsync(const Rendezvous::ParsedKey& parsed,\n+                           const Rendezvous::Args& args,\n+                           DoneCallback done,\n+                           CallOptions* opts) override;\n+\n+  void FuseRecvFromRemoteAsync(\n+      const std::vector<Rendezvous::ParsedKey>& parsed_keys,\n+      const Rendezvous::Args& args,\n+      FuseDoneCallback done) override;\n+\n+ private:\n+  ~SeastarRemoteRendezvous() override {}\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(SeastarRemoteRendezvous);\n+};\n+\n+// Used only to retrieve tensors from remote processes.\n+class SeastarRecvTensorCall : public BaseRecvTensorCall {\n+ public:\n+  SeastarRecvTensorCall() : wi_(nullptr), dst_device_(nullptr) {}\n+\n+  void Init(WorkerInterface* wi, int64 step_id, StringPiece key,\n+            AllocatorAttributes alloc_attrs, Device* dst_device,\n+            const Rendezvous::Args& recv_args, Rendezvous::DoneCallback done) {\n+    wi_ = wi;\n+    seastar_wi_ = dynamic_cast<SeastarWorkerInterface*>(wi_);\n+    alloc_attrs_ = alloc_attrs;\n+    dst_device_ = dst_device;\n+    recv_args_ = recv_args;\n+    done_ = std::move(done);\n+    req_.set_step_id(step_id);\n+    req_.set_rendezvous_key(key.data(), key.size());\n+  }\n+\n+  void Reset(WorkerCacheInterface* wc) {\n+    wc->ReleaseWorker(src_worker_, wi_);\n+    wi_ = nullptr;\n+    seastar_wi_ = nullptr;\n+    alloc_attrs_ = AllocatorAttributes();\n+    dst_device_ = nullptr;\n+    // We don't clear opts_ and assume that Init will set up the state for\n+    // opts_ appropriately.\n+    req_.Clear();\n+    resp_.Clear();\n+    {\n+      mutex_lock l(mu_);\n+      status_ = Status::OK();\n+    }\n+    done_ = nullptr;\n+  }\n+\n+  ~SeastarRecvTensorCall() override {\n+    // Since only the SeastarRecvTensorFreeList will delete an\n+    // SeastarRecvTensorCall, and it always sets this->wi_ to null when\n+    // a call object is released to it, we can assert that this->wi_ is\n+    // always null at the point of deletion.\n+    CHECK_EQ(static_cast<WorkerInterface*>(nullptr), wi_)\n+        << \"Leaking WorkerInterface in SeastarRecvTensorCall destructor.\";\n+  }\n+\n+  void Start(std::function<void()> recv_done) override {\n+    StartRTCall(std::move(recv_done));\n+  }\n+\n+  void StartAbort(const Status& s) override {\n+    {\n+      mutex_lock l(mu_);\n+      status_.Update(s);\n+    }\n+    opts_.StartCancel();\n+  }\n+\n+  Status status() const override {\n+    mutex_lock l(mu_);\n+    return status_;\n+  }\n+\n+  const Tensor& tensor() const { return resp_.GetTensor(); }\n+  bool is_dead() const { return resp_.GetIsDead(); }\n+  const Rendezvous::Args& recv_args() const { return recv_args_; }\n+  const Rendezvous::DoneCallback& done() const { return done_; }\n+\n+ private:\n+  friend class SeastarRemoteRendezvous;\n+\n+  // Start the main RecvTensor call, checking for an async abort.\n+  void StartRTCall(std::function<void()> recv_done) {\n+    resp_.InitAlloc(dst_device_, alloc_attrs_);\n+    using namespace std::placeholders;\n+    StatusCallback cb = std::bind(\n+        [this](std::function<void()> recv_done,\n+               // Begin unbound arguments.\n+               const Status& s) {\n+          if (!s.ok()) {\n+            mutex_lock l(mu_);\n+            status_.Update(s);\n+          }\n+          recv_done();\n+        },\n+        std::move(recv_done), _1);\n+    seastar_wi_->RecvTensorAsync(&opts_, &req_, &resp_, std::move(cb));\n+  }\n+\n+private:\n+  string src_worker_;\n+  string src_rel_device_;\n+  WorkerInterface* wi_;\n+  SeastarWorkerInterface* seastar_wi_;\n+  AllocatorAttributes alloc_attrs_;\n+  Device* dst_device_;\n+  CallOptions opts_;\n+  RecvTensorRequest req_;\n+  SeastarTensorResponse resp_;\n+  Rendezvous::Args recv_args_;\n+  Rendezvous::DoneCallback done_;\n+\n+  mutable mutex mu_;\n+  Status status_ GUARDED_BY(mu_);\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(SeastarRecvTensorCall);\n+};\n+\n+class SeastarRecvTensorFreeList {\n+ public:\n+  virtual ~SeastarRecvTensorFreeList() {\n+    for (size_t i = 0; i < objects_.size(); i++) {\n+      delete objects_[i];\n+    }\n+  }\n+\n+  SeastarRecvTensorCall* New() {\n+    {\n+      mutex_lock l(mu_);\n+      if (!objects_.empty()) {\n+        SeastarRecvTensorCall* result = objects_.back();\n+        objects_.pop_back();\n+        return result;\n+      }\n+    }\n+    return new SeastarRecvTensorCall;\n+  }\n+\n+  void Release(SeastarRecvTensorCall* obj, WorkerCacheInterface* wc) {\n+    obj->Reset(wc);\n+    {\n+      mutex_lock l(mu_);\n+      if (objects_.size() < kMaxObjects) {\n+        objects_.push_back(obj);\n+        return;\n+      }\n+    }\n+    delete obj;\n+  }\n+\n+ private:\n+  static const int kMaxObjects = 1000;\n+\n+  mutex mu_;\n+  std::vector<SeastarRecvTensorCall*> objects_ GUARDED_BY(mu_);\n+};\n+\n+static SeastarRecvTensorFreeList* get_call_freelist() {\n+  static SeastarRecvTensorFreeList* call_freelist =\n+    new SeastarRecvTensorFreeList();\n+  return call_freelist;\n+}\n+\n+void SeastarRemoteRendezvous::RecvFromRemoteAsync(\n+    const Rendezvous::ParsedKey& parsed, const Rendezvous::Args& recv_args,\n+    DoneCallback done, CallOptions* opts) {\n+  CHECK(is_initialized());\n+  Status s;\n+\n+  // Prepare a RecvTensor call that can handle being aborted.\n+  SeastarRecvTensorCall* call = get_call_freelist()->New();\n+\n+  // key.src_device identifies a remote device.\n+  if (!DeviceNameUtils::SplitDeviceName(parsed.src_device, &call->src_worker_,\n+                                        &call->src_rel_device_)) {\n+    s = errors::Internal(parsed.src_device,\n+                         \" is invalid remote source device.\");\n+  }\n+  WorkerSession* sess = session();\n+  WorkerInterface* rwi = sess->worker_cache->CreateWorker(call->src_worker_);\n+  if (s.ok() && rwi == nullptr) {\n+    s = errors::Internal(\"No worker known as \", call->src_worker_);\n+  }\n+\n+  Device* dst_device;\n+  if (s.ok()) {\n+    s = sess->device_mgr->LookupDevice(parsed.dst_device, &dst_device);\n+  }\n+  if (!s.ok()) {\n+    if (rwi != nullptr) {\n+      sess->worker_cache->ReleaseWorker(call->src_worker_, rwi);\n+    }\n+    get_call_freelist()->Release(call, sess->worker_cache.get());\n+    done(s, Args(), recv_args, Tensor{}, false);\n+    LOG(ERROR) << \"RecvFromRemoteAsync failed, detail \" << s.error_message();\n+    return;\n+  }\n+  call->Init(rwi, step_id_, parsed.FullKey(), recv_args.alloc_attrs, dst_device,\n+             recv_args, std::move(done));\n+\n+  // Record \"call\" in active_ so that it can be aborted cleanly.\n+  s = RegisterCall(call);\n+  if (!s.ok()) {\n+    LOG(WARNING) << \"Rendezvous has been aborted, ignore the rpc call.\"\n+                 << \", rendezvous key: \" << parsed.FullKey().ToString();\n+    session()->worker_cache->ReleaseWorker(call->src_worker_, call->wi_);\n+    call->done()(s, Args(), Args(), Tensor(), false);\n+    call->wi_ = nullptr;\n+    get_call_freelist()->Release(call, session()->worker_cache.get());\n+    return;\n+  }\n+\n+  // Start \"call\".\n+  Ref();\n+  call->Start([this, call]() {\n+    // Removes \"call\" from active_. Prevent StartAbort().\n+    DeregisterCall(call);\n+    // If StartAbort was called prior to DeregisterCall, then the\n+    // current status should be bad.\n+    Status s = call->status();\n+    call->done()(s, Args(), call->recv_args(), call->tensor(), call->is_dead());\n+    session()->worker_cache->ReleaseWorker(call->src_worker_, call->wi_);\n+    call->wi_ = nullptr;\n+    get_call_freelist()->Release(call, session()->worker_cache.get());\n+    Unref();\n+  });\n+}\n+\n+class SeastarFuseRecvTensorCall : public BaseRecvTensorCall {\n+public:\n+  SeastarFuseRecvTensorCall() : wi_(nullptr), dst_device_(nullptr) {}\n+\n+  void Init(WorkerInterface* wi, int64 step_id,\n+            const std::vector<Rendezvous::ParsedKey>& parsed_keys,\n+            AllocatorAttributes alloc_attrs, Device* dst_device,\n+            const Rendezvous::Args& recv_args,\n+            Rendezvous::FuseDoneCallback done) {\n+    wi_ = wi;\n+    seastar_wi_ = dynamic_cast<SeastarWorkerInterface*>(wi_);\n+    alloc_attrs_ = alloc_attrs;\n+    dst_device_ = dst_device;\n+    recv_args_ = recv_args;\n+    fuse_done_ = std::move(done);\n+    fuse_req_.set_step_id(step_id);\n+\n+    fuse_count_ = parsed_keys.size();\n+    for (int i = 0; i < fuse_count_; ++i) {\n+      StringPiece key = parsed_keys[i].FullKey();\n+      fuse_req_.add_rendezvous_key(key.data(), key.size());\n+    }\n+    fuse_resp_.Init(fuse_count_);\n+  }\n+\n+  void Reset(WorkerCacheInterface* wc) {\n+    wc->ReleaseWorker(src_worker_, wi_);\n+    wi_ = nullptr;\n+    seastar_wi_ = nullptr;\n+    alloc_attrs_ = AllocatorAttributes();\n+    dst_device_ = nullptr;\n+    // We don't clear opts_ and assume that Init will set up the state for\n+    // opts_ appropriately.\n+    fuse_req_.Clear();\n+    fuse_resp_.Clear();\n+    {\n+      mutex_lock l(mu_);\n+      status_ = Status::OK();\n+    }\n+    fuse_done_ = nullptr;\n+  }\n+\n+  ~SeastarFuseRecvTensorCall() override {\n+    // Since only the SeastarRecvTensorFreeList will delete an\n+    // SeastarRecvTensorCall, and it always sets this->wi_ to null when\n+    // a call object is released to it, we can assert that this->wi_ is\n+    // always null at the point of deletion.\n+    CHECK_EQ(static_cast<WorkerInterface*>(nullptr), wi_)\n+        << \"Leaking WorkerInterface in SeastarRecvTensorCall destructor.\";\n+  }\n+\n+  void Start(std::function<void()> recv_done) override {\n+    StartRTCall(std::move(recv_done));\n+  }\n+\n+  void StartAbort(const Status& s) override {\n+    {\n+      mutex_lock l(mu_);\n+      status_.Update(s);\n+    }\n+    opts_.StartCancel();\n+  }\n+\n+  Status status() const override {\n+    mutex_lock l(mu_);\n+    return status_;\n+  }\n+\n+  const std::vector<Tensor>& tensors() const { return fuse_resp_.GetTensors(); }\n+  const std::vector<bool>& is_deads() const { return fuse_resp_.GetIsDeads(); }\n+  const Rendezvous::Args& recv_args() const { return recv_args_; }\n+  const Rendezvous::FuseDoneCallback& fuse_done() const { return fuse_done_; }\n+\n+private:\n+  friend class SeastarRemoteRendezvous;\n+\n+  // Start the main FuseRecvTensor call, checking for an async abort.\n+  void StartRTCall(std::function<void()> recv_done) {\n+    // LOG(INFO) << \"StartRTCall for fuse tensor recv\";\n+    fuse_resp_.InitAlloc(dst_device_, alloc_attrs_);\n+    using namespace std::placeholders;\n+    StatusCallback cb = std::bind(\n+        [this](std::function<void()> recv_done,\n+               // Begin unbound arguments.\n+               const Status& s) {\n+          if (!s.ok()) {\n+            mutex_lock l(mu_);\n+            status_.Update(s);\n+          }\n+          recv_done();\n+        },\n+        std::move(recv_done), _1);\n+    seastar_wi_->FuseRecvTensorAsync(&opts_, &fuse_req_, &fuse_resp_,\n+                                     std::move(cb));\n+  }\n+\n+private:\n+  string src_worker_;\n+  string src_rel_device_;\n+  WorkerInterface* wi_;\n+  SeastarWorkerInterface* seastar_wi_;\n+  AllocatorAttributes alloc_attrs_;\n+  Device* dst_device_;\n+  CallOptions opts_;\n+  Rendezvous::FuseDoneCallback fuse_done_;\n+  int fuse_count_;\n+  FuseRecvTensorRequest fuse_req_;\n+  SeastarFuseTensorResponse fuse_resp_;\n+  Rendezvous::Args recv_args_;\n+\n+  mutable mutex mu_;\n+  Status status_ GUARDED_BY(mu_);\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(SeastarFuseRecvTensorCall);\n+};\n+\n+class SeastarFuseRecvTensorFreeList {\n+ public:\n+  virtual ~SeastarFuseRecvTensorFreeList() {\n+    for (size_t i = 0; i < objects_.size(); i++) {\n+      delete objects_[i];\n+    }\n+  }\n+\n+  SeastarFuseRecvTensorCall* New() {\n+    {\n+      mutex_lock l(mu_);\n+      if (!objects_.empty()) {\n+        SeastarFuseRecvTensorCall* result = objects_.back();\n+        objects_.pop_back();\n+        return result;\n+      }\n+    }\n+    return new SeastarFuseRecvTensorCall;\n+  }\n+\n+  void Release(SeastarFuseRecvTensorCall* obj, WorkerCacheInterface* wc) {\n+    obj->Reset(wc);\n+    {\n+      mutex_lock l(mu_);\n+      if (objects_.size() < kMaxObjects) {\n+        objects_.push_back(obj);\n+        return;\n+      }\n+    }\n+    delete obj;\n+  }\n+\n+ private:\n+  static const int kMaxObjects = 1000;\n+\n+  mutex mu_;\n+  std::vector<SeastarFuseRecvTensorCall*> objects_ GUARDED_BY(mu_);\n+};\n+\n+static SeastarFuseRecvTensorFreeList* get_fuse_call_freelist() {\n+  static SeastarFuseRecvTensorFreeList* fuse_call_freelist =\n+    new SeastarFuseRecvTensorFreeList();\n+  return fuse_call_freelist;\n+}\n+\n+void SeastarRemoteRendezvous::FuseRecvFromRemoteAsync(\n+    const std::vector<Rendezvous::ParsedKey>& parsed_keys,\n+    const Rendezvous::Args& recv_args, FuseDoneCallback done) {\n+  CHECK(is_initialized());\n+  int fuse_count = parsed_keys.size();\n+  Status s;\n+\n+  // Prepare a FuseRecvTensor call that can handle being aborted.\n+  SeastarFuseRecvTensorCall* call = get_fuse_call_freelist()->New();\n+\n+  // key.src_device identifies a remote device.\n+  if (!DeviceNameUtils::SplitDeviceName(parsed_keys[0].src_device,\n+                                        &call->src_worker_,\n+                                        &call->src_rel_device_)) {\n+    s = errors::Internal(parsed_keys[0].src_device,\n+                         \" is invalid remote source device.\");\n+  }\n+  WorkerSession* sess = session();\n+  WorkerInterface* rwi = sess->worker_cache->CreateWorker(call->src_worker_);\n+  if (s.ok() && rwi == nullptr) {\n+    s = errors::Internal(\"No worker known as \", call->src_worker_);\n+  }\n+\n+  Device* dst_device;\n+  if (s.ok()) {\n+    s = sess->device_mgr->LookupDevice(parsed_keys[0].dst_device, &dst_device);\n+  }\n+  if (!s.ok()) {\n+    if (rwi != nullptr) {\n+      sess->worker_cache->ReleaseWorker(call->src_worker_, rwi);\n+    }\n+    get_fuse_call_freelist()->Release(call, sess->worker_cache.get());\n+    done(s, std::vector<Args>(fuse_count), recv_args,\n+         std::vector<Tensor>(fuse_count),\n+         std::vector<bool>(fuse_count, false));\n+    return;\n+  }\n+\n+  call->Init(rwi, step_id_,\n+             parsed_keys, recv_args.alloc_attrs, dst_device,\n+             recv_args, std::move(done));\n+\n+  // Record \"call\" in active_ so that it can be aborted cleanly.\n+  RegisterCall(call);\n+\n+  // Start \"call\".\n+  Ref();\n+  call->Start([this, call]() {\n+    // Removes \"call\" from active_. Prevent StartAbort().\n+    DeregisterCall(call);\n+    // If StartAbort was called prior to DeregisterCall, then the\n+    // current status should be bad.\n+    Status s = call->status();\n+    call->fuse_done()(s, std::vector<Args>(call->fuse_count_), call->recv_args(),"
      },
      {
        "body": "This line seems broken.",
        "diff_hunk": "@@ -0,0 +1,112 @@\n+seastar_client.h #include <cstdio>"
      },
      {
        "body": "Do we need the entire boost/asio library here? Better work out a port if you only need some utility functions.",
        "diff_hunk": "@@ -0,0 +1,112 @@\n+seastar_client.h #include <cstdio>\n+\n+#include \"boost/asio/ip/address_v4.hpp\""
      },
      {
        "body": "Cannot find these headers.",
        "diff_hunk": "@@ -0,0 +1,112 @@\n+seastar_client.h #include <cstdio>\n+\n+#include \"boost/asio/ip/address_v4.hpp\"\n+#include \"core/reactor.hh\""
      },
      {
        "body": "Same above. ",
        "diff_hunk": "@@ -0,0 +1,56 @@\n+#ifndef TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_SERVER_H_\n+#define TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_SERVER_H_\n+#include \"core/channel.hh\""
      },
      {
        "body": "I am not sure about exceptions here. Better changing it to ``tensorflow::Status``.",
        "diff_hunk": "@@ -0,0 +1,522 @@\n+#include <fstream>\n+#include <map>\n+#include <stdexcept>"
      },
      {
        "body": "Same above.",
        "diff_hunk": "@@ -0,0 +1,89 @@\n+#ifndef TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_SERVER_TAG_H_\n+#define TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_SERVER_TAG_H_\n+\n+#include \"core/channel.hh\""
      },
      {
        "body": "Consider using ``absl::GetCurrentTimeNanos()``.",
        "diff_hunk": "@@ -0,0 +1,46 @@\n+#include \"tensorflow/contrib/seastar/seastar_stat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+\n+SeastarStat::SeastarStat() {\n+  Setup();\n+}\n+\n+void SeastarStat::Setup() {\n+  _counter = 0;\n+  _total_tensor_size = 0;\n+  gettimeofday(&_start, nullptr);"
      },
      {
        "body": "This file is Linux specific. If you are only using ``gettimeofday``, see my note above.",
        "diff_hunk": "@@ -0,0 +1,28 @@\n+#ifndef TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_STAT_H_\n+#define TENSORFLOW_CONTRIB_SEASTAR_SEASTAR_STAT_H_\n+#include <cstddef>\n+#include <sys/time.h>"
      },
      {
        "body": "Cannot find this file.",
        "diff_hunk": "@@ -0,0 +1,59 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include <assert.h>\n+#include \"tensorflow/contrib/seastar/seastar_tag_factory.h\"\n+#include \"tensorflow/contrib/seastar/seastar_client_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_server_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_worker_service.h\"\n+#include \"tensorflow/contrib/seastar/seastar_message.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"core/channel.hh\""
      },
      {
        "body": "Better change to ``<cassert>`` as this is C++.",
        "diff_hunk": "@@ -0,0 +1,59 @@\n+// Copyright (c) 2017, Alibaba Inc.\n+// All right reserved.\n+//\n+// Author: Liangbin LI <rangeng.llb@taobao.com>\n+// Created: 2017/11/23\n+// Description\n+\n+#include <assert.h>"
      },
      {
        "body": "Same above. Are you using these Linux specific headers?",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+\n+#include <pthread.h>"
      },
      {
        "body": "An empty line here. Better do a `clang-format` pass to your source files.",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+"
      },
      {
        "body": "I feel like all the (de-)serialization code could be moved here instead of scattering around in multiple locations. ",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+\n+#include <pthread.h>\n+#include <unistd.h>\n+#include <sys/syscall.h>\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/device.h\"\n+\n+namespace tensorflow {"
      },
      {
        "body": "Empty line here.",
        "diff_hunk": "@@ -0,0 +1,96 @@\n+"
      },
      {
        "body": "What is its use here?",
        "diff_hunk": "@@ -0,0 +1,28 @@\n+#pragma once "
      },
      {
        "body": "Any reason why you comment out these lines here?",
        "diff_hunk": "@@ -0,0 +1,434 @@\n+#include \"tensorflow/contrib/seastar/seastar_worker_service.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tag_factory.h\"\n+#include \"tensorflow/contrib/seastar/seastar_server_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/distributed_runtime/call_options.h\"\n+#include \"tensorflow/core/framework/rendezvous.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/public/session_options.h\"\n+#include \"tensorflow/contrib/verbs/verbs_util.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h\"\n+\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif  // GOOGLE_CUDA\n+\n+namespace tensorflow {\n+namespace {\n+\n+template<class RequestMessage, class ResponseMessage>\n+class SeastarCall {\n+public:\n+  RequestMessage req_;\n+  ResponseMessage resp_;\n+};\n+} // end of anonymous namespace\n+\n+using HandleRequestFunction = void (SeastarWorkerService::*)(SeastarServerTag*);\n+\n+SeastarWorkerService::SeastarWorkerService(SeastarWorker* worker)\n+  : worker_(worker) {\n+  handler_map_[SeastarWorkerServiceMethod::kRunGraph] = &SeastarWorkerService::RunGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kRecvTensor] = &SeastarWorkerService::RecvTensorHandlerRaw;\n+  handler_map_[SeastarWorkerServiceMethod::kFuseRecvTensor] = &SeastarWorkerService::FuseRecvTensorHandlerRaw;\n+  handler_map_[SeastarWorkerServiceMethod::kGetStatus] = &SeastarWorkerService::GetStatusHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kCreateWorkerSession] = &SeastarWorkerService::CreateWorkerSessionHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kRegisterGraph] = &SeastarWorkerService::RegisterGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kDeregisterGraph] = &SeastarWorkerService::DeregisterGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kCleanupGraph] = &SeastarWorkerService::CleanupGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kCleanupAll] = &SeastarWorkerService::CleanupAllHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kLogging] = &SeastarWorkerService::LoggingHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kTracing] = &SeastarWorkerService::TracingHandler;\n+}\n+\n+HandleRequestFunction SeastarWorkerService::GetHandler(SeastarWorkerServiceMethod methodId) {\n+  return handler_map_[methodId];\n+}\n+\n+void SeastarWorkerService::RunGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<RunGraphRequest, RunGraphResponse>\n+        *call = new SeastarCall<RunGraphRequest, RunGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      CallOptions* call_opts = new CallOptions;\n+      ProtoRunGraphRequest* wrapped_request =\n+        new ProtoRunGraphRequest(&call->req_);\n+      NonOwnedProtoRunGraphResponse* wrapped_response =\n+        new NonOwnedProtoRunGraphResponse(&call->resp_);\n+\n+      worker_->RunGraphAsync(call_opts, wrapped_request, wrapped_response,\n+                             [tag, call, call_opts, wrapped_request, wrapped_response](const Status& s) {\n+        tag->ProcessDone(s);\n+        delete call_opts;\n+        delete wrapped_request;\n+        delete wrapped_response;\n+        delete call;\n+      });\n+    });\n+}\n+\n+void SeastarWorkerService::GetStatusHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<GetStatusRequest, GetStatusResponse>\n+        *call = new SeastarCall<GetStatusRequest, GetStatusResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->GetStatus(&call->req_, &call->resp_);\n+\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::CreateWorkerSessionHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>\n+        *call = new SeastarCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->CreateWorkerSession(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::CleanupAllHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<CleanupAllRequest, CleanupAllResponse>\n+        *call = new SeastarCall<CleanupAllRequest, CleanupAllResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->CleanupAll(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::RegisterGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<RegisterGraphRequest, RegisterGraphResponse>\n+        *call = new SeastarCall<RegisterGraphRequest, RegisterGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->RegisterGraph(&call->req_, &call->resp_);\n+\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::DeregisterGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<DeregisterGraphRequest, DeregisterGraphResponse>\n+        *call = new SeastarCall<DeregisterGraphRequest, DeregisterGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->DeregisterGraph(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::CleanupGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<CleanupGraphRequest, CleanupGraphResponse>\n+        *call = new SeastarCall<CleanupGraphRequest, CleanupGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->CleanupGraph(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::LoggingHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<LoggingRequest, LoggingResponse>\n+        *call = new SeastarCall<LoggingRequest, LoggingResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->Logging(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::TracingHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<TracingRequest, TracingResponse>\n+        *call = new SeastarCall<TracingRequest, TracingResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->Tracing(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::RecvTensorHandlerRaw(SeastarServerTag *tag) {\n+  // LOG(INFO) << \"SeastarWorkerService::RecvTensorHandlerRaw\";\n+  Schedule([this, tag]() {\n+      CallOptions* call_opts = new CallOptions;\n+\n+      SeastarCall<RecvTensorRequest, SeastarTensorResponse> *call =\n+          new SeastarCall<RecvTensorRequest, SeastarTensorResponse>();\n+\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag, [call] (const Status& s) {delete call;});\n+\n+      worker_->RecvTensorAsync(call_opts, &call->req_, &call->resp_,\n+                               [tag, call, call_opts](const Status& s) {\n+                                 delete call_opts;\n+                                 tag->ProcessDone(s);\n+                               });\n+    });\n+}\n+\n+void SeastarWorkerService::FuseRecvTensorHandlerRaw(SeastarServerTag *tag) {\n+  // LOG(INFO) << \"SeastarWorkerService::FuseRecvTensorHandlerRaw\";\n+  Schedule([this, tag]() {\n+      CallOptions* call_opts = new CallOptions;\n+\n+      SeastarCall<FuseRecvTensorRequest, SeastarFuseTensorResponse> *call =\n+          new SeastarCall<FuseRecvTensorRequest, SeastarFuseTensorResponse>();\n+\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag, [call] (const Status& s) {delete call;});\n+\n+      worker_->FuseRecvTensorAsync(call_opts, &call->req_, &call->resp_,\n+                                   [tag, call, call_opts](const Status& s) {\n+                                     delete call_opts;\n+                                     tag->ProcessDone(s);\n+                                   });\n+    });\n+}\n+\n+void SeastarWorkerService::Schedule(std::function<void()> f) {\n+  worker_->env()->compute_pool->Schedule(std::move(f));\n+}\n+\n+WorkerEnv* SeastarWorker::env() {\n+  return env_;\n+}\n+\n+SeastarWorker::SeastarWorker(WorkerEnv* worker_env) : Worker(worker_env) {\n+}\n+\n+void SeastarWorker::RecvTensorAsync(CallOptions* opts,\n+                                    const RecvTensorRequest* request,\n+                                    SeastarTensorResponse* response,\n+                                    StatusCallback done) {\n+    const int64 step_id = request->step_id();\n+    const string& key = request->rendezvous_key();\n+    Rendezvous::ParsedKey parsed;\n+\n+    Status s = Rendezvous::ParseKey(key, &parsed);\n+    Device* src_dev = nullptr;\n+    if (s.ok()) {\n+      s = PrepareRecvTensor(parsed, &src_dev);\n+    }\n+    if (!s.ok()) {\n+      LOG(WARNING) << \"PrepareRecvTensor failed, tensor:\" << key;\n+      done(s);\n+      abort();\n+    }\n+\n+    // TODO(rangeng.llb): make call opts useful.\n+    // opts->SetCancelCallback([this, step_id]() { AbortStep(step_id); });"
      },
      {
        "body": "Same above.",
        "diff_hunk": "@@ -0,0 +1,434 @@\n+#include \"tensorflow/contrib/seastar/seastar_worker_service.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tag_factory.h\"\n+#include \"tensorflow/contrib/seastar/seastar_server_tag.h\"\n+#include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\n+#include \"tensorflow/core/distributed_runtime/call_options.h\"\n+#include \"tensorflow/core/framework/rendezvous.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/common_runtime/device_factory.h\"\n+#include \"tensorflow/core/public/session_options.h\"\n+#include \"tensorflow/contrib/verbs/verbs_util.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h\"\n+\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#endif  // GOOGLE_CUDA\n+\n+namespace tensorflow {\n+namespace {\n+\n+template<class RequestMessage, class ResponseMessage>\n+class SeastarCall {\n+public:\n+  RequestMessage req_;\n+  ResponseMessage resp_;\n+};\n+} // end of anonymous namespace\n+\n+using HandleRequestFunction = void (SeastarWorkerService::*)(SeastarServerTag*);\n+\n+SeastarWorkerService::SeastarWorkerService(SeastarWorker* worker)\n+  : worker_(worker) {\n+  handler_map_[SeastarWorkerServiceMethod::kRunGraph] = &SeastarWorkerService::RunGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kRecvTensor] = &SeastarWorkerService::RecvTensorHandlerRaw;\n+  handler_map_[SeastarWorkerServiceMethod::kFuseRecvTensor] = &SeastarWorkerService::FuseRecvTensorHandlerRaw;\n+  handler_map_[SeastarWorkerServiceMethod::kGetStatus] = &SeastarWorkerService::GetStatusHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kCreateWorkerSession] = &SeastarWorkerService::CreateWorkerSessionHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kRegisterGraph] = &SeastarWorkerService::RegisterGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kDeregisterGraph] = &SeastarWorkerService::DeregisterGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kCleanupGraph] = &SeastarWorkerService::CleanupGraphHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kCleanupAll] = &SeastarWorkerService::CleanupAllHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kLogging] = &SeastarWorkerService::LoggingHandler;\n+  handler_map_[SeastarWorkerServiceMethod::kTracing] = &SeastarWorkerService::TracingHandler;\n+}\n+\n+HandleRequestFunction SeastarWorkerService::GetHandler(SeastarWorkerServiceMethod methodId) {\n+  return handler_map_[methodId];\n+}\n+\n+void SeastarWorkerService::RunGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<RunGraphRequest, RunGraphResponse>\n+        *call = new SeastarCall<RunGraphRequest, RunGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      CallOptions* call_opts = new CallOptions;\n+      ProtoRunGraphRequest* wrapped_request =\n+        new ProtoRunGraphRequest(&call->req_);\n+      NonOwnedProtoRunGraphResponse* wrapped_response =\n+        new NonOwnedProtoRunGraphResponse(&call->resp_);\n+\n+      worker_->RunGraphAsync(call_opts, wrapped_request, wrapped_response,\n+                             [tag, call, call_opts, wrapped_request, wrapped_response](const Status& s) {\n+        tag->ProcessDone(s);\n+        delete call_opts;\n+        delete wrapped_request;\n+        delete wrapped_response;\n+        delete call;\n+      });\n+    });\n+}\n+\n+void SeastarWorkerService::GetStatusHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<GetStatusRequest, GetStatusResponse>\n+        *call = new SeastarCall<GetStatusRequest, GetStatusResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->GetStatus(&call->req_, &call->resp_);\n+\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::CreateWorkerSessionHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>\n+        *call = new SeastarCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->CreateWorkerSession(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::CleanupAllHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<CleanupAllRequest, CleanupAllResponse>\n+        *call = new SeastarCall<CleanupAllRequest, CleanupAllResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->CleanupAll(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::RegisterGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<RegisterGraphRequest, RegisterGraphResponse>\n+        *call = new SeastarCall<RegisterGraphRequest, RegisterGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->RegisterGraph(&call->req_, &call->resp_);\n+\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::DeregisterGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<DeregisterGraphRequest, DeregisterGraphResponse>\n+        *call = new SeastarCall<DeregisterGraphRequest, DeregisterGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->DeregisterGraph(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::CleanupGraphHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<CleanupGraphRequest, CleanupGraphResponse>\n+        *call = new SeastarCall<CleanupGraphRequest, CleanupGraphResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->CleanupGraph(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::LoggingHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<LoggingRequest, LoggingResponse>\n+        *call = new SeastarCall<LoggingRequest, LoggingResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->Logging(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::TracingHandler(SeastarServerTag* tag) {\n+  Schedule([this, tag]() {\n+      SeastarCall<TracingRequest, TracingResponse>\n+        *call = new SeastarCall<TracingRequest, TracingResponse>();\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag);\n+\n+      Status s = worker_->Tracing(&call->req_, &call->resp_);\n+      tag->ProcessDone(s);\n+      delete call;\n+  });\n+}\n+\n+void SeastarWorkerService::RecvTensorHandlerRaw(SeastarServerTag *tag) {\n+  // LOG(INFO) << \"SeastarWorkerService::RecvTensorHandlerRaw\";\n+  Schedule([this, tag]() {\n+      CallOptions* call_opts = new CallOptions;\n+\n+      SeastarCall<RecvTensorRequest, SeastarTensorResponse> *call =\n+          new SeastarCall<RecvTensorRequest, SeastarTensorResponse>();\n+\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag, [call] (const Status& s) {delete call;});\n+\n+      worker_->RecvTensorAsync(call_opts, &call->req_, &call->resp_,\n+                               [tag, call, call_opts](const Status& s) {\n+                                 delete call_opts;\n+                                 tag->ProcessDone(s);\n+                               });\n+    });\n+}\n+\n+void SeastarWorkerService::FuseRecvTensorHandlerRaw(SeastarServerTag *tag) {\n+  // LOG(INFO) << \"SeastarWorkerService::FuseRecvTensorHandlerRaw\";\n+  Schedule([this, tag]() {\n+      CallOptions* call_opts = new CallOptions;\n+\n+      SeastarCall<FuseRecvTensorRequest, SeastarFuseTensorResponse> *call =\n+          new SeastarCall<FuseRecvTensorRequest, SeastarFuseTensorResponse>();\n+\n+      InitSeastarServerTag(&call->req_, &call->resp_, tag, [call] (const Status& s) {delete call;});\n+\n+      worker_->FuseRecvTensorAsync(call_opts, &call->req_, &call->resp_,\n+                                   [tag, call, call_opts](const Status& s) {\n+                                     delete call_opts;\n+                                     tag->ProcessDone(s);\n+                                   });\n+    });\n+}\n+\n+void SeastarWorkerService::Schedule(std::function<void()> f) {\n+  worker_->env()->compute_pool->Schedule(std::move(f));\n+}\n+\n+WorkerEnv* SeastarWorker::env() {\n+  return env_;\n+}\n+\n+SeastarWorker::SeastarWorker(WorkerEnv* worker_env) : Worker(worker_env) {\n+}\n+\n+void SeastarWorker::RecvTensorAsync(CallOptions* opts,\n+                                    const RecvTensorRequest* request,\n+                                    SeastarTensorResponse* response,\n+                                    StatusCallback done) {\n+    const int64 step_id = request->step_id();\n+    const string& key = request->rendezvous_key();\n+    Rendezvous::ParsedKey parsed;\n+\n+    Status s = Rendezvous::ParseKey(key, &parsed);\n+    Device* src_dev = nullptr;\n+    if (s.ok()) {\n+      s = PrepareRecvTensor(parsed, &src_dev);\n+    }\n+    if (!s.ok()) {\n+      LOG(WARNING) << \"PrepareRecvTensor failed, tensor:\" << key;\n+      done(s);\n+      abort();\n+    }\n+\n+    // TODO(rangeng.llb): make call opts useful.\n+    // opts->SetCancelCallback([this, step_id]() { AbortStep(step_id); });\n+    env_->rendezvous_mgr->RecvLocalAsync(\n+      step_id, parsed,\n+      [opts, request, response, done, src_dev, key](const Status& status,\n+                                           const Rendezvous::Args& send_args,\n+                                           const Rendezvous::Args& recv_args,\n+                                           const Tensor& val, const bool is_dead) {\n+        //opts->ClearCancelCallback();\n+\n+        if (!status.ok()) {\n+          LOG(WARNING) << \"env_->rendezvous_mgr->RecvLocalAsync failed, error msg is: \"\n+                       << status.error_message();\n+        }\n+\n+        if (status.ok()) {\n+          response->SetIsDead(is_dead);\n+          bool can_memcpy = DataTypeCanUseMemcpy(val.dtype());\n+\n+          if (src_dev->tensorflow_gpu_device_info() &&\n+              (!send_args.alloc_attrs.on_host())) {\n+#if GOOGLE_CUDA\n+            CHECK(send_args.device_context)\n+              << \"send dev name: \" << src_dev->name()\n+              << \" gpu_info: \" << src_dev->tensorflow_gpu_device_info();\n+\n+            if (can_memcpy) {\n+              Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+              Tensor* cpu_copy = new Tensor(alloc, val.dtype(), val.shape());\n+\n+              GPUUtil::CopyGPUTensorToCPU(src_dev, send_args.device_context, &val, cpu_copy,\n+                                          [response, cpu_copy, done](const Status& s) {\n+                                            CHECK(s.ok()) << \"copy tensor from gpu sync\";\n+                                            response->SetTensor(*cpu_copy);\n+                                            delete cpu_copy;\n+                                            done(s);\n+                                          });\n+            } else {\n+              // NOTE(rangeng.llb): Should not be executed currrently.\n+              Tensor* copy = new Tensor(val);\n+              GPUUtil::SetProtoFromGPU(*copy,\n+                                       src_dev,\n+                                       send_args.device_context,\n+                                       &response->GetTensorProto(),\n+                                       is_dead,\n+                                       [response, copy, done] (const Status& s) {\n+                                         CHECK(s.ok()) << \"copy proto from gpu sync\";\n+                                         response->SetTensor(*copy);\n+                                         delete copy;\n+                                         done(s);\n+                                       });\n+            }\n+#else\n+            done(errors::Internal(\"No GPU device in process\"));\n+#endif\n+          } else {\n+            // tensor is in CPU memory.\n+            response->SetTensor(val);\n+            if (!can_memcpy) {\n+              val.AsProtoTensorContent(&response->GetTensorProto());\n+            }\n+            done(Status());\n+          }\n+        } else {\n+          // !s.ok()\n+          done(status);\n+        }\n+      });\n+}\n+\n+void SeastarWorker::FuseRecvTensorAsync(CallOptions* opts,\n+                                        const FuseRecvTensorRequest* request,\n+                                        SeastarFuseTensorResponse* response,\n+                                        StatusCallback done) {\n+    const int64 step_id = request->step_id();\n+    int fuse_count = request->rendezvous_key_size();\n+    std::vector<Rendezvous::ParsedKey> parsed_keys(fuse_count);\n+    std::vector<Device*>* src_devs = new std::vector<Device*>(fuse_count, nullptr);\n+\n+    for (int idx = 0; idx < fuse_count; ++idx) {\n+      const string& key = request->rendezvous_key(idx);\n+      Status s = Rendezvous::ParseKey(key, &parsed_keys[idx]);\n+      // LOG(INFO) << \"parsed_keys at index \" << idx << \" is \" << parsed_keys[idx].FullKey()\n+      //          << \" incarnation is \" << parsed_keys[idx].src_incarnation;\n+      if (s.ok()) {\n+        s = PrepareRecvTensor(parsed_keys[idx], &(*src_devs)[idx]);\n+      }\n+      \n+      if (!s.ok()) {\n+        LOG(WARNING) << \"PrepareRecvTensor failed, tensor:\" << key;\n+        delete src_devs;\n+        done(s);\n+        abort();\n+      }\n+    }\n+\n+    // TODO(rangeng.llb): make call opts useful.\n+    // opts->SetCancelCallback([this, step_id]() { AbortStep(step_id); });\n+    env_->rendezvous_mgr->FuseRecvLocalAsync(\n+      step_id, parsed_keys,\n+      [opts, request, response, done, fuse_count, src_devs](\n+          const Status& status,\n+          const std::vector<Rendezvous::Args>& send_argses,\n+          const Rendezvous::Args& recv_args,\n+          const std::vector<Tensor>& vals,\n+          const std::vector<bool>& is_deads) {\n+        // opts->ClearCancelCallback();\n+\n+        CHECK(status.ok()) << \"env_->rendezvous_mgr->FuseRecvLocalAsync failed\"\n+                           << status.error_message();\n+        \n+        if (status.ok()) {\n+          response->Init(fuse_count);\n+          int *fuse_counter = new int(fuse_count);"
      }
    ],
    "body": "This PR serves as a placeholder for contribution from @liutongxuan and his colleagues in Alibaba. \r\n\r\nSince TF is going to have yet another (hopefully last) release before 2.0 (r1.14 to be cut on April 15), I am not sure if we have enough time (or incentive) to push this feature into the main repo. \r\n\r\n[tensorflow/networking](https://github.com/tensorflow/networking) should serve as its final target after 2.0. As we are still transitioning and @annarev is working on the networking C API, I would still like to submit this PR against the main repo, mainly for the convenience of reviewing. When we feel like ready, we shall have another PR in https://github.com/tensorflow/networking for refactoring it as a standalone plugin.\r\n\r\nPing @jbedorf @poxvoculi; if you have time feel free to join the reviewing process.",
    "timestamp": "2025-05-06 01:27:30"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/4686",
    "comments": [
      "Can one of the admins verify this patch?\n",
      "Please note that this is related to the `seq2seq_loss` optimization from [this](https://github.com/tensorflow/tensorflow/pull/4382) PR.\n",
      "Hi alrojo. The dynamic decoder looks great, thanks for making it! It would be great to have at least some unit tests for it, could you add some to seq2seq_test in kernel_tests? Thanks!\n",
      "@lukaszkaiser Thanks for your fast response, I can look into the testing part, sure.\n\nBy `kernel_test` do you mean [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/seq2seq_test.py)? And do you have any specific guidelines or standards that I should follow, or should I just try and make something similar to what is already in the `seq2seq_test.py` file?\n",
      "Yes, that's exactly the test I meant. But after Eugene's comment on the other issue, I think it might be even better if you moved the code to a new tf.contrib.seq2seq module and made a test for it there. Would that be ok with you? If not, we can put it in seq2seq and then we'll move it. But it seems to make sense to have a new module where the input is always a tensor, never a list of tensors, so we can build a consistent API. Does it sound good to you?\n",
      "Hi alrojo. How is this PR now that the skeleton for seq2seq is there? Are you ready to rebase in contrib so I can review and we can merge this? Thanks again!\n",
      "Hi lukasz,\nI will update the PR with test cases for the sequence loss and rnn decoder in the new contrib folder this Saturday/Sunday.\n",
      "Hi alrojo,\n\nThanks for contributing! I'm starting to try your code on some real translation data (so there will be more comments).\n\nFor now, the first thing I noticed is perhaps to set the default value of encoder_projection to None & have check like:\n    if encoder_projection:\n      state = encoder_projection(initial_state, cell.output_size)\n    else:\n      state = initial_state\n(Maybe you can throw some error if the size doesn't match).\n\n-Thang\n",
      "Thanks for your feedback @ebrevdo, I will look into it.\n",
      "Lukasz and I discussed the existing API.  Here's how we think it could be improved.  First, remove the embedding stuff and make `inputs` optional.  `sequence_length` may be provided iff `inputs` is.  If `inputs` is provided, decoding is allowed to run up to the number of frames in `inputs` and no longer than that (regarldess of what `decoder_fn` says).  If `inputs` is not provided, `decoder_fn` must say when to stop for each minibatch entry.\n\nThere should be separate `decoder_fn` impls for training and for inference; see below for details on that:\n1. change the decoder_fn to have the following signature (up to you what you call the arguments and the outputs, of course; these are just placeholders)\n\n```\ndecoder_fn(time : scalar int,\n  cur_state : tensor_tuple [batch_size, ...],\n  cur_output : tensor_tuple [batch_size, ...],\n  cur_input tensor_tuple [batch_size, ...],\n  decoder_state : tensor_tuple [anything])\n-> \n(done : bool vec [batch_size],\n next_state : tensor_tuple [batch_size, ...],\n next_input : tensor_tuple [batch_size, ...],\n next_decoder_state : tensor_tuple[anything])\n```\n\nhere tensor_tuple means a (possibly nested) tuple of tensors or single tensor; and, e.g., `next_state` must have the same nested structure and shape as `cur_state` (same for `next_decoder_state`); and `next_input` should have consistent structure and shape across all calls.\n\nhere is how decoder_fn gets called:\n\n```\ntraining time (inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **Not None** (first input)\n  decoder_state: None\ntraining time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: not None (first input)\n  decoder_state: previous emitted decoder state\n\ninference time (**NO** inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **None**\n  decoder_state: None\ninference time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: **None**\n  decoder_state: previous emitted decoder state\n```\n\nhere's are the two simplest decoder_fns (one for training, one for inference):\n\n```\nsimplest case, training:\n  1. if cur_state: None, cur_output: None, cur_input: not None, decoder_state: None\n   return\n     next_state: your initial state\n     next_input: cur_input\n     done = [False] * batch_size\n     next_decoder_state: None\n  2. if cur_state: not None, cur_output: not None, cur_input: not None, decoder_state: None\n    return\n      next_state = cur_state\n      next_input: cur_input\n      done: [False] * batch_size\n      next_decoder_state: None\n```\n\n```\nsimplest case, inference:\n  1. if cur_state: None, cur_output: None, cur_input: None, decoder_state: None\n    return\n       next_state: your initial state\n       next input: your go symbol input\n       done: [False] * batch_size\n       next_decoder_state: None\n   2. if cur_state: not None, cur_output: not None, cur_input: None, decoder_state: None\n     return\n       next_state: cur_state\n       next_input: your choice, e.g. embed of argmax of cur_output\n       done: if argmax of cur output is done symbol\n       next_decoder_state: None\n```\n\nunit tests should contain these two decoder_fns.\n",
      "I have started writing up a simple `decoder_fn` to better understand how to support it.  And I have a question @ebrevdo @lukaszkaiser :\n\n```\ndef decoder_fn(...)\nArgs:\n`time` is current timestep, denoted `t`\n`cur_state` is the previous decoder state denoted `s_{t-1}`\n`cur_output` is ?? (same as `cur_state` or `y_{t-1}`?)\n`cur_input` is the input `x_t`\n`decoder_state` is a, possibly tuple, structure to store information between\n  timesteps (usable for beam search, a simple decoder_fn will not use it).\n\nReturns:\n`done` is a boolean vector used by raw_rnn on when to halt computation and\n  copy states\n`next_state` is the next decoder state denoted `s_t`\n`next_input` is the next input denoted `x_{t+1}`\n`next_decoder_state` is a, possibly tuple, structure to store information\n  between timesteps (usable for beam search, a simple decoder_fn will not use it).\n```\n\nWhat is the purpose of `cur_output`? Especially as we do not return a `next_output`?\nThanks\n",
      "Also, regarding `cur_input` as an argument to the `decoder_fn`. The raw_rnn api does not provide a `cur_input` as an argument to the [loop_fn function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1106). But I don't think we need it anyway in the decoder_fn. I will try and code something up.\n",
      "I think of it as follows: assume in step {t-1} we got (_, old_state, cur_input, context) = decoder_fn(...). So we have our old_state and cur_input and we run the cell, so: cur_output, cur_state = RNNCell(cur_input, old_state). Now we again run decoder_fn on (time, cur_state, cur_output, cur_input, context).\n\nNote that cur_state is actually after the RNNCell -- I think that's the only reasonable way (if you want the previous one, you can transport it in context). So decoder_fn gets access to everything the Cell produces, so it doesn't need to run it. Is that reasonable?\n",
      "Yes and cur_input you provide by reading from the input TensorArray.\n\nOn Oct 19, 2016 4:46 PM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> I think of it as follows: assume in step {t-1} we got (_, old_state,\n> cur_input, context) = decoder_fn(...). So we have our old_state and\n> cur_input and we run the cell, so: cur_output, cur_state =\n> RNNCell(cur_input, old_state). Now we again run decoder_fn on (time,\n> cur_state, cur_output, cur_input, context).\n> \n> Note that cur_state is actually after the RNNCell -- I think that's the\n> only reasonable way (if you want the previous one, you can transport it in\n> context). So decoder_fn gets access to everything the Cell produces, so it\n> doesn't need to run it. Is that reasonable?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254972317,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim4d4jJzR63M16YLixpk_ojPBNZfYks5q1qvPgaJpZM4KLMiD\n> .\n",
      "I have tried coding the `decoder_fn` and avoiding users handling tensor_arrays. However, I kept coming back to the `decoder_fn` essentially being equal to the `loop_fn` and the `rnn_decoder` just being a wrapper for `loop_fn`.\n\nInstead (or perhaps as an extension to the `decoder_fn`), I made `rnn_decoder` into a class and modularized all tasks in the `loop_fn` call so the user can provide custom functions for `new_state`, `new_input`, `new_output`, `new_loop_state`, all with access to `self` allowing calls to input_ta and similar if needed. This will allow the user to only have to change the \"modules\" they need when injecting new code. E.g. I only had to change the `input_fn` (a few lines of code) when going from training to evaluating. And without having tested it, I also think that you only need to change `input_fn` for attention support.\n\nMy hopes is that this will achieve the equivalent of what our `decoder_fn` is described to do, but with even less code.\n\nAs a result, the decoder_train and decoder_eval are now separated (you call them with different `input_fn`) and I made an `input_fn` with the eos and embedding loop_up for early stopping.\n\nI just finished running the code with no bugs \u00bd hour ago, so I won't be able to submit a clean commit tonight, but I will be at Google tomorrow by 10 to code with @lmthang. If you want more descriptions, see the code or want to discuss feel free to come by.\n\nEDIT: here is the `loop_fn` from the \"new\" `rnn_decoder`:\n\n``` python\n    def loop_fn(time, cell_output, cell_state, loop_state):\n      elements_finished = (time >= self.sequence_length) #TODO handle seq_len=None\n      # get s_t, y_t\n      emit_output = cell_output\n      if cell_output is None:\n        next_cell_state = self.state\n      else:\n        next_cell_state = self.state_fn(self, cell_state)\n      # get x_{t+1}\n      next_input, elements_finished = self.inputs_fn(self, time, next_cell_state, elements_finished)\n      # get loop_state\n      next_loop_state = self.loop_fn(self, loop_state)\n      return (elements_finished, next_input, next_cell_state,\n              emit_output, next_loop_state)\n```\n",
      "Dear Alex -- if decoder_rnn becomes a simple function, that's good, right? That's the best about good abstractions, if they solve the problem and are simple, even better! Let's never add complexity for complexity's sake.\n\nI'm not convinced about your grouping of functions. For example, you enforce that state_fn only takes state. But for beam search, it'll have to have access to the context of the beam to re-group the states.\n\nHow about doing the \"easy\" version in this PR and putting it in. We can see then if it's too little and re-do if needed. I'd really prefer such incremental approach to getting in a larger new thing and only later seeing the problems.\n",
      "FYI, I worked with @alrojo last Friday & was able to run his rnn code on real translation data (which produces similar results as the dynamic_rnn). I have also proposed several simplifications to the code. So, looking forward to a new and cleaner version from @alrojo soon!\n",
      "Yes please revert to the changes we suggested. We prefer a functional style\nover complicated class heirarchies.\n\nOn Oct 21, 2016 12:16 AM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> Dear Alex -- if decoder_rnn becomes a simple function, that's good, right?\n> That's the best about good abstractions, if they solve the problem and are\n> simple, even better! Let's never add complexity for complexity's sake.\n> \n> I'm not convinced about your grouping of functions. For example, you\n> enforce that state_fn only takes state. But for beam search, it'll have to\n> have access to the context of the beam to re-group the states.\n> \n> How about doing the \"easy\" version in this PR and putting it in. We can\n> see then if it's too little and re-do if needed. I'd really prefer such\n> incremental approach to getting in a larger new thing and only later seeing\n> the problems.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-255311260,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim6dRCeBiRwcdVUK5hK5eNVceAoBEks5q2GbCgaJpZM4KLMiD\n> .\n",
      "Hi Alexander,\n\nThanks again for working on this! Looking forward to the new API we\ndiscussed yesterday (I can quickly test it today if you have some initial\nversion ready).\n\n-Thang\n\nOn Wed, Oct 19, 2016 at 5:44 PM ebrevdo notifications@github.com wrote:\n\nYes and cur_input you provide by reading from the input TensorArray.\n\nOn Oct 19, 2016 4:46 PM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> I think of it as follows: assume in step {t-1} we got (_, old_state,\n> cur_input, context) = decoder_fn(...). So we have our old_state and\n> cur_input and we run the cell, so: cur_output, cur_state =\n> RNNCell(cur_input, old_state). Now we again run decoder_fn on (time,\n> cur_state, cur_output, cur_input, context).\n> \n> Note that cur_state is actually after the RNNCell -- I think that's the\n> only reasonable way (if you want the previous one, you can transport it in\n> context). So decoder_fn gets access to everything the Cell produces, so it\n> doesn't need to run it. Is that reasonable?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254972317\n> ,\n> or mute the thread\n> <\n> https://github.com/notifications/unsubscribe-auth/ABtim4d4jJzR63M16YLixpk_ojPBNZfYks5q1qvPgaJpZM4KLMiD\n> \n> .\n\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254980175,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAYNRXgI4YJ7yz3sP8vekzzSijsr5mdqks5q1rlbgaJpZM4KLMiD\n.\n",
      "I will make a push (without attention, this might take until the weekend as I am currently traveling) that should work, including all of our wish-list changes in a few hours. It will be without official TestCase style kernel_tests however.\n\nThree comments:\n@ebrevdo : We have tried to make `rnn_decoder` a class instance in order to maximize user-friendliness (went through this with @lmthang this Friday). The reason is we need to access class variables when writing user-friendly `decoder_fn`'s.\n\n@lmthang : I don't think we can have `sequence_length` be `None`, this could cause the decoder to get struck and eventually run OOM. Instead I have made it possible to just supply an integer value instead, representing the \"maximal_allowed\" amount of steps to decoder.\n\nDo you have any specific wishes for the `kernel_tests`?\n",
      "It was super nice meeting the team, thanks for having me!\n\n@lmthang you can try to run the code form the new commit, should support `inputs=None`. Made an extensive docstring for the `rnn_decoder` on how to use it.\n",
      "> we had discussed a clean API on friday; but this seems like a big diversion from that.\n\nand\n\n> why does decoder_fn need self?\n\nFrom previous answer:\n\n> I have tried coding the `decoder_fn` and avoiding users handling tensor_arrays. However, I kept coming back to the `decoder_fn` essentially being equal to the `loop_fn` and the `rnn_decoder` just being a wrapper for `loop_fn`.\n> \n> Instead (or perhaps as an extension to the `decoder_fn`), I made `rnn_decoder` into a class and modularized all tasks in the `loop_fn` call so the user can provide custom functions for `new_state`, `new_input`, `new_output`, `new_loop_state`, all with access to `self` allowing calls to input_ta and similar if needed. This will allow the user to only have to change the \"modules\" they need when injecting new code. E.g. I only had to change the `input_fn` (a few lines of code) when going from training to evaluating. And without having tested it, I also think that you only need to change `input_fn` for attention support.\n> \n> My hopes is that this will achieve the equivalent of what our `decoder_fn` is described to do, but with even less code.\n> \n> As a result, the decoder_train and decoder_eval are now separated (you call them with different `input_fn`) and I made an `input_fn` with the eos and embedding loop_up for early stopping.\n> \n> I just finished running the code with no bugs \u00bd hour ago, so I won't be able to submit a clean commit tonight, but I will be at Google tomorrow by 10 to code with @lmthang. If you want more descriptions, see the code or want to discuss feel free to come by.\n> \n> EDIT: here is the `loop_fn` from the \"new\" `rnn_decoder`:\n> \n> ``` python\n>     def loop_fn(time, cell_output, cell_state, loop_state):\n>       elements_finished = (time >= self.sequence_length) #TODO handle seq_len=None\n>       # get s_t, y_t\n>       emit_output = cell_output\n>       if cell_output is None:\n>         next_cell_state = self.state\n>       else:\n>         next_cell_state = self.state_fn(self, cell_state)\n>       # get x_{t+1}\n>       next_input, elements_finished = self.inputs_fn(self, time, next_cell_state, elements_finished)\n>       # get loop_state\n>       next_loop_state = self.loop_fn(self, loop_state)\n>       return (elements_finished, next_input, next_cell_state,\n>               emit_output, next_loop_state)\n> ```\n\nThen given Lukasz Answer:\n\n> I'm not convinced about your grouping of functions. For example, you enforce that state_fn only takes state. But for beam search, it'll have to have access to the context of the beam to re-group the states.\n\nI changed the interface to take the entire `loop_fn` instead (named  `decoder_fn`) with access to `self` instead of the four functions used above to allow maximal customization. Leaving the `rnn_decoder` a wrapper for `raw_rnn` to handle things like [this](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/seq2seq.py#L258).\n\nThe reason for access to self is to get access to some of the processed variables in `rnn_decoder`, such as: `inputs_ta`, `sequence_length`, `input_depth`, `batch_size`, `dtype`, `state`, `time_major` (will be used for attention) and future variables.\n\n> decoder_fn should not know anything about inputs_ta\n\nOften the main difference between training, evaluating, attention is to switch the input part. For this we need access to `inputs_ta`, [see here](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py#L52).\n\n> variables are accessible via vs.get_variable(); there's no need to store them anywhere\n\nand\n\n> batch_size can be inferred from the input shapes.\n\nAlternatively to using classes and `self` to store information, I guess we could either allow `rnn_decoder` to pass a `**kwargs` argument or store all of the above in `tf.Variable`'s and then use `vs.get_variable();`? (I might need to read a bit more about using `vs.get_variable` for the latter).\n\nPlease let me know what you think on the current architecture of having `rnn_decoder` as a wrapper for `raw_rnn` with `decoder_fn` supplying the `loop_fn` and if yes, if we should use a `**kwargs` architecture or `tf.Variable` architecture to send variables in-between the `rnn_decoder` and `decoder_fn` instead of using classes and `self`.\n",
      "@ebrevdo: maybe you can help @alrojo  (and perhaps me) here. If it seems too complicated to talk through github, I'll meet you offline.\n\nMaybe, let's look at the two functions `loop_fn_train` (line 39) and `loop_fn` (for eval, line 87) in this [file](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py). Are these good starting points to talk?\n",
      "> @ebrevdo: maybe you can help @alrojo (and perhaps me) here. If it seems too complicated to talk through github, I'll meet you offline.\n> \n> Maybe, let's look at the two functions loop_fn_train (line 39) and loop_fn (for eval, line 87) in this file. Are these good starting points to talk?\n\nI will be on standby with further development of the `kernel_tests` and the `decoder_fn_attention` until we have agreed on an interface between the `rnn_decoder` and `decoder_fn` then.\n\nI am also available on skype/hangout/appear.in today and from Friday onwards in the morning hours (as I will be gmt +1 from Friday).\n",
      "@lmthang and i will meet tomorrow and discuss the changes.\n",
      "[dynamic_decoder.txt](https://github.com/tensorflow/tensorflow/files/557353/dynamic_decoder.txt)\n\nHi @alrojo,\n\nI worked with @lukaszkaiser and @ebrevdo. It's a bit tricky to get things right in terms of the API we discussed last time. But I finally have some concrete realization of the API for you to look at (see the attached file, or [here](https://codeshare.io/XBTzz) for some syntax highlighting). This is only for training, see decoder_fn_train at the end. Writing decoder_fn_eval will be more interesting, which the attached code does give some hints. Maybe you can take a look to see if the code make sense and if you can help write decoder_fn_eval?\n\nThanks!\n-Thang\n",
      "Nice, great work! I like that you are moving the `encoder_state` to the `decoder_fn` as future seq2seq models might have different initialization approaches.\n\nYou might want to consider having the `decoder_fn` as classes to allow user-specific parameters to be passed.\nE.g. the code provided in [codeshare link](https://codeshare.io/XBTzz) would not work if you had not defined `encoder_state` somewhere above.\nBelow is `decoder_fn_train` and `decoder_fn_eval`, we should consider also providing an abstract class like the [RNNCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L87).\n\n``` python\nclass decoder_fn_train():\n\n  def __init__(self, encoder_state):\n    self.encoder_state = encoder_state\n\n  def __call__(self, cell_state, cell_output, context_state):\n    if cell_state == None: # first call, return encoder_state\n      return (None, self.encoder_state, None, cell_output, context_state)\n    else:\n      return (None, cell_state, None, cell_output, context_state)\n\nclass decoder_fn_eval():\n\n  def __init__(self, encoder_state, embeddings, sos_id, eos_id,\n               output_fn=None, dtype=tf.int32):\n    self.encoder_state = encoder_state\n    self.embeddings = embeddings\n    self.dtype = dtype\n    self.output_fn = output_fn\n    self.sos_id = tf.Variable(sos_id, dtype=self.dtype)\n    self.eos_id = tf.Variable(eos_id, dtype=self.dtype)\n\n    self.batch_size = array_ops.shape(self.encoder_state)[0]\n    # If you have an output projection on your cell, set output_fn to None\n    if self.output_fn is None:\n      self.output_fn = lambda x: x\n\n  def __call__(self, cell_state, cell_output, context_state):\n\n    if cell_output == None:\n      # invariant that this is time == 0\n      next_input_id = tf.ones([self.batch_size], dtype=self.dtype) * self.sos_id\n      done = tf.zeros([self.batch_size,], dtype=tf.bool)\n      cell_state = self.encoder_state\n    else:\n      next_input_id = tf.argmax(self.output_fn(cell_output), 1)\n      # set to None as I provide sequence_length when debugging\n      done = None#tf.equal(next_input_id, self.eos_id)\n    next_input = tf.gather(self.embeddings, next_input_id)\n\n    return (done, cell_state, next_input, cell_output, context_state)\n```\n\nOne note:\nAs pointed in earlier post regarding allowing `sequence_lengths=None`:\n\n> I don't think we can have sequence_length be None, this could cause the decoder to get stuck and eventually run OOM. Instead I have made it possible to just supply an integer value, representing the \"maximal_allowed\" amount of steps to decoder.\n\nBy allowing `sequence_lengths = None` we rely on the `decoder_fn_eval` to eventually return `True` for all sequences.\nImagine validating with an untrained network on a word-to-word model with 80k classes.\nThen you would have a 1 in 80k chance of \"randomly\" reaching the `eos` symbol.\n\nEDIT:\nThe alternative \"max_int\" value I refered in previous post to is used [here](https://github.com/tensorflow/tensorflow/pull/4686/commits/15eba5e1160ab34d2940a916bc9de8f8502317a6#diff-193a5806e6330e98e72ce0499036b9a4R261)\n\nFrom here you can provide a logical \"or\" on the time>=max_seq_len to allow earlier stopping.\n\nEDIT:\nIs the `context_state` tuple only used for infering `batch_size` or will it be needed for later for beam search?\n",
      "Regarding attention, we would need to \"concat\" something extra to the input.\n\nWe can do this by either adding the input to the `decoder_fn` or by reformulating the following:\n\n``` python\n    # call decoder function\n    if inputs is not None: # training\n      # get next_cell_input\n      if cell_state == None:\n        next_cell_input = inputs_ta.read(0)\n      else:\n        batch_size = array_ops.shape(done)[0]\n        next_cell_input = control_flow_ops.cond(\n            tf.equal(time, max_time),\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n            lambda: inputs_ta.read(time))\n      (next_done, next_cell_state, _, emit_output, next_context_state) = (\n          decoder_fn(cell_state, cell_output, context_state))\n```\n\ninto the following\n\n``` python\n    # call decoder function\n    if inputs is not None: # training\n      # get next_cell_input\n      if cell_state == None:\n        next_cell_input = inputs_ta.read(0)\n      else:\n        batch_size = array_ops.shape(done)[0]\n        next_cell_input = control_flow_ops.cond(\n            tf.equal(time, max_time),\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n            lambda: inputs_ta.read(time))\n      (next_done, next_cell_state, additional_input, emit_output, next_context_state) = (\n          decoder_fn(cell_state, cell_output, context_state))\n      if additional_input is not None:\n          # concat along feature dimension\n          next_cell_input = tf.concat(concat_dim=-1, [next_cell_input, additional_input])\n```\n",
      "You don't need a class, just a function that accepts the encoder_state and\ncreates an appropriate decoder_fn with the encoder state available via\nclosure. That creator function can return the new decoder_fn to the user.\n\nOn Oct 29, 2016 11:04 AM, \"Alexander Rosenberg Johansen\" <\nnotifications@github.com> wrote:\n\n> Regarding attention, we would need to \"concat\" something extra to the\n> input.\n> \n> We can do this by either adding the input to the decoder_fn or by\n> reformulating the following:\n> \n> ```\n> # call decoder function\n> if inputs is not None: # training\n>   # get next_cell_input\n>   if cell_state == None:\n>     next_cell_input = inputs_ta.read(0)\n>   else:\n>     batch_size = array_ops.shape(done)[0]\n>     next_cell_input = control_flow_ops.cond(\n>         tf.equal(time, max_time),\n>         lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n>         lambda: inputs_ta.read(time))\n>   (next_done, next_cell_state, _, emit_output, next_context_state) = decoder_fn(cell_state, cell_output, context_state)\n> ```\n> \n> into\n> \n> ```\n> # call decoder function\n> if inputs is not None: # training\n>   # get next_cell_input\n>   if cell_state == None:\n>     next_cell_input = inputs_ta.read(0)\n>   else:\n>     batch_size = array_ops.shape(done)[0]\n>     next_cell_input = control_flow_ops.cond(\n>         tf.equal(time, max_time),\n>         lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n>         lambda: inputs_ta.read(time))\n>   (next_done, next_cell_state, additional_input, emit_output, next_context_state) = decoder_fn(cell_state, cell_output, context_state)\n>   if additional_input is not None:\n>       # concat along feature dimension\n>       tf.concat(concat_dim=-1, [next_cell_input, additional_input])\n> ```\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-257106172,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim5DkMDEtNGiCBWvmGMSXTDEYByb0ks5q44mrgaJpZM4KLMiD\n> .\n",
      "Hi @alrojo,\n\nThanks for the update! Let's use functions instead of classes as @ebrevdo suggested (see below - you did something similar before).\n\n``` python\ndef decoder_fn_train(encoder_state):\n  def decoder_fn(cell_state, cell_output, context_state):\n    if cell_state == None: # first call, return encoder_state\n      return (None, encoder_state, None, cell_output, context_state)\n    else:\n      return (None, cell_state, None, cell_output, `context_state)\n  return decoder_fn  \n```\n\nI agree about the max_len at inference, which I think you can pass to the wrapper as below:\n\n``` python\ndef decoder_fn_eval(encoder_state, embeddings, sos_id, eos_id, max_len):\n  def decoder_fn(time, cell_state, cell_output, context_state):\n\n  return decoder_fn  \n```\n\nAbove I added back the `time` variable to decoder_fn (so that you can compare with max_len), which means we need to update the arguments for decoder_fn_train & dynamic_decoder as well. \n\nPerhaps to keep things simple for now, let's not use output_fn & worry about attention later.\n\nIf these sound good, maybe you can update your commits so that @ebrevdo can check and I can test on real data.\n\n-Thang\n"
    ],
    "review_comments": [
      {
        "body": "can't import from contrib inside core.  anyway, we're not adding major new features to core until they've gone through contrib first.\n",
        "diff_hunk": "@@ -74,6 +75,8 @@\n from tensorflow.python.ops import variable_scope\n from tensorflow.python.util import nest\n \n+from tensorflow.contrib.layers import fully_connected"
      },
      {
        "body": "in general we never build the training & eval subgraphs into the same graph.  i dont think we want to start supporting that here.  i'm worried about the API here, mainly because it's fairly monolithic and implements a specific format of seq2seq training.\n\non top of that, specific algorithms (even seq2seq ones) that are coming from the open source community into contrib need to be vetted as implementing a very major/popular NN architecture.  because someone internal will have to become a maintainer of that code.  so not only should this method be broken up, you'll need to provide justification that these are, indeed, very popular architectures by providing links to the url/article you're implementing, and it probably needs to be sufficiently well cited.\n",
        "diff_hunk": "@@ -1127,3 +1130,171 @@ def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n               softmax_loss_function=softmax_loss_function))\n \n   return outputs, losses\n+\n+\n+\"\"\"Used to project encoder state in `dynamic_rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n+\n+\n+def dynamic_rnn_decoder(cell, decoder_inputs, initial_state,"
      },
      {
        "body": "we don't allow this due to namespace pollution; in general we don't allow non-module level imports.  please use layers.... and los... to refer to methods therein.  alternatively inside your function context you can do:\n\nf1 = layers.f1\ng1 = loss.g1\n\nand use f1, g2 inside your function.\n",
        "diff_hunk": "@@ -22,5 +22,5 @@\n import sys\n \n # pylint: disable=unused-import,wildcard-import,line-too-long\n-from tensorflow.contrib.seq2seq.python.ops import layers\n-from tensorflow.contrib.seq2seq.python.ops import loss\n+from tensorflow.contrib.seq2seq.python.ops.layers import * "
      },
      {
        "body": "all caps only allowed for global variables; here you're inside a function so use lowercase.\n",
        "diff_hunk": "@@ -25,10 +25,92 @@\n \n class LayersTest(tf.test.TestCase):\n \n-  def testRNNDecoder(self):\n-    pass\n+  # test time_major=False\n+  def test_rnn_decoder1(self):\n+    with self.test_session() as sess:\n+      with tf.variable_scope(\"root\", initializer=tf.constant_initializer(0.5)):\n+        BATCH_SIZE = 2"
      },
      {
        "body": "more explanatory test name than \"test_rnn_decoder1\"\n",
        "diff_hunk": "@@ -25,10 +25,92 @@\n \n class LayersTest(tf.test.TestCase):\n \n-  def testRNNDecoder(self):\n-    pass\n+  # test time_major=False\n+  def test_rnn_decoder1(self):"
      },
      {
        "body": "don't use backslash line continuations; use parentheses instead.\n",
        "diff_hunk": "@@ -25,10 +25,92 @@\n \n class LayersTest(tf.test.TestCase):\n \n-  def testRNNDecoder(self):\n-    pass\n+  # test time_major=False\n+  def test_rnn_decoder1(self):\n+    with self.test_session() as sess:\n+      with tf.variable_scope(\"root\", initializer=tf.constant_initializer(0.5)):\n+        BATCH_SIZE = 2\n+        INPUT_SIZE = 3\n+        DECODER_INPUT_SIZE = 4\n+        ENCODER_SIZE = 6\n+        DECODER_SIZE = 7\n+        INPUT_SEQUENCE_LENGTH = 8\n+        DECODER_SEQUENCE_LENGTH = 9\n+\n+        inputs = tf.constant(0.5, shape=[BATCH_SIZE,\n+                                         INPUT_SEQUENCE_LENGTH,\n+                                         INPUT_SIZE])\n+        _, encoder_state = tf.nn.dynamic_rnn(\n+            cell=tf.nn.rnn_cell.GRUCell(ENCODER_SIZE), inputs=inputs,\n+            dtype=tf.float32, time_major=False)\n+\n+        decoder_inputs = tf.constant(0.4, shape=[BATCH_SIZE,\n+                                                 DECODER_SEQUENCE_LENGTH,\n+                                                 DECODER_INPUT_SIZE])\n+        decoder_length = tf.constant(DECODER_SEQUENCE_LENGTH, dtype=tf.int32,\n+                                     shape=[BATCH_SIZE,])\n+        decoder_fn = lambda state: tf.constant(0.5,\n+                                               shape=[BATCH_SIZE,\n+                                                      DECODER_INPUT_SIZE])\n+        decoder_outputs, valid_decoder_outputs = \\"
      },
      {
        "body": "prefer that you call this seq2seq.py over layers.py\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division"
      },
      {
        "body": "only use module-level imports (import layers; use layers.fully_connected)\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected"
      },
      {
        "body": "s/decoder_inputs/inputs/\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,"
      },
      {
        "body": "change quotes to backquotes: `cell`; here and everywhere you refer to objects (rnn_decoder below, the namespace dynamic_rnn, etc)\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'."
      },
      {
        "body": "`time == 0`\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known"
      },
      {
        "body": "The parameter `inputs`\n\ns/nessesary/necessary/\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation."
      },
      {
        "body": "While you're decoding, can't you choose to end decoding based on what you just decoded?  i.e., if you decoded an end-of-sentence token, shouldn't that mark being done?  In fact, it seems that `sequence_length` should only be necessary at training, never at eval.\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for"
      },
      {
        "body": "you write default parallel_iterations is 32, but here it's None.  which is it?\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,"
      },
      {
        "body": "ah i see what you do here.  actually, i prefer you have the argument to encoder_projection be None, and inside the code do:\n\nif encoder_projection is None:\n  encoder_projection = partial(...)\n\nkeep it out of the global namespace.\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)"
      },
      {
        "body": "(and document that in the docstring)\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)"
      },
      {
        "body": "Must be shaped `[batch_size, num_features]` (use backqutoes), where `num_features` ... etc\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to"
      },
      {
        "body": "<newline>\n\n``` python\ndef decoder_fn(...)\n...\n```\n\n<newline>\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):"
      },
      {
        "body": "extract_argmax_and_embed is a hidden implementation detail in the core seq2seq code; no one knows about it.  so don't mention it; please instead describe its purpose and signature requirements.\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`."
      },
      {
        "body": "`prev = tf.contrib.layers.linear(state)`... etc...\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias"
      },
      {
        "body": "`activation_fn=tf.tanh`.\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`."
      },
      {
        "body": "\"rnn_decoder\"\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:"
      },
      {
        "body": "what's the point of this?  can't the user do the projection themselves?  why does this function need to do it?\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:\n+    # Project initial_state as described in Bahdanau et al. 2014\n+    # https://arxiv.org/abs/1409.0473\n+    state = encoder_projection(initial_state, cell.output_size)"
      },
      {
        "body": "fix TODOs\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:\n+    # Project initial_state as described in Bahdanau et al. 2014\n+    # https://arxiv.org/abs/1409.0473\n+    state = encoder_projection(initial_state, cell.output_size)\n+    # Setup of RNN (dimensions, sizes, length, initial state, dtype)\n+    # Setup dtype\n+    dtype = state.dtype\n+    if not time_major:\n+      # [batch, seq, features] -> [seq, batch, features]\n+      decoder_inputs = array_ops.transpose(decoder_inputs, perm=[1, 0, 2])\n+    # Get data input information\n+    batch_size = array_ops.shape(decoder_inputs)[1]\n+    decoder_input_depth = int(decoder_inputs.get_shape()[2])\n+    # Setup decoder inputs as TensorArray\n+    decoder_inputs_ta = tensor_array_ops.TensorArray(dtype, size=0,\n+                                                     dynamic_size=True)\n+    decoder_inputs_ta = decoder_inputs_ta.unpack(decoder_inputs)\n+\n+    # Define RNN: loop function for training.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_train(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None"
      },
      {
        "body": "make sure that decoder_inputs.get_shape()[2].value is not None;  if it is, raise a ValueError.\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:\n+    # Project initial_state as described in Bahdanau et al. 2014\n+    # https://arxiv.org/abs/1409.0473\n+    state = encoder_projection(initial_state, cell.output_size)\n+    # Setup of RNN (dimensions, sizes, length, initial state, dtype)\n+    # Setup dtype\n+    dtype = state.dtype\n+    if not time_major:\n+      # [batch, seq, features] -> [seq, batch, features]\n+      decoder_inputs = array_ops.transpose(decoder_inputs, perm=[1, 0, 2])\n+    # Get data input information\n+    batch_size = array_ops.shape(decoder_inputs)[1]\n+    decoder_input_depth = int(decoder_inputs.get_shape()[2])"
      },
      {
        "body": "don't use backslash continuations:\n\noutputs_ta_train, _, _ = rnn.raw_rnn(\n   ...)\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:\n+    # Project initial_state as described in Bahdanau et al. 2014\n+    # https://arxiv.org/abs/1409.0473\n+    state = encoder_projection(initial_state, cell.output_size)\n+    # Setup of RNN (dimensions, sizes, length, initial state, dtype)\n+    # Setup dtype\n+    dtype = state.dtype\n+    if not time_major:\n+      # [batch, seq, features] -> [seq, batch, features]\n+      decoder_inputs = array_ops.transpose(decoder_inputs, perm=[1, 0, 2])\n+    # Get data input information\n+    batch_size = array_ops.shape(decoder_inputs)[1]\n+    decoder_input_depth = int(decoder_inputs.get_shape()[2])\n+    # Setup decoder inputs as TensorArray\n+    decoder_inputs_ta = tensor_array_ops.TensorArray(dtype, size=0,\n+                                                     dynamic_size=True)\n+    decoder_inputs_ta = decoder_inputs_ta.unpack(decoder_inputs)\n+\n+    # Define RNN: loop function for training.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_train(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None\n+      finished = math_ops.reduce_all(elements_finished)\n+      # Next input must return zero state for last element explanation below\n+      # https://github.com/tensorflow/tensorflow/issues/4519\n+      next_input = control_flow_ops.cond(\n+          finished,\n+          lambda: array_ops.zeros([batch_size, decoder_input_depth],\n+                                  dtype=dtype),\n+          lambda: decoder_inputs_ta.read(time))\n+      next_loop_state = None\n+      return (elements_finished, next_input, next_cell_state,\n+              emit_output, next_loop_state)\n+\n+    # Define RNN: loop function for evaluation.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_eval(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None\n+      finished = math_ops.reduce_all(elements_finished)\n+      # Next input must return zero state for last element explanation below\n+      # https://github.com/tensorflow/tensorflow/issues/4519\n+      next_input = control_flow_ops.cond(\n+          finished,\n+          lambda: array_ops.zeros([batch_size, decoder_input_depth],\n+                                  dtype=dtype),\n+          lambda: control_flow_ops.cond(math_ops.greater(time, 0),\n+              lambda: decoder_fn(next_cell_state), # Gather max prediction.\n+              lambda: decoder_inputs_ta.read(0))) # Read <EOS> tag\n+      next_loop_state = None\n+      return (elements_finished, next_input, next_cell_state,\n+              emit_output, next_loop_state)\n+\n+    # Run raw_rnn function\n+    outputs_ta_train, _, _ = \\"
      },
      {
        "body": "remove\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:\n+    # Project initial_state as described in Bahdanau et al. 2014\n+    # https://arxiv.org/abs/1409.0473\n+    state = encoder_projection(initial_state, cell.output_size)\n+    # Setup of RNN (dimensions, sizes, length, initial state, dtype)\n+    # Setup dtype\n+    dtype = state.dtype\n+    if not time_major:\n+      # [batch, seq, features] -> [seq, batch, features]\n+      decoder_inputs = array_ops.transpose(decoder_inputs, perm=[1, 0, 2])\n+    # Get data input information\n+    batch_size = array_ops.shape(decoder_inputs)[1]\n+    decoder_input_depth = int(decoder_inputs.get_shape()[2])\n+    # Setup decoder inputs as TensorArray\n+    decoder_inputs_ta = tensor_array_ops.TensorArray(dtype, size=0,\n+                                                     dynamic_size=True)\n+    decoder_inputs_ta = decoder_inputs_ta.unpack(decoder_inputs)\n+\n+    # Define RNN: loop function for training.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_train(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None\n+      finished = math_ops.reduce_all(elements_finished)\n+      # Next input must return zero state for last element explanation below\n+      # https://github.com/tensorflow/tensorflow/issues/4519\n+      next_input = control_flow_ops.cond(\n+          finished,\n+          lambda: array_ops.zeros([batch_size, decoder_input_depth],\n+                                  dtype=dtype),\n+          lambda: decoder_inputs_ta.read(time))\n+      next_loop_state = None\n+      return (elements_finished, next_input, next_cell_state,\n+              emit_output, next_loop_state)\n+\n+    # Define RNN: loop function for evaluation.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_eval(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None\n+      finished = math_ops.reduce_all(elements_finished)\n+      # Next input must return zero state for last element explanation below\n+      # https://github.com/tensorflow/tensorflow/issues/4519\n+      next_input = control_flow_ops.cond(\n+          finished,\n+          lambda: array_ops.zeros([batch_size, decoder_input_depth],\n+                                  dtype=dtype),\n+          lambda: control_flow_ops.cond(math_ops.greater(time, 0),\n+              lambda: decoder_fn(next_cell_state), # Gather max prediction.\n+              lambda: decoder_inputs_ta.read(0))) # Read <EOS> tag\n+      next_loop_state = None\n+      return (elements_finished, next_input, next_cell_state,\n+              emit_output, next_loop_state)\n+\n+    # Run raw_rnn function\n+    outputs_ta_train, _, _ = \\\n+      rnn.raw_rnn(cell, loop_fn_train,\n+                  parallel_iterations=parallel_iterations,\n+                  swap_memory=swap_memory, scope=varscope)\n+    # Reuse the cell for evaluation\n+    varscope.reuse_variables()\n+    outputs_ta_eval, _, _ = \\\n+      rnn.raw_rnn(cell, loop_fn_eval,\n+                  parallel_iterations=parallel_iterations,\n+                  swap_memory=swap_memory, scope=varscope)\n+    outputs_train = outputs_ta_train.pack()\n+    outputs_eval = outputs_ta_eval.pack()\n+    if not time_major:\n+      # [seq, batch, features] -> [batch, seq, features]\n+      outputs_train = array_ops.transpose(outputs_train, perm=[1, 0, 2])\n+      outputs_eval = array_ops.transpose(outputs_eval, perm=[1, 0, 2])\n+    return outputs_train, outputs_eval\n \n \n def rnn_decoder_attention(*args, **kwargs):"
      },
      {
        "body": "this is completely inconsistent with your code; which returns something really different.\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:"
      },
      {
        "body": "i'm starting to wonder why all this stuff can't be part of the RNNCell?\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`."
      },
      {
        "body": "Is that the vim command `:%s/decoder_inputs/inputs/`?\n",
        "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,"
      }
    ],
    "body": "In the [raw_rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1032) @ebrevdo [mentions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1043) that `raw_rnn` would be suitable to build a dynamic decoder with.\n\nThis is a dynamic decoder based on the `raw_rnn` as specified above.\n\nDiscussion points:\n- (Critical) The PR makes use of functools, is this allowed according to your standards?\n- (Critical) The PR makes use of contrib.layers, is this allowed according to the ops folder standards?\n- (Matter of taste) The PR could make use of [_extract_argmax_and_embed](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1043), but the `decoder_fn` makes it more customizable.\n- (Matter of taste) The PR requires decoder inputs while validating to access the start-of-sequence symbol (could be an issue for another PR later).\n",
    "timestamp": "2025-05-06 01:27:33"
  },
  {
    "pr_url": "https://github.com/tensorflow/tensorflow/pull/50489",
    "comments": [
      "@joker-eph @sanjoy @sherhut @gbaned\r\n\r\nThis is Hanhan from IREE team. I usually land changes through internal flow, and I'm not familiar with Github flow. The PR looks good to me. Could you take a look and help on landing the change? \r\n\r\nThanks!",
      "You just need to approve the PR, and the automation adds the \"ready-to-pull\" label and pulls this into a CL.\r\nIn the list of \"builds\"  you'll find a link to the internal CL when it is created.",
      "> I somehow can not comment on l.436. There is an empty statement, please delete it?\r\n\r\ndone :) \r\n\r\n\r\n",
      "@joker-eph I think it is ready to restart the integration process, could you approve again? Thanks!",
      "> @joker-eph I think it is ready to restart the integration process, could you approve again? Thanks!\r\n\r\n@hanhanW Will it not restart if you approve it instead? \ud83d\ude04 \r\n",
      "I don't have the permission. I probably could, I have to check it and see if I can get it.",
      "@joker-eph Sorry for bothering you again... Could you help approve it to restart the process again? Thanks!\r\n\r\n(I'm requesting the permission, but not get approved yet)",
      "(I got the permission and approve it to restart the process.)"
    ],
    "review_comments": [
      {
        "body": "It's used for `unordered_map<string, xxx>`, I think we can use `llvm::StringMap` instead.",
        "diff_hunk": "@@ -16,6 +16,9 @@ limitations under the License.\n // This file implements logic for lowering HLO/LHLO dialect to Linalg dialect.\n \n #include <numeric>\n+#include <set>\n+#include <string>\n+#include <unordered_map>"
      },
      {
        "body": "Use const reference to avoid copies?\r\n\r\n```suggestion\r\n    const std::set<std::string>& input_ind, const std::set<std::string>& reduction_dims) {\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {"
      },
      {
        "body": "style nit: IIRC, we would wrap braces for non-single line if-else.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());"
      },
      {
        "body": "I think we can use `llvm::StringSet`",
        "diff_hunk": "@@ -16,6 +16,9 @@ limitations under the License.\n // This file implements logic for lowering HLO/LHLO dialect to Linalg dialect.\n \n #include <numeric>\n+#include <set>"
      },
      {
        "body": "`tensor_cur_type`?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();"
      },
      {
        "body": "style nit: output_exprs",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;"
      },
      {
        "body": "This can be `SmallVector<AffineExpr>`, I think it's 4 by default.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig("
      },
      {
        "body": "Unused, delete it?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}"
      },
      {
        "body": "wrap braces.\r\n\r\nhttps://google.github.io/styleguide/cppguide.html#Conditionals",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {\n+      AffineMap scalar_map = AffineMap::get(nloops, 0, rewriter.getContext());\n+      auto inputExprs = getExprFromConfig(\n+          input_ind, input_loops_dim_vec[it.index()], str_affine_dim_umap);\n+      AffineMap multidim_map =\n+          AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+      maps.push_back(is_scalar(it.value()) ? scalar_map : multidim_map);\n+    }\n+    auto inputExprs =\n+        getExprFromConfig(input_ind, all_loop_dim.back(), str_affine_dim_umap);\n+    AffineMap multidim_map =\n+        AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+    maps.push_back(multidim_map);\n+\n+    // Creating Regions to be put into linalg.generic\n+    // If has reduction axes, operation will become mulf+addf\n+    // If no reduction is done, then operation will become just mulf\n+    auto reduction_region = [&](OpBuilder& nestedBuilder, Location nestedLoc,\n+                                ValueRange args) {\n+      Value result_val;\n+      auto MulOp =\n+          nestedBuilder.create<mlir::MulFOp>(nestedLoc, args[0], args[1]);\n+      if (reduction_axe.size() > 0) {\n+        auto AddOp = nestedBuilder.create<mlir::AddFOp>(nestedLoc, args[2],\n+                                                        MulOp.getResult());\n+        result_val = AddOp.getResult();\n+      } else\n+        result_val = MulOp.getResult();"
      },
      {
        "body": "I think we can put a lambda here, since reduction_region is used once and other patterns also do it.\r\n\r\n```cpp\r\n auto linalg_op = rewriter.create<linalg::GenericOp>(\r\n        loc, result_ty ? *result_ty : TypeRange{}, inputs, output, maps,\r\n        GetEinsumLoopsAttrs(input_ind, reduction_axe), [&](...) {...}));\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {\n+      AffineMap scalar_map = AffineMap::get(nloops, 0, rewriter.getContext());\n+      auto inputExprs = getExprFromConfig(\n+          input_ind, input_loops_dim_vec[it.index()], str_affine_dim_umap);\n+      AffineMap multidim_map =\n+          AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+      maps.push_back(is_scalar(it.value()) ? scalar_map : multidim_map);\n+    }\n+    auto inputExprs =\n+        getExprFromConfig(input_ind, all_loop_dim.back(), str_affine_dim_umap);\n+    AffineMap multidim_map =\n+        AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+    maps.push_back(multidim_map);\n+\n+    // Creating Regions to be put into linalg.generic\n+    // If has reduction axes, operation will become mulf+addf\n+    // If no reduction is done, then operation will become just mulf\n+    auto reduction_region = [&](OpBuilder& nestedBuilder, Location nestedLoc,\n+                                ValueRange args) {\n+      Value result_val;\n+      auto MulOp =\n+          nestedBuilder.create<mlir::MulFOp>(nestedLoc, args[0], args[1]);\n+      if (reduction_axe.size() > 0) {\n+        auto AddOp = nestedBuilder.create<mlir::AddFOp>(nestedLoc, args[2],\n+                                                        MulOp.getResult());\n+        result_val = AddOp.getResult();\n+      } else\n+        result_val = MulOp.getResult();\n+      nestedBuilder.create<linalg::YieldOp>(nestedLoc, result_val);\n+    };\n+\n+    // Build `linalg.generic` op.\n+    auto linalg_op = rewriter.create<linalg::GenericOp>(\n+        loc, result_ty ? *result_ty : TypeRange{}, inputs, output, maps,\n+        GetEinsumLoopsAttrs(input_ind, reduction_axe), reduction_region);"
      },
      {
        "body": "[optional] I prefer using `.`.\r\n```suggestion\r\n    rewriter.replaceOp(op, linalg_op.getResults());\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {\n+      AffineMap scalar_map = AffineMap::get(nloops, 0, rewriter.getContext());\n+      auto inputExprs = getExprFromConfig(\n+          input_ind, input_loops_dim_vec[it.index()], str_affine_dim_umap);\n+      AffineMap multidim_map =\n+          AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+      maps.push_back(is_scalar(it.value()) ? scalar_map : multidim_map);\n+    }\n+    auto inputExprs =\n+        getExprFromConfig(input_ind, all_loop_dim.back(), str_affine_dim_umap);\n+    AffineMap multidim_map =\n+        AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+    maps.push_back(multidim_map);\n+\n+    // Creating Regions to be put into linalg.generic\n+    // If has reduction axes, operation will become mulf+addf\n+    // If no reduction is done, then operation will become just mulf\n+    auto reduction_region = [&](OpBuilder& nestedBuilder, Location nestedLoc,\n+                                ValueRange args) {\n+      Value result_val;\n+      auto MulOp =\n+          nestedBuilder.create<mlir::MulFOp>(nestedLoc, args[0], args[1]);\n+      if (reduction_axe.size() > 0) {\n+        auto AddOp = nestedBuilder.create<mlir::AddFOp>(nestedLoc, args[2],\n+                                                        MulOp.getResult());\n+        result_val = AddOp.getResult();\n+      } else\n+        result_val = MulOp.getResult();\n+      nestedBuilder.create<linalg::YieldOp>(nestedLoc, result_val);\n+    };\n+\n+    // Build `linalg.generic` op.\n+    auto linalg_op = rewriter.create<linalg::GenericOp>(\n+        loc, result_ty ? *result_ty : TypeRange{}, inputs, output, maps,\n+        GetEinsumLoopsAttrs(input_ind, reduction_axe), reduction_region);\n+    rewriter.replaceOp(op, linalg_op->getResults());"
      },
      {
        "body": "[optional] I don't know if there are conventions. I usually name it as `en` when using `llvm::enumerate`.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {"
      },
      {
        "body": "style nit: `GetExprFromConfig`",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig("
      },
      {
        "body": "outdated comment?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops."
      },
      {
        "body": "Let's mark them constant.\r\n\r\n```cpp\r\nconst std::string kArrow = ...\r\n...\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";"
      },
      {
        "body": "I think we don't need optional?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;"
      },
      {
        "body": "Maybe `RankedShapedType` so we don't check `hasRank` below?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();"
      },
      {
        "body": "How about\r\n\r\n```cpp\r\nhas_elip = pre_elip == std::string::npos;\r\nif (has_elip) pre_elip = ...\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }"
      },
      {
        "body": "We don't need all_loops variable, you can rewrite it like `ArrayRef<std::string>{...}` in the for-loop.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {"
      },
      {
        "body": "Delete the comment?\r\n\r\nhttps://google.github.io/styleguide/cppguide.html#Implementation_Comment_Donts",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {\n+      AffineMap scalar_map = AffineMap::get(nloops, 0, rewriter.getContext());\n+      auto inputExprs = getExprFromConfig(\n+          input_ind, input_loops_dim_vec[it.index()], str_affine_dim_umap);\n+      AffineMap multidim_map =\n+          AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+      maps.push_back(is_scalar(it.value()) ? scalar_map : multidim_map);\n+    }\n+    auto inputExprs =\n+        getExprFromConfig(input_ind, all_loop_dim.back(), str_affine_dim_umap);\n+    AffineMap multidim_map =\n+        AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+    maps.push_back(multidim_map);\n+\n+    // Creating Regions to be put into linalg.generic\n+    // If has reduction axes, operation will become mulf+addf\n+    // If no reduction is done, then operation will become just mulf\n+    auto reduction_region = [&](OpBuilder& nestedBuilder, Location nestedLoc,\n+                                ValueRange args) {\n+      Value result_val;\n+      auto MulOp =\n+          nestedBuilder.create<mlir::MulFOp>(nestedLoc, args[0], args[1]);\n+      if (reduction_axe.size() > 0) {\n+        auto AddOp = nestedBuilder.create<mlir::AddFOp>(nestedLoc, args[2],\n+                                                        MulOp.getResult());\n+        result_val = AddOp.getResult();\n+      } else\n+        result_val = MulOp.getResult();\n+      nestedBuilder.create<linalg::YieldOp>(nestedLoc, result_val);\n+    };\n+\n+    // Build `linalg.generic` op."
      },
      {
        "body": "do we need scalar_map? Will `getExprFromConfig` return empty exprs if it is scalar case?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {\n+      AffineMap scalar_map = AffineMap::get(nloops, 0, rewriter.getContext());\n+      auto inputExprs = getExprFromConfig(\n+          input_ind, input_loops_dim_vec[it.index()], str_affine_dim_umap);\n+      AffineMap multidim_map =\n+          AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+      maps.push_back(is_scalar(it.value()) ? scalar_map : multidim_map);"
      },
      {
        "body": "maybe merge them into a single nested statement?\r\n\r\n```cpp\r\nmaps.emplace_back(AffineMap::get(...));\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);\n+\n+    // Check for contraction/summation indices\n+    std::set<std::string> reduction_axe =\n+        FindSummationAxes(input_ind, output_ind);\n+\n+    // Find input/output values and types.\n+    auto loc = op.getLoc();\n+    ValueRange inputs = args;\n+\n+    // Setting up Initial buffer for Output Tensor\n+    Value output;\n+    auto dyn_sizes = ExtractDynamicEinsumSizes(\n+        rewriter, loc, args.front(), args.back(), all_loop_dim[0],\n+        all_loop_dim[1], all_loop_dim[2]);\n+    output = GetInitTensor(rewriter, loc, *result_ty, dyn_sizes);\n+\n+    // Create indexing maps.\n+    // Create a 1:1 map from f:StrDimension -> affineDimension\n+    int64_t nloops = input_ind.size();\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap;\n+    for (auto it : llvm::enumerate(input_ind)) {\n+      str_affine_dim_umap[it.value()] = rewriter.getAffineDimExpr(it.index());\n+    }\n+    // For each Operands, we generate the affine map for accessing pattern.\n+    // Using knowledge from einsum_config/loop of each operand in string type,\n+    // we can generate the equivalent information in affine type using\n+    // \"getExprFromConfig\". Then we can feed it into the affinemap builder\n+    // where: map = mapBuilder(number_of_dims, num_constant, einsum_config,\n+    // mlirCtx)\n+    SmallVector<AffineMap, 4> maps;\n+    for (auto it : llvm::enumerate(inputs)) {\n+      AffineMap scalar_map = AffineMap::get(nloops, 0, rewriter.getContext());\n+      auto inputExprs = getExprFromConfig(\n+          input_ind, input_loops_dim_vec[it.index()], str_affine_dim_umap);\n+      AffineMap multidim_map =\n+          AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+      maps.push_back(is_scalar(it.value()) ? scalar_map : multidim_map);\n+    }\n+    auto inputExprs =\n+        getExprFromConfig(input_ind, all_loop_dim.back(), str_affine_dim_umap);\n+    AffineMap multidim_map =\n+        AffineMap::get(nloops, 0, inputExprs, rewriter.getContext());\n+    maps.push_back(multidim_map);"
      },
      {
        "body": "StringMap makes a copy of the keys, prefer `DenseMap<StringRef, ...` if possible.",
        "diff_hunk": "@@ -16,6 +16,9 @@ limitations under the License.\n // This file implements logic for lowering HLO/LHLO dialect to Linalg dialect.\n \n #include <numeric>\n+#include <set>\n+#include <string>\n+#include <unordered_map>"
      },
      {
        "body": "Why the `.str()`? Please avoid copying string around unnecessarily: try to use as much StringRef as possible everywhere.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();"
      },
      {
        "body": "Should be able to make it a `SmallVector<StringRef>` hopefully.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};"
      },
      {
        "body": "SmallVector<int>",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;"
      },
      {
        "body": "```suggestion\r\n        all_loop_dim.push_back(std::move(loop_dim));\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);"
      },
      {
        "body": "```suggestion\r\n      all_loop_dim.push_back(std::move(loop_dim));\r\n```",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);"
      },
      {
        "body": "This is making vectors copies for the sole purpose of iterating in the loop, seems wasteful?",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};"
      },
      {
        "body": "There are many uses of `auto` that seems like worth spelling out explicitly: the type isn't immediately obvious to me in general and it does not seem to help readability to erase it here.",
        "diff_hunk": "@@ -224,6 +227,280 @@ static bool HasCanonicalDimensionNumbers(\n   return true;\n }\n \n+//===----------------------------------------------------------------------===//\n+// mhlo.Einsum conversion patterns.\n+//===----------------------------------------------------------------------===//\n+\n+// Looks through a set of dimension that has been marked as Reduction axes\n+// if it is found within the set, then we set it as \"reduction\", otherwise\n+// we can label it as \"parallel\"\n+SmallVector<StringRef, 3> GetEinsumLoopsAttrs(\n+    std::set<std::string> input_ind, std::set<std::string> reduction_dims) {\n+  SmallVector<StringRef, 3> res;\n+  for (std::string dim : input_ind) {\n+    if (reduction_dims.find(dim) == reduction_dims.end())\n+      res.push_back(getParallelIteratorTypeName());\n+    else\n+      res.push_back(getReductionIteratorTypeName());\n+  }\n+  return res;\n+}\n+\n+// Check for where each dimension of the output operand came from\n+// If that dimension is dynamically sized then we push it to dyn_size vector\n+SmallVector<Value, 2> ExtractDynamicEinsumSizes(\n+    OpBuilder& b, Location loc, Value lhs, Value rhs,\n+    std::vector<std::string> lhs_loop_vec,\n+    std::vector<std::string> rhs_loop_vec,\n+    std::vector<std::string> output_loop_vec) {\n+  std::vector<Value> tensor_vec = {lhs, rhs};\n+  std::vector<ArrayRef<int64_t>> tensor_shape_vec;\n+  for (auto tensor_val : tensor_vec) {\n+    auto tensorCur_type = tensor_val.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorCur_type) return {};\n+    tensor_shape_vec.push_back(tensorCur_type.getShape());\n+  }\n+  SmallVector<Value, 2> dyn_sizes;\n+  for (std::string const& dim_ind : output_loop_vec) {\n+    int arg_number = 0;\n+    auto dim_ind_it =\n+        std::find(lhs_loop_vec.begin(), lhs_loop_vec.end(), dim_ind);\n+    auto dim_ind_pos = dim_ind_it - lhs_loop_vec.begin();\n+    if (dim_ind_it == lhs_loop_vec.end()) {\n+      arg_number = 1;\n+      dim_ind_it = std::find(rhs_loop_vec.begin(), rhs_loop_vec.end(), dim_ind);\n+      dim_ind_pos = dim_ind_it - rhs_loop_vec.begin();\n+    }\n+    if (tensor_shape_vec[arg_number][dim_ind_pos] != ShapedType::kDynamicSize)\n+      continue;\n+    dyn_sizes.push_back(\n+        b.create<memref::DimOp>(loc, tensor_vec[arg_number], dim_ind_pos));\n+  }\n+  return dyn_sizes;\n+}\n+\n+// Adds indices/axes that are missing from output set\n+std::set<std::string> FindSummationAxes(std::set<std::string> input_set,\n+                                        std::set<std::string> output_set) {\n+  std::set<std::string> summation_axes;\n+  for (auto ind : input_set) {\n+    if (output_set.find(ind) == output_set.end()) summation_axes.insert(ind);\n+  }\n+  return summation_axes;\n+}\n+\n+// Given a 1:1 map from std::string -> affine dimension expression\n+// we can get the affine expression of dimensions that an\n+// operand will access based on the input_str of einsum_config\n+// For example:\n+// let string_dim_umap = {'a' : d0, 'b' : d1, 'c' : d2}\n+// for einsum_config \"abc,cb->acb\"\n+// the first_input_operand will get umap[{\"a\",\"b\",\"c\"}] -> (d0, d1, d2)\n+// the second_input_operand will get umap[{\"c\",\"b\"}] -> (d2, d1)\n+// the ouput_operand will get umap[{\"a\",\"c\",\"b\"}] -> (d0, d2, d1)\n+SmallVector<AffineExpr, 4> getExprFromConfig(\n+    std::set<std::string> input_set, std::vector<std::string> loop_dims,\n+    std::unordered_map<std::string, AffineExpr> str_affine_dim_umap) {\n+  SmallVector<AffineExpr, 4> OutputExpr;\n+  for (std::string const& dim : loop_dims) {\n+    OutputExpr.push_back(str_affine_dim_umap[dim]);\n+  }\n+  return OutputExpr;\n+}\n+\n+// Given a real number n, will generate string of: \"012...(n-1)\"\n+std::string rangeStr(size_t number) {\n+  std::string range_str;\n+  for (size_t i = 0; i < number; i++) range_str.append(std::to_string(i));\n+  return range_str;\n+}\n+\n+// Convert mhlo.einsum op into linalg.generic\n+class EinsumToLinalgConverter : public OpConversionPattern<mhlo::EinsumOp> {\n+ public:\n+  using OpConversionPattern<mhlo::EinsumOp>::OpConversionPattern;\n+\n+  LogicalResult matchAndRewrite(\n+      mhlo::EinsumOp op, ArrayRef<Value> args,\n+      ConversionPatternRewriter& rewriter) const final {\n+    // Find maximum rank / number of loops.\n+    auto get_rank = [](Value v) {\n+      return v.getType().cast<ShapedType>().getRank();\n+    };\n+    auto is_scalar = [&](Value v) { return get_rank(v) == 0; };\n+    auto einsum_config = op.einsum_config().str();\n+\n+    // Setting Up Commonly Used Symbols as strings\n+    std::string arrow = \"->\";\n+    std::string comma = \",\";\n+    std::string ellipsis = \"...\";\n+\n+    // With the assumption of binary Input Operand and Single Output\n+    // Get the inputs and output operands' indices\n+    // einsum_config = \"lhs_loop,rhs_loop->output_loop\"\n+    std::size_t pos_arrow = einsum_config.find(arrow);\n+    std::size_t pos_comma = einsum_config.find(comma);\n+\n+    std::string lhs_loop = einsum_config.substr(0, pos_comma);\n+    std::string rhs_loop = einsum_config.substr(\n+        pos_comma + comma.size(), pos_arrow - (pos_comma + comma.size()));\n+    std::string output_loop = einsum_config.substr(pos_arrow + arrow.size());\n+\n+    // Check for Invalid Configs\n+    // 1.Check that there is only maximum 2 inputs\n+    // 2.Check that there is only maximum 1 output\n+    // 3.Check that there is 1 arrow\n+    if (rhs_loop.find(comma) != std::string::npos ||\n+        output_loop.find(comma) != std::string::npos ||\n+        output_loop.find(arrow) != std::string::npos) {\n+      return rewriter.notifyMatchFailure(op, \"Invalid Einsum Config!\");\n+    }\n+\n+    // Find result type, if on tensors.\n+    Optional<ShapedType> result_ty;\n+    result_ty = this->typeConverter->convertType(op->getResultTypes().front())\n+                    .template dyn_cast<ShapedType>();\n+\n+    // Check result type compatibility.\n+    if (!result_ty || !result_ty->hasRank() ||\n+        !(result_ty->getElementType().isSignlessIntOrFloat() ||\n+          result_ty->getElementType().isa<ComplexType>())) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"mismatched operand/result types or iterator count\");\n+    }\n+\n+    // Convert the representation from string/vector<char> to vector<string>\n+    // i.e (\"abc\") -> {\"a\", \"b\", \"c\"} S.T we can have no limit on number of\n+    // loops Since ASCII Chars has only 256 number of characters Find elilipsis\n+    // in loop and replace and find number of dim/rank ellipsis represent then\n+    // add batch dims into loop_dim as loop_dim.push_back({\"0\",... \"N\"}) where N\n+    // is rank of batch then we can add the remaining dimensions. For example\n+    // given config \"ab...cde\" representing where batch rank is 3 then we will\n+    // get loop_dim = f(\"ab...cde\") = {\"a\",\"b\",\"0\",\"1\",\"2\",\"c\",\"d\",\"e\"}\n+    std::vector<std::string> all_loops = {lhs_loop, rhs_loop, output_loop};\n+    std::vector<std::vector<std::string>> all_loop_dim;\n+    std::vector<int> batch_rank_vec;\n+    size_t max_operand_sz = 3;  // 2 Inputs + 1 output operand\n+    for (auto loop : llvm::enumerate(all_loops)) {\n+      std::vector<std::string> loop_dim;\n+      size_t pre_elip = loop.value().find(ellipsis);\n+      bool has_elip = true;\n+      if (pre_elip == std::string::npos) {\n+        pre_elip = loop.value().size();\n+        has_elip = false;\n+      }\n+      // Adding dim up to ellipsis if it exist\n+      for (int pre_ch_ind = 0; pre_ch_ind < pre_elip; pre_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[pre_ch_ind]));\n+      if (!has_elip) {\n+        all_loop_dim.push_back(loop_dim);\n+        continue;\n+      }\n+      // Case where Ellipsis presence:\n+      size_t postElip = loop.value().size() - (pre_elip + ellipsis.size());\n+      size_t operand_rank = loop.index() < (max_operand_sz - 1)\n+                                ? get_rank(args[loop.index()])\n+                                : result_ty->getRank();\n+      size_t batch_rank = operand_rank - (pre_elip + postElip);\n+      // Add (\"0\",...,\"N\") where N is rank of batch into the loop\n+      for (int batch_ind = 0; batch_ind < batch_rank; batch_ind++)\n+        loop_dim.push_back(std::to_string(batch_ind));\n+      // Add the dimension after ellipsis into the loop\n+      for (int post_ch_ind = pre_elip + ellipsis.size();\n+           post_ch_ind < loop.value().size(); post_ch_ind++)\n+        loop_dim.push_back(std::string(1, loop.value()[post_ch_ind]));\n+      batch_rank_vec.push_back(batch_rank);\n+      all_loop_dim.push_back(loop_dim);\n+    }\n+    // Check that all ellipsis represent same rank of batch\n+    if (batch_rank_vec.size() > 1) {\n+      if (!std::equal(batch_rank_vec.begin() + 1, batch_rank_vec.end(),\n+                      batch_rank_vec.begin())) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Invalid Elipsis(ellipsis) within Einsum config!\");\n+      }\n+    }\n+\n+    // Find all unique indices in the input and output\n+    std::set<std::string> input_ind;\n+    std::set<std::string> output_ind;\n+    int num_of_input_operands = args.size();\n+    std::vector<std::vector<std::string>> input_loops_dim_vec = {\n+        all_loop_dim[0], all_loop_dim[1]};\n+    for (auto input_loops_dim : input_loops_dim_vec) {\n+      for (auto dim : input_loops_dim) input_ind.insert(dim);\n+    }\n+    for (auto dim : all_loop_dim.back()) output_ind.insert(dim);"
      }
    ],
    "body": "",
    "timestamp": "2025-05-06 01:27:36"
  }
]